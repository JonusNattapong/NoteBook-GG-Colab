[Skip to content](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md#start-of-content)

You signed in with another tab or window. [Reload](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md) to refresh your session.Dismiss alert

[amrzv](https://github.com/amrzv)/ **[awesome-colab-notebooks](https://github.com/amrzv/awesome-colab-notebooks)** Public

- [Notifications](https://github.com/login?return_to=%2Famrzv%2Fawesome-colab-notebooks) You must be signed in to change notification settings
- [Fork\\
257](https://github.com/login?return_to=%2Famrzv%2Fawesome-colab-notebooks)
- [Star\\
1.4k](https://github.com/login?return_to=%2Famrzv%2Fawesome-colab-notebooks)


## Files

main

/

# README.md

Copy path

Blame

Blame

## Latest commit

[![amrzv](https://avatars.githubusercontent.com/u/36787333?v=4&size=40)](https://github.com/amrzv)[amrzv](https://github.com/amrzv/awesome-colab-notebooks/commits?author=amrzv)

[COURCES;](https://github.com/amrzv/awesome-colab-notebooks/commit/95350c4d8c231f22aceca3c1b3fb1cf37c9fc4c5)

Feb 3, 2025

[95350c4](https://github.com/amrzv/awesome-colab-notebooks/commit/95350c4d8c231f22aceca3c1b3fb1cf37c9fc4c5) · Feb 3, 2025

## History

[History](https://github.com/amrzv/awesome-colab-notebooks/commits/main/README.md)

599 lines (586 loc) · 810 KB

/

# README.md

Top

## File metadata and controls

- Preview

- Code

- Blame


599 lines (586 loc) · 810 KB

[Raw](https://github.com/amrzv/awesome-colab-notebooks/raw/refs/heads/main/README.md)

[![Hits](https://camo.githubusercontent.com/3c081342bc71cea575ae0762088eee49dd92ba0bac6e17e494588e0cd44797bb/68747470733a2f2f686974732e736565796f756661726d2e636f6d2f6170692f636f756e742f696e63722f62616467652e7376673f75726c3d68747470733a2f2f6769746875622e636f6d2f616d727a762f617765736f6d652d636f6c61622d6e6f7465626f6f6b73)](https://hits.seeyoufarm.com/)[![awesome-colab-notebooks](https://camo.githubusercontent.com/2f1649be878f9212316c56a393c1969413a2ec2d5b0c950c95377189c800634e/68747470733a2f2f636f756e742e6765746c6f6c692e636f6d2f6765742f40617765736f6d652d636f6c61622d6e6f7465626f6f6b733f7468656d653d72756c653334)](https://camo.githubusercontent.com/2f1649be878f9212316c56a393c1969413a2ec2d5b0c950c95377189c800634e/68747470733a2f2f636f756e742e6765746c6f6c692e636f6d2f6765742f40617765736f6d652d636f6c61622d6e6f7465626f6f6b733f7468656d653d72756c653334)

[![Word Cloud](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/cloud.svg)](https://github.com/amrzv/awesome-colab-notebooks/blob/main/images/cloud.svg)

The page might not be rendered properly. Please open [README.md](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md) file directly

# Awesome colab notebooks collection for ML experiments

[Permalink: Awesome colab notebooks collection for ML experiments](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md#awesome-colab-notebooks-collection-for-ml-experiments)

## Trending

[Permalink: Trending](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md#trending)

| repositories | papers | packages |
| --- | --- | --- |
| - TabPFN [![](https://camo.githubusercontent.com/bb23b472deef1236ff48f173cfcd5072a66d2ea02639a7eff1112314fa94c912/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6175746f6d6c2f54616250464e3f7374796c653d736f6369616c)](https://github.com/automl/TabPFN)<br>- crawl4ai [![](https://camo.githubusercontent.com/72f9e500bc258c28125926ccfcf0af0ef85a76b85cfcbed8fcab6a39f6ebf34b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756e636c65636f64652f637261776c3461693f7374796c653d736f6369616c)](https://github.com/unclecode/crawl4ai)<br>- opik [![](https://camo.githubusercontent.com/53d496d377ea9ce24214b26c2afd09523040749e1b383052d9d74d87231d016d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f6d65742d6d6c2f6f70696b3f7374796c653d736f6369616c)](https://github.com/comet-ml/opik)<br>- InvSR [![](https://camo.githubusercontent.com/23731fe92cd8da0536602d84867a67b46946af37ae7984dd5f31b46f43c9f6ce/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a73794f414f412f496e7653523f7374796c653d736f6369616c)](https://github.com/zsyOAOA/InvSR)<br>- moondream [![](https://camo.githubusercontent.com/58a411121543468c9cef0fcf3a55a536064d46eafe37753a4f23bcae857bf0d3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76696b687961742f6d6f6f6e647265616d3f7374796c653d736f6369616c)](https://github.com/vikhyat/moondream)<br>- unsloth [![](https://camo.githubusercontent.com/1b27822e8ac242c69b6392cda35db29df4aed3ea4a700371600b85b053448e8d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756e736c6f746861692f756e736c6f74683f7374796c653d736f6369616c)](https://github.com/unslothai/unsloth)<br>- ollama [![](https://camo.githubusercontent.com/57812a95a319d8a004bd11776a82cf59fb1d76c0648972874d7fbd3d53a34a01/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f6c6c616d612f6f6c6c616d613f7374796c653d736f6369616c)](https://github.com/ollama/ollama)<br>- langgraph [![](https://camo.githubusercontent.com/88b532f8e44248b0ff46c78ca47929bdc10d9f7493b7ce4cf03548860ffbf0fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e67636861696e2d61692f6c616e6767726170683f7374796c653d736f6369616c)](https://github.com/langchain-ai/langgraph)<br>- ARENA\_3.0 [![](https://camo.githubusercontent.com/c9e451dd95d274a2db1f682983270654c51a846c232b8924f789f07e7df105fd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f63616c6c756d6d63646f7567616c6c2f4152454e415f332e303f7374796c653d736f6369616c)](https://github.com/callummcdougall/ARENA_3.0)<br>- SAELens [![](https://camo.githubusercontent.com/663e9d11ee3d9d4f473a03455c5a3fac9cadbc5f10e5bf54d47085ed80b9f13f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a626c6f6f6d4175732f5341454c656e733f7374796c653d736f6369616c)](https://github.com/jbloomAus/SAELens)<br>- Qwen [![](https://camo.githubusercontent.com/bed1c796d00202633d29383062ecebcc8e3881693db098ca081d924b679bd01a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5177656e4c4d2f5177656e3f7374796c653d736f6369616c)](https://github.com/QwenLM/Qwen)<br>- BiRefNet [![](https://camo.githubusercontent.com/f3978c582c00ad63a5a2e5022f859c1cb8a946176a1c88d930650423743207fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5a68656e6750656e67372f42695265664e65743f7374796c653d736f6369616c)](https://github.com/ZhengPeng7/BiRefNet)<br>- TransformerLens [![](https://camo.githubusercontent.com/006f073f0ecf6c0e8b6e23a01c466d0c0db0ce83a2e475a0f5d96a979a856195/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5472616e73666f726d65724c656e734f72672f5472616e73666f726d65724c656e733f7374796c653d736f6369616c)](https://github.com/TransformerLensOrg/TransformerLens)<br>- swarm [![](https://camo.githubusercontent.com/c144a0e783e92062296c1af8e0d2eafb305215f954643a5033fd1d5d8ceaa795/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e61692f737761726d3f7374796c653d736f6369616c)](https://github.com/openai/swarm)<br>- ComfyUI [![](https://camo.githubusercontent.com/57efebdb98eae1b1a9ccda191d64c0937aa4a3ac775c6aacd529671e7d7bc566/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f6d6679616e6f6e796d6f75732f436f6d667955493f7374796c653d736f6369616c)](https://github.com/comfyanonymous/ComfyUI)<br>- autogen [![](https://camo.githubusercontent.com/bbb6473b9c51bb308dd56301a0b8326babe2f298f927cbebf276c59781266e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f6175746f67656e3f7374796c653d736f6369616c)](https://github.com/microsoft/autogen)<br>- instructor [![](https://camo.githubusercontent.com/e9daea3f9c8e116f5f9d47723f87bea22e2c1dc05af0367ef42ad3ee20281d86/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a786e6c2f696e7374727563746f723f7374796c653d736f6369616c)](https://github.com/jxnl/instructor)<br>- rlds [![](https://camo.githubusercontent.com/4c716714eeeda26ecd419e917182c5c15ffb733010f0b1c4a9f3efab9d00bfdd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f726c64733f7374796c653d736f6369616c)](https://github.com/google-research/rlds)<br>- trl [![](https://camo.githubusercontent.com/39edb3493dd498fa6daf101de06e1b9f5f2325629a505583dbc1d3f44f2abbf7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f74726c3f7374796c653d736f6369616c)](https://github.com/huggingface/trl)<br>- datachain [![](https://camo.githubusercontent.com/93b34d0029ee244dfde7b397d0021bb762fc53aaa84b94b36f50b36ea0d7610b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6974657261746976652f64617461636861696e3f7374796c653d736f6369616c)](https://github.com/iterative/datachain)<br>- langfun [![](https://camo.githubusercontent.com/e2464132cc99dedd579545dbeab3bd66919c09fe4fc7a86fc4f1b1abaaabc437/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6c616e6766756e3f7374796c653d736f6369616c)](https://github.com/google/langfun)<br>- PuLID [![](https://camo.githubusercontent.com/1ff043fbc58090053e01ebbafa4973bafe1905dc9655dff617e4d5acddd1a15c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f546f546865426567696e6e696e672f50754c49443f7374796c653d736f6369616c)](https://github.com/ToTheBeginning/PuLID)<br>- py-irt [![](https://camo.githubusercontent.com/9b7c00e31013a132ecc0b70f1efb285d463951b82bd56b37c873e56923fbebc3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e642d62616c6c2f70792d6972743f7374796c653d736f6369616c)](https://github.com/nd-ball/py-irt) | - BiRefNet [![](https://camo.githubusercontent.com/7a0cb7053bbe5f72bb637466230943bd79cc3f059f408a771639fcff9bd6c0c6/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e32363539392f4149522e323032342e39313530303338)](https://doi.org/10.26599/AIR.2024.9150038)<br>- DifFace [![](https://camo.githubusercontent.com/092abe1048e3574a8f5172c740c541bac77cb7af08a024994cad44e72d7e3e27/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5450414d492e323032342e33343332363531)](https://doi.org/10.1109/TPAMI.2024.3432651)<br>- LIDA [![](https://camo.githubusercontent.com/2cc6cd49bab6c69da9d7f7d4e5a0654f908f248707b040560a6cc087db50a808/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f323032332e61636c2d64656d6f2e3131)](https://doi.org/10.18653/v1/2023.acl-demo.11)<br>- EPro-PnP [![](https://camo.githubusercontent.com/0bae14b38f2b4a85d19f97e1faa3b3096c056bd15071e5e1425a6aea9ba57ddf/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5450414d492e323032342e33333534393937)](https://doi.org/10.1109/TPAMI.2024.3354997)<br>- UniFormerV2 [![](https://camo.githubusercontent.com/10ee65f4433905686231749e71486160800909b0f63f8e931931f86f8837548c/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435635313037302e323032332e3030313537)](https://doi.org/10.1109/ICCV51070.2023.00157)<br>- Panini-Net [![](https://camo.githubusercontent.com/200070c4f7d5198dc84bf0a4060256d1ec8834988f9f685b59e212acb57b94af/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313630392f616161692e76333669332e3230313539)](https://doi.org/10.1609/aaai.v36i3.20159)<br>- AudioLM [![](https://camo.githubusercontent.com/0829ee537d0711c8057d6da1cb5b23161d00a0a4f052f0fd0740df60850f0bb1/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5441534c502e323032332e33323838343039)](https://doi.org/10.1109/TASLP.2023.3288409)<br>- GraphCast [![](https://camo.githubusercontent.com/c21ac70080ccec32b8cc389d4e9a02cef451c572fd807c6299407698e20b118a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313132362f736369656e63652e61646932333336)](https://doi.org/10.1126/science.adi2336)<br>- VRT [![](https://camo.githubusercontent.com/d1d950750b1e94ce05c0e4baf1c0efb9e91bec942729e1525d4a303c9fb7b15c/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5449502e323032342e33333732343534)](https://doi.org/10.1109/TIP.2024.3372454)<br>- Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes [![](https://camo.githubusercontent.com/dd21c1579d26d074f5b9515be56f0a796e91a640b8df1d3a1c5b6c9f2a59a7d9/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031323032)](https://doi.org/10.1109/CVPR46437.2021.01202)<br>- GLIP [![](https://camo.githubusercontent.com/e979ecdfb47e425654465c2186bd25ae0e442fa789708c8ef8ddef3e96772b0c/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031303639)](https://doi.org/10.1109/CVPR52688.2022.01069)<br>- Contextualized Topic Models [![](https://camo.githubusercontent.com/4f07f511f6f917583e55859b3a733776a24c832459b08cbfb7d79613a03c1294/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f323032312e6561636c2d6d61696e2e313433)](https://doi.org/10.18653/v1/2021.eacl-main.143)<br>- SCALE [![](https://camo.githubusercontent.com/a54bb55079ba490c09616960923e3162403d2d2095e2aaa8ebf2695d035e7c6d/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031353832)](https://doi.org/10.1109/CVPR46437.2021.01582)<br>- AlphaPose [![](https://camo.githubusercontent.com/8ad25e6585759c2f4f882bf7d70fb67b97b875d19125c470a6d8660b43629151/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5450414d492e323032322e33323232373834)](https://doi.org/10.1109/TPAMI.2022.3222784)<br>- HiGAN [![](https://camo.githubusercontent.com/19d256907229e010bcc04bd4af3173277d906bc12264a39720224875ebb76fda/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f7331313236332d3032302d30313432392d35)](https://doi.org/10.1007/s11263-020-01429-5)<br>- Parallel WaveGAN [![](https://camo.githubusercontent.com/f494223e8cc85c4ed78e848652555a50f73b9e9ad9155bae9222f6ad1c0688f2/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f49434153535034303737362e323032302e39303533373935)](https://doi.org/10.1109/ICASSP40776.2020.9053795)<br>- DualStyleGAN [![](https://camo.githubusercontent.com/92c92334027d0048ed11402cfeaf16633db91fb73514052c9bda5b0ecf9a04ee/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030373534)](https://doi.org/10.1109/CVPR52688.2022.00754)<br>- SAHI [![](https://camo.githubusercontent.com/1885819521f9ab2a57cf396f4a28c2f0c90789d079013cac0956684748df2f27/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943495034363537362e323032322e39383937393930)](https://doi.org/10.1109/ICIP46576.2022.9897990)<br>- Gaussian Splatting [![](https://camo.githubusercontent.com/93a5d019e7b54e05da79b0fd8c1f346a35ad402f854723361819d208f335b2e6/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f33353932343333)](https://doi.org/10.1145/3592433)<br>- Swin2SR [![](https://camo.githubusercontent.com/df44874b77ce8c1fb1d2e488239699a953f4ec35338737c24b5687832dc38c13/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d32353036332d375f3432)](https://doi.org/10.1007/978-3-031-25063-7_42)<br>- AlphaTensor [![](https://camo.githubusercontent.com/b15aac575292c70af1275fc4cdb14d8a24805e4ae8c42dc555b9ec64b6b6327a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313033382f7334313538362d3032322d30353137322d34)](https://doi.org/10.1038/s41586-022-05172-4)<br>- MMRotate [![](https://camo.githubusercontent.com/59bcd41886baca0f233a9d0163d55cfde27b9095cf3e46b0e9c62690602852e1/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333530333136312e33353438353431)](https://doi.org/10.1145/3503161.3548541)<br>- PoolFormer [![](https://camo.githubusercontent.com/9fe7e400167b685a5bd5017dc04f1e409ea2462f014414e8698d16ef6c65b8ae/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031303535)](https://doi.org/10.1109/CVPR52688.2022.01055) | - Crawl4AI [![](https://camo.githubusercontent.com/d54e604ce85c2f4d3a1916a63e1abbf36a8d4d35ff0a9fb0d6d875fca35d937c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f437261776c3441493f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/Crawl4AI/)<br>- tensorflow-graphics [![](https://camo.githubusercontent.com/cd5331b397a9506e0931343e6abbbfc00e13fbff2a1603bf7cb7f40d12011543/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f74656e736f72666c6f772d67726170686963733f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/tensorflow-graphics/)<br>- opik [![](https://camo.githubusercontent.com/31af311b0e7f456646bc09b201827a83cf0e0ddbe7c9d825a1147a345b172791/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6f70696b3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/opik/)<br>- bert-score [![](https://camo.githubusercontent.com/b945e65b43a6dfea2506d967d5d4a81aa13e69298412d8c01b0ae05035fb6135/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f626572742d73636f72653f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/bert-score/)<br>- unsloth [![](https://camo.githubusercontent.com/610d5dc723fe55aab27407677e1ad0948e04a51aa93fb99c752e4759383bd828/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f756e736c6f74683f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/unsloth/)<br>- langgraph [![](https://camo.githubusercontent.com/791b7ffff0b3f9e26b5ff383df55c26e1230f361e815d53c5287f80199fde4d3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6c616e6767726170683f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/langgraph/)<br>- ollama [![](https://camo.githubusercontent.com/bcc55c939f4b70410dd392782459b3c36c0b662b58245a4efb3d487ea36b6d38/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6f6c6c616d613f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/ollama/)<br>- autofaiss [![](https://camo.githubusercontent.com/df0d821ff70adb53ce0a5e2ff2a4c6f6dd8d0679631b318a559ad699eceac183/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6175746f66616973733f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/autofaiss/)<br>- langchain [![](https://camo.githubusercontent.com/2d915942e1436ec1342a1aee82fbb03fa7cef0d41c8fa9942d7fed133cefbbfd/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6c616e67636861696e3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/langchain/)<br>- deeplabcut [![](https://camo.githubusercontent.com/84c5b4fdaf9df878f0d7f5ba4ba4f385004ee65ee66030d483d981f494e679e6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f646565706c61626375743f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/deeplabcut/)<br>- llama-index [![](https://camo.githubusercontent.com/ecae3d66659235bfc3db169f7b0313fee1fe73d92c000e883c108f7cf35b2bc6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6c6c616d612d696e6465783f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/llama-index/)<br>- pyglove [![](https://camo.githubusercontent.com/f2c35c426cbb2aac0c1f27e1f79b3b2edbf515895dbca28b6488c3fc8684b7f3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f7079676c6f76653f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/pyglove/)<br>- pybullet [![](https://camo.githubusercontent.com/40c3aa2aff7ab438e744ad85c10425c3b2c4e4b755ee2c7369c14a87f96abad7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f707962756c6c65743f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/pybullet/)<br>- mmrotate [![](https://camo.githubusercontent.com/fc0bcecd4f82005a9acf8d4e0a2b4eada51f1c292f2e75ea3afc772bbc3d1dc7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6d6d726f746174653f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/mmrotate/)<br>- folium [![](https://camo.githubusercontent.com/f76cfb5de083107767f4dba2904ad435eda29e8194b84e1badde0070cc3f7146/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f666f6c69756d3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/folium/)<br>- mistral-inference [![](https://camo.githubusercontent.com/44bff806ddbf655acb958d9585be560eacef11bef6715795e59ead1bb12ad08c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6d69737472616c2d696e666572656e63653f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/mistral-inference/)<br>- tensorflow-federated [![](https://camo.githubusercontent.com/fbf8a26aa177465936f7f88b24610b8f5bb909cd3c2b63c694752425fd50350f/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f74656e736f72666c6f772d6665646572617465643f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/tensorflow-federated/)<br>- gin-config [![](https://camo.githubusercontent.com/3dc57a0f17ae50dc8f146e0d4b709c35caa2ae3c3bac44b841986eec420a3272/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f67696e2d636f6e6669673f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/gin-config/)<br>- neural-tangents [![](https://camo.githubusercontent.com/bf22dac922f43193f2445c155c783a9f5e8c59e10ab3d2aed8453752fc1ca469/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6e657572616c2d74616e67656e74733f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/neural-tangents/)<br>- sae-lens [![](https://camo.githubusercontent.com/d1161015e51e0eac47a40deaa82d45eff92e11004bc107ca028e19b5bf0ef99e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f7361652d6c656e733f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/sae-lens/)<br>- mmsegmentation [![](https://camo.githubusercontent.com/574b499513bca4ab3542cbb7a51dd37c5e3e8f634bcf14cc093654e78adddd85/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f6d6d7365676d656e746174696f6e3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/mmsegmentation/)<br>- xgboost [![](https://camo.githubusercontent.com/55412e7f9063bffcd266c0898a31bd2fd28926f41bddbec54d4212a86d7386bd/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f7867626f6f73743f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/xgboost/)<br>- rl-games [![](https://camo.githubusercontent.com/4c572e70b8b17df21d797ba1e62e9956eef634537bae63e3c42f55a555e9cf52/68747470733a2f2f696d672e736869656c64732e696f2f707970692f64772f726c2d67616d65733f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/rl-games/) |

## Cources

[Permalink: Cources](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md#cources)

COURCES

| name | description | authors | links | colaboratory | update |
| --- | --- | :-- | :-- | :-: | :-: |
| ARENA | Provide talented individuals with the skills, tools, and environment necessary for upskilling in ML engineering, for the purpose of contributing directly to AI alignment in technical roles | [Callum McDougall](https://www.perfectlynormal.co.uk/) | [![](https://camo.githubusercontent.com/c9e451dd95d274a2db1f682983270654c51a846c232b8924f789f07e7df105fd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f63616c6c756d6d63646f7567616c6c2f4152454e415f332e303f7374796c653d736f6369616c)](https://github.com/callummcdougall/ARENA_3.0) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.00593)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://join.slack.com/t/arena-uk/shared_invite/zt-2noug8mpy-TRYbCnc3pzj7ITNrZIjKww)<br>- [website](https://arena-resources.notion.site/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1vuQOB2Gd7OcfzH2y9djXm9OdZA_DcxYz) | 30.12.2024 |
| The Autodiff Cookbook | You'll go through a whole bunch of neat autodiff ideas that you can cherry pick for your own work, starting with the basics | - [Alex Wiltschko](https://github.com/alexbw)<br>- [Matthew Johnson](http://people.csail.mit.edu/mattjj/) | [![](https://camo.githubusercontent.com/a95ee1502aa00e0efacaad4964bf58a8232dccf871220889d044d27926599077/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6a61783f7374796c653d736f6369616c)](https://github.com/google/jax/issues/446#issuecomment-467105048)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1406.2572), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.04454), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1802.03451), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1811.07062)<br>- [book](https://mitpress.mit.edu/sites/default/files/titles/content/sicm_edition_2/book.html), [book](https://mitpress.mit.edu/books/functional-differential-geometry)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google/jax#auto-vectorization-with-vmap), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hips/autograd)<br>- [tutorial](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Truncated_Newton_method), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Pullback_(differential_geometry)), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Holomorphic_function), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb) | 20.09.2024 |
| Machine Learning Simplified | A Gentle Introduction to Supervised Learning | [Andrew Wolf](https://5x12.ai/) | [![](https://camo.githubusercontent.com/a3c575313cff2f51615ae3f185ced095f2299029aa2e2ea811fbd9f8c3f2adb6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f357831322f7468656d6c73626f6f6b3f7374796c653d736f6369616c)](https://github.com/5x12/themlsbook)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/geekculture/i-found-a-great-machine-learning-book-deed11db2688)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/Python/comments/t8st9l/i_wrote_a_book_on_machine_learning_w_python_code/), [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/learnmachinelearning/comments/snxlly/machine_learning_simplified_book/)<br>- [website](https://www.themlsbook.com/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/5x12/themlsbook/blob/master/chapter2/knn.ipynb) | 29.08.2024 |
| Anthropic courses | Anthropic's educational courses | [Anthropic](https://www.anthropic.com/) | [![](https://camo.githubusercontent.com/9c3347e3253c91bb45fd1118ccc59c9ca470025c95a0726ff03976793baea99d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616e7468726f706963732f636f75727365733f7374796c653d736f6369616c)](https://github.com/anthropics/courses)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.anthropic.com/en/docs/resources/courses)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/ClaudeAI/comments/1f7czsx/anthropics_official_educational_courses_on_prompt/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/anthropics/courses/blob/master/anthropic_api_fundamentals/01_getting_started.ipynb) | 22.08.2024 |
| mlcourse.ai | Open Machine Learning Course | [Yury Kashnitsky](https://yorko.github.io/) | [![](https://camo.githubusercontent.com/20b9b17c0dcfb92de5794091110da86798337b8da29e7676381acf0f7882311d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f596f726b6f2f6d6c636f757273652e61693f7374796c653d736f6369616c)](https://github.com/Yorko/mlcourse.ai) <br>- [blog post](https://habr.com/company/ods/blog/344044/)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/kashnitsky/mlcourse)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/open-machine-learning-course)<br>- [project](https://mlcourse.ai/book/index.html)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://opendatascience.slack.com/archives/C91N8TL83/p1567408586359500)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Yorko/mlcourse.ai/blob/main/jupyter_english/topic01_pandas_data_analysis/topic1_pandas_data_analysis.ipynb) | 19.08.2024 |
| Deep RL Course | The Hugging Face Deep Reinforcement Learning Course | - [Thomas Simonini](https://www.simoninithomas.com/)<br>- [Omar Sanseviero](https://osanseviero.github.io/hackerllama/)<br>- [Sayak Paul](https://sayak.dev/) | [![](https://camo.githubusercontent.com/12db6d762710f0b410e8a0c3e0dd385651e142023c1b2de9733584445779c83c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f646565702d726c2d636c6173733f7374796c653d736f6369616c)](https://github.com/huggingface/deep-rl-class)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/alex-petrenko/sample-factory)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/deep-rl-course/unit0/introduction), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)<br>- [syllabus](https://simoninithomas.github.io/deep-rl-course)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/2GwBez0D20A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/CsuIANBnSq8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/AQKAOXJa6qg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb) | 24.06.2024 |
| Generative AI for Beginners - A Course | A 12 Lesson course teaching everything you need to know to start building Generative AI applications | [microsoft](https://www.microsoft.com/) | [![](https://camo.githubusercontent.com/5e8267ad241586e8c47191bd1e702a329090b61bf71df632bcc85f3aa344814f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f78722d646576656c6f706d656e742d666f722d626567696e6e6572733f7374796c653d736f6369616c)](https://github.com/microsoft/xr-development-for-beginners) <br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://aka.ms/genai-discord)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/Web-Dev-For-Beginners)<br>- [project](https://microsoft.github.io/generative-ai-for-beginners/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/microsoft/generative-ai-for-beginners/blob/main/06-text-generation-apps/notebook-azure-openai.ipynb) | 22.02.2024 |
| DSP theory | Theory of digital signal processing: signals, filtration (IIR, FIR, CIC, MAF), transforms (FFT, DFT, Hilbert, Z-transform) etc | - [Alexander Kapitanov](https://github.com/hukenovs)<br>- [Vladimir Fadeev](https://github.com/kirlf)<br>- [Karina Kvanchiani](https://github.com/karinakvanchiani)<br>- [Elizaveta Petrova](https://github.com/kleinsbotle)<br>- [Andrei Makhliarchuk](https://github.com/anotherhelloworld) | [![](https://camo.githubusercontent.com/5af37fcf7ff6111202386872a1eae8948e5b4ded3cbbb9727b1330e3070dc894/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756b656e6f76732f6473702d7468656f72793f7374796c653d736f6369616c)](https://github.com/hukenovs/dsp-theory) <br>- [blog post](https://habr.com/ru/articles/460445/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/hukenovs/dsp-theory/blob/master/src/dsp_theory_1_signals.ipynb) | 18.10.2022 |
| Machine learning course | This course is broad and shallow, but author will provide additional links so that you can deepen your understanding of the ML method you need | [Тимчишин Віталій](https://github.com/fbeilstein) | [![](https://camo.githubusercontent.com/73cf280c0e0b951086ed549d1f8942f44446e89fc20a11f1f1159e65e374f2c7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f666265696c737465696e2f6d616368696e655f6c6561726e696e673f7374796c653d736f6369616c)](https://github.com/fbeilstein/machine_learning) <br>- [blog post](https://vas3k.com/blog/machine_learning/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLkDeTjsoxDVgnb2lIYo9-1l4XYhrIyS6A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-RdOwhmqP5s), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/R13BD8qKeTg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ZkjP5RJLQF4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/J4Wdy0Wc_xQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/mBcLRGuAFUk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YIGtalP1mv0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Yz5pySyEtsU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/x5zLaWT5KPs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/yBwpo-L80Mc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/fbeilstein/machine_learning/blob/master/lecture_01_introduction.ipynb) | 02.09.2021 |
| NYU-DLSP20 | This course concerns the latest techniques in deep learning and representation learning, focusing on supervised and unsupervised deep learning, embedding methods, metric learning, convolutional and recurrent nets, with applications to computer vision, natural language understanding, and speech recognition | - [Yann LeCun](https://yann.lecun.com/)<br>- [Alfredo Canziani](https://atcold.github.io/) | [![](https://camo.githubusercontent.com/9a506a5c899c7ef64a0ba7a165c38f81f5e7db35ca84a929eed74798f5098e6b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4174636f6c642f4e59552d444c535032303f7374796c653d736f6369616c)](https://github.com/Atcold/NYU-DLSP20)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/CthuqsX8Pb)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Atcold/NYU-DLSP21), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Atcold/NYU-DLFL22)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/NYU_DeepLearning/)<br>- [website](https://atcold.github.io/NYU-DLSP20/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Atcold/NYU-DLSP20/blob/master/00-logic_neuron_programming.ipynb) | 30.10.2019 |

## Research

[Permalink: Research](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md#research)

RESEARCH

| name | description | authors | links | colaboratory | update |
| --- | --- | :-- | :-- | :-: | :-: |
| AlphaFold | Highly accurate protein structure prediction | - [John Jumper](https://scholar.google.com/citations?user=a5goOh8AAAAJ)<br>- [Richard Evans](http://www.doc.ic.ac.uk/~re14/)<br>- [Alexander Pritzel](https://scholar.google.com/citations?user=GPgAyU0AAAAJ)<br>- [Tim Green](http://tfgg.me/)<br>others[Michael Figurnov](https://figurnov.ru/)<br>[Olaf Ronneberger](https://lmb.informatik.uni-freiburg.de/people/ronneber/)<br>[Kathryn Tunyasuvunakool](https://scholar.google.com/citations?user=eEqNGagAAAAJ)<br>[Russ Bates](https://scholar.google.com/citations?user=Koes5ewAAAAJ)<br>[Augustin Žídek](https://augustin.zidek.eu/)<br>[Anna Potapenko](http://apotapenko.com/)<br>[Alex Bridgland](https://scholar.google.com/citations?user=VWmXKPMAAAAJ)<br>[Clemens Meyer](https://scholar.google.com/citations?user=EWLZiM8AAAAJ)<br>[Simon Kohl](https://www.simonkohl.com/)<br>[Andrew Ballard](https://scholar.google.com/citations?user=syjQhAMAAAAJ)<br>[Bernardino Romera-Paredes](https://sites.google.com/site/romeraparedes/)<br>[Stanislav Nikolov](https://scholar.google.co.uk/citations?user=O-b7pBEAAAAJ)<br>[Rishub Jain](http://rishub.me/) | [![](https://camo.githubusercontent.com/8f7a7e3b0986e960b45ad9d55f67d8d77db012af63bd610413b7ac809cc39088/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313033382f7334313538362d3032312d30333831392d32)](https://doi.org/10.1038/s41586-021-03819-2) [![](https://camo.githubusercontent.com/22a12de512ba8d62c984600effc709f1642df64113a47aa0fc82c52a72af14cb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f616c706861666f6c643f7374796c653d736f6369616c)](https://github.com/deepmind/alphafold/) <br>- [blog post](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology), [blog post](https://deepmind.com/blog/article/putting-the-power-of-alphafold-into-the-worlds-hands)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/tree), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/chex)<br>- [paper](https://www.nature.com/articles/s41586-021-03828-1)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/method/alphafold)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/AlphaFold)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=gg7WjuFs8F4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=B9PL__gVxLI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/alphafold/blob/master/notebooks/AlphaFold.ipynb) | 29.01.2025 |
| DeepLabCut | Efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data | - [Alexander Mathis](https://github.com/AlexEMG)<br>- [Pranav Mamidanna](https://pranavm19.github.io/)<br>- [Kevin Cury](https://kevincury.com/)<br>- [Taiga Abe](https://cellistigs.github.io/)<br>others[Venkatesh Murthy](https://github.com/venkateshnmurthy)<br>[Mackenzie Mathis](https://github.com/MMathisLab)<br>[Matthias Bethge](https://bethgelab.org/) | [![](https://camo.githubusercontent.com/f40a38780935dfd621aff7c621d417a0e2db792bb690a7c61d11ee6fe9a43982/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313033382f7334313539332d3031382d303230392d79)](https://doi.org/10.1038/s41593-018-0209-y)[![](https://camo.githubusercontent.com/498596c28788c973057877a903c735d4d06cbe87addbb3dd5d72b14d5091c56b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f446565704c61624375742f446565704c61624375743f7374796c653d736f6369616c)](https://github.com/DeepLabCut/DeepLabCut)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1605.03170), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1804.03142), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.11229), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2009.00564), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.13868), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.13868)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/deeplabcut/deeplabcut)<br>- [forum](https://forum.image.sc/tag/deeplabcut)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DeepLabCut/DLCutils), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@cziscience/how-open-source-software-contributors-are-accelerating-biomedicine-1a5f50f6846a)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/deeplabcut/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/DeepLabCut)<br>- [website](https://www.deeplabcut.org/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/@deeplabcut7702), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uWZu3rnj-kQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Teb5r2TNAYs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_maDLC_TrainNetwork_VideoAnalysis.ipynb) | 27.01.2025 |
| STAR | Spatial Temporal Augmentation with T2V models for Real-world video super-resolution, a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency | - [Rui Xie](https://github.com/CSRuiXie)<br>- [Yinhong Liu](https://github.com/yhliu04)<br>- [Penghao Zhou](https://scholar.google.com/citations?user=yWq1Fd4AAAAJ)<br>- [Chen Zhao](https://scholar.google.com/citations?user=Uhp3JKgAAAAJ)<br>others[Jun Zhou](https://scholar.google.com/citations?user=w03CHFwAAAAJ)<br>[Kai Zhang](https://github.com/amrzv/awesome-colab-notebooks/blob/main)<br>[Zhenyu Zhang](https://jessezhang92.github.io/)<br>[Jian Yang](https://scholar.google.com/citations?user=6CIDtZQAAAAJ)<br>[Zhenheng Yang](https://scholar.google.com/citations?user=Ds5wwRoAAAAJ)<br>[Ying Tai](https://tyshiwo.github.io/index.html) | [![](https://camo.githubusercontent.com/a64ed52ac742e66ef5a03bc9083d36b452d62ca926ef984de938f0b58cf6f04e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e4a552d5043414c61622f535441523f7374796c653d736f6369616c)](https://github.com/NJU-PCALab/STAR)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2501.02976)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ali-vilab/VGen), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Vchitect/VEnhancer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NJU-PCALab/OpenVid-1M), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hpcaitech/Open-Sora/tree/main/tools/caption#pllava-captioning)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/SherryX/STAR)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://artgor.medium.com/paper-review-star-spatial-temporal-augmentation-with-text-to-video-models-for-real-world-video-ff0ddcc6352f)<br>- [project](https://nju-pcalab.github.io/projects/STAR)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hx0zrql-SrU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1K8A1U_BNpAteRhhW9A8pAYs6LWjItQs_) | 22.01.2025 |
| InvSR | Image super-resolution technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance | - [Zongsheng Yue](https://zsyoaoa.github.io/)<br>- [Kang Liao](https://kangliao929.github.io/)<br>- [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) | [![](https://camo.githubusercontent.com/23731fe92cd8da0536602d84867a67b46946af37ae7984dd5f31b46f43c9f6ce/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a73794f414f412f496e7653523f7374796c653d736f6369616c)](https://github.com/zsyOAOA/InvSR)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2412.09013)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/csjcai/RealSR)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/OAOA/InvSR), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/sd-turbo)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/mCd7ODSmols) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1hjgCFnAU4oUUhh9VRfTwsFN1AiIjdcSR) | 21.01.2025 |
| ModernBERT | Bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders | - [Benjamin Warner](https://benjaminwarner.dev/)<br>- [Antoine Chaffin](https://antoine.chaffin.fr/)<br>- [Benjamin Clavié](https://ben.clavie.eu/)<br>- [Orion Weller](https://orionweller.github.io/)<br>others[Oskar Hallström](https://github.com/ohallstrom)<br>[Said Taghadouini](https://github.com/staghado)<br>[Alexis Gallagher](https://alexisgallagher.com/)<br>[Raja Biswas](https://github.com/rbiswasfc)<br>[Faisal Ladhak](https://github.com/fladhak)<br>[Tom Aarsen](https://www.tomaarsen.com/home)<br>[Nathan Cooper](https://nathancooper.io/)<br>[Griffin Adams](https://github.com/griff4692)<br>[Jeremy Howard](https://jeremy.fast.ai/)<br>[Iacopo Poli](https://github.com/iacolippo) | [![](https://camo.githubusercontent.com/48dc819140e5256d3249437f065321a9ed8113e5a7ec54b6ce251e0bc544c85f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f416e73776572446f7441492f4d6f6465726e424552543f7374796c653d736f6369616c)](https://github.com/AnswerDotAI/ModernBERT)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2412.13663)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mosaicml/examples/tree/main/examples/benchmarks/bert), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lightonai/pylate)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/blog/modernbert)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@sahin.samia/modernbert-the-next-generation-of-encoder-models-a-guide-to-using-and-fine-tuning-for-nlp-tasks-995a50d5232f)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Z1Dl3juwtSU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-kXUWeNcfUw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/ZWo6Q8580sA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/92HKsDHD9XI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/AnswerDotAI/ModernBERT/blob/master/examples/finetune_modernbert_on_glue.ipynb) | 22.12.2024 |
| GraphCast | Learning skillful medium-range global weather forecasting | - [Rémi Lam](https://github.com/remilam)<br>- [Alvaro Sanchez-Gonzalez](https://github.com/alvarosg)<br>- [Matthew Willson](https://github.com/mjwillson)<br>- [Peter Wirnsberger](https://pewi.org/)<br>others[Meire Fortunato](https://scholar.google.com/citations?user=_fMHSIUAAAAJ)<br>[Ferran Alet](https://scholar.google.com/citations?user=1lmBq3QAAAAJ)<br>[Suman Ravuri](https://www.linkedin.com/in/suman-ravuri-81928082)<br>[Timo Ewalds](https://github.com/tewalds)<br>[Zach Eaton-Rosen](https://scholar.google.com/citations?user=mQ3zD_wAAAAJ)<br>[Weihua Hu](https://weihua916.github.io/)<br>[Alexander Merose](https://alex.merose.com/)<br>[Stephan Hoyer](https://stephanhoyer.com/)<br>[George Holland](https://www.linkedin.com/in/g-aracil-holland)<br>[Oriol Vinyals](https://research.google/people/oriol-vinyals/)<br>[Jacklynn Stott](https://linkedin.com/in/jacklynnstott)<br>[Alexander Pritzel](https://github.com/a-pritzel)<br>[Shakir Mohamed](https://www.shakirm.com/)<br>[Peter Battaglia](https://scholar.google.com/citations?user=nQ7Ij30AAAAJ) | [![](https://camo.githubusercontent.com/c21ac70080ccec32b8cc389d4e9a02cef451c572fd807c6299407698e20b118a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313132362f736369656e63652e61646932333336)](https://doi.org/10.1126/science.adi2336) [![](https://camo.githubusercontent.com/19e8968fffda52ea708c3376cc79b32484d23f9e44f2fd7b6f8f0bf902707545/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d646565706d696e642f6772617068636173743f7374796c653d736f6369616c)](https://github.com/google-deepmind/graphcast) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.12794)<br>- [data](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5)<br>- [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-deepmind/chex), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/dask/dask), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-deepmind/jaxline), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-deepmind/tree), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mikedh/trimesh)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/graphcast-how-to-get-things-done-f2fd5630c5fb)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BufUW7h9TB8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PD1v5PCJs_o), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Eul-JN9Nwb0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BTyhgp9Hugc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/aJ_H4exg0xU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/graphcast/blob/master/graphcast_demo.ipynb) | 04.12.2024 |
| TAPIR | Tracking Any Point with per-frame Initialization and temporal Refinement | - [Carl Doersch](http://www.carldoersch.com/)<br>- [Yi Yang](https://yangyi02.github.io/)<br>- [Mel Vecerik](https://scholar.google.com/citations?user=Jvi_XPAAAAAJ)<br>- [Dilara Gokay](https://scholar.google.com/citations?user=cnbENAEAAAAJ)<br>others[Ankush Gupta](https://ankushgupta.org/)<br>[Yusuf Aytar](https://people.csail.mit.edu/yusuf/)<br>[Joao Carreira](https://scholar.google.com/citations?user=IUZ-7_cAAAAJ)<br>[Andrew Zisserman](https://www.robots.ox.ac.uk/~az/) | [![](https://camo.githubusercontent.com/eaefa78e8205346212c189a7b56147762867fc8d995a39f008dd49093efbb3a7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d646565706d696e642f7461706e65743f7374796c653d736f6369616c)](https://github.com/google-deepmind/tapnet)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.08637), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2308.15975)<br>- [blog post](https://deepmind-tapir.github.io/), [blog post](https://deepmind-tapir.github.io/blogpost.html)<br>- [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://www.deepmind.com/open-source/kinetics)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-research/kubric/tree/main/challenges/point_tracking)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@jumabek4044/what-is-tapir-tracking-any-point-with-per-frame-initialization-and-temporal-refinement-and-how-it-bdad9946dc53)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/58168e8a92994655d6da3939e7cc0918-Abstract-Datasets_and_Benchmarks.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/2HSHofqoJ9M), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/I1DQJH3v7Nk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/tapnet/blob/master/colabs/causal_tapir_demo.ipynb) | 30.11.2024 |
| T2M-GPT | Conditional generative framework based on Vector Quantised-Variational AutoEncoder and Generative Pre-trained Transformer for human motion generation from textural descriptions | - [Jianrong Zhang](https://github.com/Jiro-zhang)<br>- [Yangsong Zhang](https://github.com/Mael-zys)<br>- [Xiaodong Cun](https://vinthony.github.io/academic/)<br>- [Shaoli Huang](https://shaoli-huang.github.io/)<br>others[Yong Zhang](https://yzhang2016.github.io/)<br>[Hongwei Zhao](https://teachers.jlu.edu.cn/zhaohongwei/en/index.htm)<br>[Hongtao Lu](https://www.cs.sjtu.edu.cn/en/PeopleDetail.aspx?id=156)<br>[Xi Shen](https://xishen0220.github.io/) | [![](https://camo.githubusercontent.com/f967fe9da8e17b90a1b3d12bfcd5acfbe700e9a631ec468204df3e40b898f754/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323732392e323032332e3031343135)](https://doi.org/10.1109/CVPR52729.2023.01415)[![](https://camo.githubusercontent.com/e1b8887651af1f57a074bd3d7b06d422ba368d0f1ec6d3f1ffd703b95b6ec2d2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d61656c2d7a79732f54324d2d4750543f7374796c653d736f6369616c)](https://github.com/Mael-zys/T2M-GPT)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.06052)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/EricGuo5513/HumanML3D), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/EricGuo5513/text-to-motion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/GuyTevet/motion-diffusion-model), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/EricGuo5513/TM2T)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/vumichien/T2M-GPT), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/vumichien/generate_human_motion)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@kaveh.kamali/t2m-gpt-pioneering-human-motion-generation-from-textual-descriptions-48dc62b5cd7a)<br>- [project](https://mael-zys.github.io/T2M-GPT/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/09K2cx9P0_0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Vy69w2q2d-Hg19F-KibqG0FRdpSj3L4O) | 24.11.2024 |
| PuLID | Pure and Lightning ID customization, a tuning-free ID customization method for text-to-image generation | - [Zinan Guo](https://github.com/guozinan126)<br>- [Yanze Wu](https://tothebeginning.github.io/)<br>- [Zhuowei Chen](https://scholar.google.com/citations?user=ow1jGJkAAAAJ)<br>- [Lang Chen](https://scholar.google.com/citations?user=h5xex20AAAAJ)<br>- [Qian He](https://scholar.google.com/citations?user=9rWWCgUAAAAJ) | [![](https://camo.githubusercontent.com/1ff043fbc58090053e01ebbafa4973bafe1905dc9655dff617e4d5acddd1a15c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f546f546865426567696e6e696e672f50754c49443f7374796c653d736f6369616c)](https://github.com/ToTheBeginning/PuLID)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2404.16022)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cubiq/PuLID_ComfyUI), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-PuLID-ZHO), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Mikubill/sd-webui-controlnet/pull/2838)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/comfyui/comments/1cnv269/pulid_pure_and_lightning_id_customization_via/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/PuLID-jupyter/blob/main/PuLID_jupyter.ipynb) | 09.11.2024 |
| CoTracker | Architecture that jointly tracks multiple points throughout an entire video | - [Nikita Karaev](https://nikitakaraevv.github.io/)<br>- [Ignacio Rocco](https://www.irocco.info/)<br>- [Benjamin Graham](https://ai.meta.com/people/benjamin-graham/)<br>- [Natalia Neverova](https://nneverova.github.io/)<br>others[Andrea Vedaldi](https://www.robots.ox.ac.uk/~vedaldi/)<br>[Christian Rupprecht](https://chrirupp.github.io/) | [![](https://camo.githubusercontent.com/758d63693383c8414cd2d85a97c0d7cf11981b30c9776c8a801bf18d092b8271/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f636f2d747261636b65723f7374796c653d736f6369616c)](https://github.com/facebookresearch/co-tracker)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2307.07635), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.11898)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/benjiebob/BADJA)<br>- [project](https://co-tracker.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/w5QVc7BVGPA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/co-tracker/blob/main/notebooks/demo.ipynb) | 16.10.2024 |
| PIFu | Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization | - [Ryota Natsume](https://github.com/nanopoteto)<br>- [Shunsuke Saito](https://shunsukesaito.github.io/)<br>- [Zeng Huang](https://zeng.science/)<br>- [Angjoo Kanazawa](https://people.eecs.berkeley.edu/~kanazawa/)<br>- [Hao Li](http://hao.li/) | [![](https://camo.githubusercontent.com/43d0e68e7d9348cff526d58bbaccbf401ab1ef6a9fb36bd9ceb43a6542737855/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343562e323031392e3030323339)](https://doi.org/10.1109/ICCV.2019.00239)[![](https://camo.githubusercontent.com/89641f2ebbfba1342545f9dd2b49c37dde6ca2ed34809de7d4ae4a541fea57a0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7368756e73756b65736169746f2f504946753f7374796c653d736f6369616c)](https://github.com/shunsukesaito/PIFu)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1905.05172)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=S1FpjwKqtPs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1GFSsqP2BWz4gtq0e-nki00ZHSirXwFyY) | 08.10.2024 |
| DifFace | Method that is capable of coping with unseen and complex degradations more gracefully without complicated loss designs | - [Zongsheng Yue](https://zsyoaoa.github.io/)<br>- [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) | [![](https://camo.githubusercontent.com/092abe1048e3574a8f5172c740c541bac77cb7af08a024994cad44e72d7e3e27/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5450414d492e323032342e33343332363531)](https://doi.org/10.1109/TPAMI.2024.3432651)[![](https://camo.githubusercontent.com/c885c501791e1018a3855ffe26bd72797aad7d84056f07b5ebbc3b41586c5202/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7a73794f414f412f446966466163653f7374796c653d736f6369616c)](https://github.com/zsyOAOA/DifFace)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.06512)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/improved-diffusion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepcam-cn/yolov5-face), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/facexlib)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/OAOA/DifFace) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1BNtoPPRuJwNDvqfwDOOmD9XJyF05Zh4m) | 05.10.2024 |
| Segment Anything 2 | Foundation model towards solving promptable visual segmentation in images and videos | - [Nikhila Ravi](https://nikhilaravi.com/)<br>- [Valentin Gabeur](https://gabeur.github.io/)<br>- [Yuan-Ting Hu](https://scholar.google.com/citations?user=E8DVVYQAAAAJ)<br>- [Ronghang Hu](https://ronghanghu.com/)<br>others[Chaitanya Ryali](https://scholar.google.com/citations?user=4LWx24UAAAAJ)<br>[Tengyu Ma](https://scholar.google.com/citations?user=VeTSl0wAAAAJ)<br>[Haitham Khedr](https://hkhedr.com/)<br>[Roman Rädle](https://scholar.google.de/citations?user=Tpt57v0AAAAJ)<br>[Chloé Rolland](https://scholar.google.com/citations?user=n-SnMhoAAAAJ)<br>[Laura Gustafson](https://scholar.google.com/citations?user=c8IpF9gAAAAJ)<br>[Eric Mintun](https://ericmintun.github.io/)<br>[Junting Pan](https://junting.github.io/)<br>\[Kalyan Vasudev\](lwala\]( [https://scholar.google.co.in/citations?user=m34oaWEAAAAJ](https://scholar.google.co.in/citations?user=m34oaWEAAAAJ))<br>[Nicolas Carion](https://www.nicolascarion.com/)<br>\[Chao-Yuan\](u\]( [https://chaoyuan.org/](https://chaoyuan.org/))<br>[Ross Girshick](https://www.rossgirshick.info/)<br>[Piotr Dollár](https://pdollar.github.io/)<br>[Christoph Feichtenhofer](https://feichtenhofer.github.io/) | [![](https://camo.githubusercontent.com/c91bab0b21914147b3b863f6195ebd6262c69260cb2fcf8f8c63b341097324c1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7365676d656e742d616e797468696e672d323f7374796c653d736f6369616c)](https://github.com/facebookresearch/segment-anything-2) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2408.00714)<br>- [demo](https://sam2.metademolab.com/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zsef123/Connected_components_PyTorch)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/models?search=facebook/sam2)<br>- [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/), [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://ai.meta.com/datasets/segment-anything-video), [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://ai.meta.com/blog/segment-anything-2)<br>- [project](https://ai.meta.com/sam2/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://x.com/AIatMeta/status/1818055906179105010)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=w-cmMcMZoZ4&t=2325s), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/O8QdvZbRDp4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/Dv003fTyO-Y), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/IW7jFq3vQbw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/segment-anything-2/blob/main/notebooks/image_predictor_example.ipynb) | 01.10.2024 |
| Open-Unmix | A deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists | - [Fabian-Robert Stöter](http://faroit.com/)<br>- [Antoine Liutkus](https://github.com/aliutkus) | [![](https://camo.githubusercontent.com/e6d784959ac87ec2c092999809fe3157d0a937f65027365fa788b66a236b238b/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e32313130352f6a6f73732e3031363637)](https://doi.org/10.21105/joss.01667) [![](https://camo.githubusercontent.com/8ceee932ce63b06bea6b3ce935e5ae9b7d86768a7b10c9e58eae2c186c48059b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7369677365702f6f70656e2d756e6d69782d7079746f7263683f7374796c653d736f6369616c)](https://github.com/sigsep/open-unmix-pytorch) <br>- [data](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sigsep/norbert)<br>- [project](https://sigsep.github.io/open-unmix/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/music-source-separation-on-musdb18?p=open-unmix-a-reference-implementation-for)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLhA3b2k8R3t0VpYCpCTU2B1h604rvnV4N) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ) | 25.09.2024 |
| Deep Painterly Harmonization | Algorithm produces significantly better results than photo compositing or global stylization techniques and that it enables creative painterly edits that would be otherwise difficult to achieve | - [Fujun Luan](https://luanfujun.github.io/)<br>- [Sylvain Paris](http://people.csail.mit.edu/sparis/)<br>- [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)<br>- [Kavita Bala](https://www.cs.cornell.edu/~kb/) | [![](https://camo.githubusercontent.com/01d52061c912088cabfe060347d3dfa695c1e1a694ec2a0b2f9b3205a067e37d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c75616e66756a756e2f646565702d7061696e7465726c792d6861726d6f6e697a6174696f6e3f7374796c653d736f6369616c)](https://github.com/luanfujun/deep-painterly-harmonization)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1804.03189), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1701.08893)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jcjohnson/neural-style), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/torch/torch7), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/szagoruyko/loadcaffe) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/gist/eyaler/5303782669fb43510d398bd346c6e3e6/deep-painterly-harmonization.ipynb) | 23.09.2024 |
| audio2photoreal | Framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction | - [Evonne Ng](https://people.eecs.berkeley.edu/~evonne_ng/)<br>- [Javier Romero](https://scholar.google.com/citations?user=Wx62iOsAAAAJ)<br>- [Timur Bagautdinov](https://scholar.google.ch/citations?user=oLi7xJ0AAAAJ)<br>- [Shaojie Bai](https://jerrybai1995.github.io/)<br>others[Trevor Darrell](https://people.eecs.berkeley.edu/~trevor/)<br>[Angjoo Kanazawa](https://people.eecs.berkeley.edu/~kanazawa/)<br>[Alexander Richard](https://alexanderrichard.github.io/) | [![](https://camo.githubusercontent.com/df01d8fcfc4044528a4bf05164ab7de596ea043674d0c26f590e9d09cb343a73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f617564696f3270686f746f7265616c3f7374796c653d736f6369616c)](https://github.com/facebookresearch/audio2photoreal) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2401.01885)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/ca_body)<br>- [project](https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Y0GMaMtUynQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1A6EwKM3PeX7dcKV66zxQWuP-v_dKlX_0) | 13.09.2024 |
| Fast Segment Anything | CNN Segment Anything Model trained using only 2% of the SA-1B dataset published by SAM authors | - [Xu Zhao](https://scholar.google.com/citations?user=F0cYEyAAAAAJ)<br>- [Wenchao Ding](https://github.com/berry-ding)<br>- [Yongqi An](https://github.com/an-yongqi)<br>- [Yinglong Du](https://github.com/YinglongDu)<br>others[Tao Yu](https://github.com/tianjinren)<br>[Min Li](https://github.com/limin2021)<br>[Ming Tang](https://www.researchgate.net/profile/Ming-Tang-2)<br>[Jinqiao Wang](https://scholar.google.com/citations?user=7_BkyxEAAAAJ) | [![](https://camo.githubusercontent.com/fecffc60740bee83b36af99fb1b05fd22d6066434ae01fa32d154e8bf9e1ef43/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f43415349412d4956412d4c61622f4661737453414d3f7374796c653d736f6369616c)](https://github.com/CASIA-IVA-Lab/FastSAM)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.12156), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10003)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ChuRuaNh0/FastSam_Awsome_TensorRT)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@mahimairaja/so-what-exactly-is-fastsam-the-ultimate-guide-ddae21d3b486)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/yHNPyqazYYU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/SslzS0AsiAw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/qvqkjP1wCDE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1oX14f6IneGGw612WgVlAiy91UHwFAvr9) | 10.09.2024 |
| Neuralangelo | Framework for high-fidelity 3D surface reconstruction from RGB video captures | - [Zhaoshuo Li](https://mli0603.github.io/)<br>- [Thomas Müller](https://tom94.net/)<br>- [Alex Evans](https://scholar.google.com/citations?user=ToqGImkAAAAJ)<br>- [Russell Taylor](https://www.cs.jhu.edu/~rht/)<br>others[Mathias Unberath](https://mathiasunberath.github.io/)<br>[Ming-Yu Liu](https://mingyuliu.net/)<br>[Chen-Hsuan Lin](https://chenhsuanlin.bitbucket.io/) | [![](https://camo.githubusercontent.com/cb724e2cd4315d53e66b26d4167661b5af341fcdb2fb3a71f0ea736ba24c03c8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e566c6162732f6e657572616c616e67656c6f3f7374796c653d736f6369616c)](https://github.com/NVlabs/neuralangelo) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.03092)<br>- [blog post](https://blogs.nvidia.com/blog/2023/06/01/neuralangelo-ai-research-3d-reconstruction/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mli0603/BlenderNeuralangelo)<br>- [project](https://research.nvidia.com/labs/dir/neuralangelo/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PQMNCXR-WF8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Qpdw3SW54kI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/lC2uPDfaTcE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1i16s8W_OV0Hd3-PIuo64JKDwwdOesgXQ) | 02.09.2024 |
| BiRefNet | Bilateral reference framework for high-resolution dichotomous image segmentation | - [Peng Zheng](https://zhengpeng7.github.io/about/)<br>- [Dehong Gao](https://teacher.nwpu.edu.cn/dehonggao)<br>- [Deng-Ping Fan](https://dengpingfan.github.io/)<br>- [Li Liu](https://scholar.google.com/citations?user=9cMQrVsAAAAJ)<br>others[Jorma Laaksonen](https://scholar.google.com/citations?user=qQP6WXIAAAAJ)<br>[Wanli Ouyang](https://wlouyang.github.io/)<br>[Nicu Sebe](https://disi.unitn.it/~sebe/) | [![](https://camo.githubusercontent.com/7a0cb7053bbe5f72bb637466230943bd79cc3f059f408a771639fcff9bd6c0c6/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e32363539392f4149522e323032342e39313530303338)](https://doi.org/10.26599/AIR.2024.9150038)[![](https://camo.githubusercontent.com/f3978c582c00ad63a5a2e5022f859c1cb8a946176a1c88d930650423743207fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5a68656e6750656e67372f42695265664e65743f7374796c653d736f6369616c)](https://github.com/ZhengPeng7/BiRefNet)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2401.03407), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2302.14485)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/d9NN5sgFrq)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Kazuhito00/BiRefNet-ONNX-Sample), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/viperyl/ComfyUI-BiRefNet)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/ZhengPeng7/BiRefNet)<br>- [project](https://www.birefnet.top/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te1?p=bilateral-reference-for-high-resolution), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/camouflaged-object-segmentation-on-cod?p=bilateral-reference-for-high-resolution), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/rgb-salient-object-detection-on-davis-s?p=bilateral-reference-for-high-resolution) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK) | 23.08.2024 |
| SPIN | Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop | - [Nikos Kolotouros](https://www.nikoskolot.com/)<br>- [Georgios Pavlakos](https://geopavlakos.github.io/)<br>- [Michael Black](https://ps.is.mpg.de/~black)<br>- [Kostas Daniilidis](https://www.cis.upenn.edu/~kostas/) | [![](https://camo.githubusercontent.com/1e1af2ac3b04fbb713c932446c2b14278d2053cb36fef10d7780842a1c49024d/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343562e323031392e3030323334)](https://doi.org/10.1109/ICCV.2019.00234)[![](https://camo.githubusercontent.com/fc841f39f429957817f3e4ec6fba3db0642f4e536d46c107459c620d6d1b8138/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e6b6f6c6f742f5350494e3f7374796c653d736f6369616c)](https://github.com/nkolot/SPIN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.12828)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/chaneyk/spin)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vchoutas/smplify-x), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CMU-Perceptual-Computing-Lab/openpose)<br>- [project](https://www.nikoskolot.com/projects/spin/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1uH2JtavOtDrFl6RsipyIncCSr19GWW4x) | 21.08.2024 |
| YOLOv10 | Aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture | - [Ao Wang](https://github.com/jameslahm)<br>- [Hui Chen](https://huichen24.github.io/)<br>- [Kai Chen](https://scholar.google.com/citations?user=bZQX708AAAAJ)<br>- [Zijia Lin](https://sites.google.com/site/linzijia72)<br>others[Jungong Han](https://jungonghan.github.io/)<br>[Guiguang Ding](https://scholar.google.com/citations?user=B7F3yt4AAAAJ) | [![](https://camo.githubusercontent.com/64311ea4b42dfd7858489a772f1384ff46d8a9301ef1ee04b28a38705888e475/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5448552d4d49472f796f6c6f7631303f7374796c653d736f6369616c)](https://github.com/THU-MIG/yolov10) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2405.14458)<br>- [blog post](https://learnopencv.com/yolov10/)<br>- [demo](https://openbayes.com/console/public/tutorials/im29uYrnIoz)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rlggyp/YOLOv10-OpenVINO-CPP-Inference), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Seeed-Projects/jetson-examples/blob/main/reComputer/scripts/yolov10/README.md), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kaylorchen/rk3588-yolo-demo), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/yolov10-optimization/yolov10-optimization.ipynb), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sujanshresstha/YOLOv10_DeepSORT), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CVHub520/X-AnyLabeling), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DanielSarmiento04/yolov10cpp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lyuwenyu/RT-DETR)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/collections/jameslahm/yolov10-665b0d90b0b5bb85129460c2), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/jameslahm/YOLOv10), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/kadirnar/Yolov10), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/Xenova/yolov10-web)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@batuhansenerr/yolov10-custom-object-detection-bd7298ddbfd3), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@sunidhi.ashtekar/yolov10-revolutionizing-real-time-object-detection-72ef04ad441a)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/GPTFutureScience/comments/1d34rj1/yolov10_the_future_of_realtime_object_detection/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/29tnSxhB3CY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/2ZFJbeJXXDM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/wM6nO75keOQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb) | 20.08.2024 |
| SpecVQGAN | Taming the visually guided sound generation by shrinking a training dataset to a set of representative vectors | - [Vladimir Iashin](https://iashin.ai/)<br>- [Esa Rahtu](https://esa.rahtu.fi/) | [![](https://camo.githubusercontent.com/0f62a82894590dff1fb32248243640074334240076d3e11d50fdb0c69a54fca6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f762d69617368696e2f53706563565147414e3f7374796c653d736f6369616c)](https://github.com/v-iashin/SpecVQGAN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/2110.08791), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.09841), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1711.00937), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2008.00820), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1712.01393), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1512.08512)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PeihaoChen/regnet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/toshas/torch-fidelity), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/descriptinc/melgan-neurips), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google/lyra)<br>- [project](https://iashin.ai/SpecVQGAN)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Foley_(filmmaking)), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Row-_and_column-major_order), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=Bucb3nAa398) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1pxTIMweAKApJZ3ZFqyBee3HtMqFpnwQ0) | 12.07.2024 |
| LivePortrait | Video-driven portrait animation framework with a focus on better generalization, controllability, and efficiency for practical usage | - [Jianzhu Guo](https://guojianzhu.com/)<br>- [Dingyun Zhang](https://github.com/DingyunZhang)<br>- [Xiaoqiang Liu](https://github.com/Liu-lxq)<br>- [Zhizhou Zhong](https://scholar.google.com/citations?user=t88nyvsAAAAJ)<br>others[Yuan Zhang](https://scholar.google.com/citations?user=_8k1ubAAAAAJ)<br>[Pengfei Wan](https://scholar.google.com/citations?user=P6MraaYAAAAJ)<br>[Di Zhang](https://openreview.net/profile?id=~Di_ZHANG3) | [![](https://camo.githubusercontent.com/e63a1f1c8107492c02a825df8e393ef92341ff8790798a925dffa231439e4713/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b7761695647492f4c697665506f7274726169743f7374796c653d736f6369616c)](https://github.com/KwaiVGI/LivePortrait)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2407.03168)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kijai/ComfyUI-LivePortraitKJ), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/shadowcz007/comfyui-liveportrait), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/SPADE), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepinsight/insightface)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/KwaiVGI/LivePortrait)<br>- [project](https://liveportrait.github.io/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1dvepjx/liveportrait_efficient_portrait_animation_with/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uyjSTAOY7yI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8-IcDDmiUMM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/aFcS31OWMjE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bRHf2oQwgG4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/FPtpNrmuwXk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/wG7oPp01COg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/LivePortrait-jupyter/blob/main/LivePortrait_jupyter.ipynb) | 10.07.2024 |
| Wav2Lip | A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild | - [Prajwal Renukanand](https://github.com/prajwalkr)<br>- [Rudrabha Mukhopadhyay](https://rudrabha.github.io/)<br>- [Vinay Namboodiri](https://vinaypn.github.io/)<br>- [C. V. Jawahar](https://faculty.iiit.ac.in/~jawahar/) | [![](https://camo.githubusercontent.com/176527e53ddf24ca103e9832a1d61c873e675252a20a7d2d2378f4fb4f9a36d7/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333339343137312e33343133353332)](https://doi.org/10.1145/3394171.3413532) [![](https://camo.githubusercontent.com/d68510333ad1a72f1c2e3af00f60d893c568a8ed3ff20bb25447d0c175f8ca9c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f52756472616268612f576176324c69703f7374796c653d736f6369616c)](https://github.com/Rudrabha/Wav2Lip) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2008.10010)<br>- [data](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)<br>- [demo](http://bhaasha.iiit.ac.in/lipsync/)<br>- [project](http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=0fXaDCZNOJc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/eyaler/avatars4all/blob/master/melaflefon.ipynb) | 27.06.2024 |
| FELIX | Feature Engineering with LLMs for Interpretability and Explainability, a novel approach harnessing the vast world knowledge embedded in pre-trained Large Language Models to automatically generate a set of features describing the data | - [Simon Malberg](https://github.com/simonmalberg)<br>- [Edoardo Mosca](https://edoardomosca.github.io/)<br>- [Georg Groh](https://socvm1.cit.tum.de/) | [![](https://camo.githubusercontent.com/b706c90598f0806295bb44ca235c8d98082be0b0fddab26fae1c10dfe2ed0c05/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d37303335392d315f3134)](https://doi.org/10.1007/978-3-031-70359-1_14)[![](https://camo.githubusercontent.com/1442e7e9597e2669d1127e6e4f833fceafe83e073c4c462b99254e1288d1246c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73696d6f6e6d616c626572672f66656c69783f7374796c653d736f6369616c)](https://github.com/simonmalberg/felix) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/simonmalberg/felix/blob/main/Paper/FELIX.ipynb) | 13.06.2024 |
| PoolFormer | MetaFormer Is Actually What You Need for Vision | - [Weihao Yu](https://whyu.me/)<br>- [Mi Luo](https://luomi97.github.io/)<br>- [Pan Zhou](https://panzhous.github.io/)<br>- [Chenyang Si](https://github.com/ChenyangSi)<br>others[Yichen Zhou](https://dblp.org/pid/55/10422.html)<br>[Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)<br>[Jiashi Feng](https://sites.google.com/site/jshfeng/)<br>[Shuicheng Yan](https://yanshuicheng.ai/) | [![](https://camo.githubusercontent.com/9fe7e400167b685a5bd5017dc04f1e409ea2462f014414e8698d16ef6c65b8ae/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031303535)](https://doi.org/10.1109/CVPR52688.2022.01055)[![](https://camo.githubusercontent.com/c4e3927e0b8f67a6093474f5fb883d3ffca6215ed72e4b16463d5f4b25f88465/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7361696c2d73672f706f6f6c666f726d65723f7374796c653d736f6369616c)](https://github.com/sail-sg/poolformer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.11418)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rwightman/pytorch-image-models), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/fvcore), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/apex)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/poolformer) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/sail-sg/poolformer/blob/main/misc/poolformer_demo.ipynb) | 01.06.2024 |
| StoryDiffusion | Way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner | - [Yupeng Zhou](https://mmcheng.net/zyp/)<br>- [Daquan Zhou](https://github.com/zhoudaquan)<br>- [Ming-Ming Cheng](https://mmcheng.net/cmm/)<br>- [Jiashi Feng](https://sites.google.com/site/jshfeng/?pli=1)<br>- [Qibin Hou](https://houqb.github.io/) | [![](https://camo.githubusercontent.com/795aaedbd52c1f35326263c353b4e596143a9de5bd8d023edc5f3f4b53b064f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f48566973696f6e2d4e4b552f53746f7279446966667573696f6e3f7374796c653d736f6369616c)](https://github.com/HVision-NKU/StoryDiffusion) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2405.01434)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://youtu.be/GeNyP4VY9rE?si=qW1jcW_GbKutmKQv)<br>- [project](https://storydiffusion.github.io/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StoryDiffusion/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/jZWRENqCl6I), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/GeNyP4VY9rE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/HVision-NKU/StoryDiffusion/blob/main/Comic_Generation.ipynb) | 04.05.2024 |
| FILM | A frame interpolation algorithm that synthesizes multiple intermediate frames from two input images with large in-between motion | - [Fitsum Reda](https://fitsumreda.github.io/)<br>- [Janne Kontkanen](https://scholar.google.com/citations?user=MnXc4JQAAAAJ)<br>- [Eric Tabellion](http://www.tabellion.org/et/)<br>- [Deqing Sun](https://deqings.github.io/)<br>others[Caroline Pantofaru](https://scholar.google.com/citations?user=vKAKE1gAAAAJ)<br>[Brian Curless](https://homes.cs.washington.edu/~curless/) | [![](https://camo.githubusercontent.com/4cc5859bc145cd2612a5a6f3b030605bf6aac3cfae9b9db333d473815db2db7f/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d32303037312d375f3135)](https://doi.org/10.1007/978-3-031-20071-7_15) [![](https://camo.githubusercontent.com/a1c4058b2f64f6a2adc80fcfa0379bcd3a40e3794344080b7eaef6d57367de12/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f6672616d652d696e746572706f6c6174696f6e3f7374796c653d736f6369616c)](https://github.com/google-research/frame-interpolation) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.04901)<br>- [data](http://data.csail.mit.edu/tofu/testset/vimeo_interp_test.zip), [data](https://vision.middlebury.edu/flow/data), [data](https://people.cs.umass.edu/~hzjiang/projects/superslomo/UCF101_results.zip)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sniklaus/softmax-splatting/blob/master/benchmark.py)<br>- [project](https://film-net.github.io/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/load_data/tfrecord), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/train/Example), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/saved_model)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/OAD-BieIjH4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1sK0uc-GJxmdnaxHhYqD2afRknakpdTNZ) | 03.05.2024 |
| VoiceCraft | token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech on audiobooks, internet videos, and podcasts | - [Puyuan Peng](https://jasonppy.github.io/)<br>- [Po-Yao Huang](https://berniebear.github.io/)<br>- [Shang-Wen Li](https://swdanielli.github.io/)<br>- [Abdelrahman Mohamed](https://www.cs.toronto.edu/~asamir/)<br>- [David Harwath](https://www.cs.utexas.edu/~harwath/) | [![](https://camo.githubusercontent.com/3223deca49f22c00f3acb4f8ead2ebc82d8b2978f79e6495ce51a5731a8afbdd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a61736f6e7070792f566f69636543726166743f7374796c653d736f6369616c)](https://github.com/jasonppy/VoiceCraft) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2403.16973)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lifeiteng/vall-e)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/pyp1/VoiceCraft)<br>- [project](https://jasonppy.github.io/VoiceCraft_web/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/LocalLLaMA/comments/1bmxfk3/voicecraft_zeroshot_speech_editing_and/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/eikybOi8iwU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PJ2qSjycLcw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/JxRrHpq-hys) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jasonppy/VoiceCraft/blob/master/voicecraft-gradio-colab.ipynb) | 21.04.2024 |
| ZeST | Method for zero-shot material transfer to an object in the input image given a material exemplar image | - [Ta-Ying Cheng](https://ttchengab.github.io/)<br>- [Prafull Sharma](https://prafullsharma.net/)<br>- [Andrew Markham](https://www.cs.ox.ac.uk/people/andrew.markham/)<br>- [Niki Trigoni](https://www.cs.ox.ac.uk/people/niki.trigoni/)<br>- [Varun Jampani](https://varunjampani.github.io/) | [![](https://camo.githubusercontent.com/f5b88a88a7673656326add35003bd0bfebcc2ad63ad34b6f64b23093d9cb77ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74746368656e6761622f7a6573745f636f64653f7374796c653d736f6369616c)](https://github.com/ttchengab/zest_code)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2404.06425)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kealiu/ComfyUI-ZeroShot-MTrans)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/h94/IP-Adapter), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://xthemadgenius.medium.com/zest-unlocks-material-magic-in-single-image-transfers-05f7ff7ee483)<br>- [project](https://ttchengab.github.io/zest/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/learnmachinelearning/comments/1c0wpjd/zest_zeroshot_material_transfer_from_a_single/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/atG1VvgeG_g) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/zest-jupyter/blob/main/zest_jupyter.ipynb) | 16.04.2024 |
| InstantMesh | Feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability | - [Jiale Xu](https://github.com/bluestyle97)<br>- [Weihao Cheng](https://www.cheng.website/)<br>- [Yiming Gao](https://scholar.google.com/citations?user=uRCc-McAAAAJ)<br>- [Xintao Wang](https://xinntao.github.io/)<br>others[Shenghua Gao](https://scholar.google.com/citations?user=fe-1v0MAAAAJ)<br>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ) | [![](https://camo.githubusercontent.com/d44afddfbfe8a1b47c40008e4258e47fa3f5cf5c16ad4c8703570475d7c4aeb8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f54656e63656e744152432f496e7374616e744d6573683f7374796c653d736f6369616c)](https://github.com/TencentARC/InstantMesh)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2404.07191)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/danielgatis/rembg), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/3DTopia/OpenLRM), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nv-tlabs/FlexiCubes)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/TencentARC/InstantMesh)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1c5hs3e/instantmesh_efficient_3d_mesh_generation_from_a/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BvngSJOStvQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/InstantMesh-jupyter/blob/main/InstantMesh_jupyter.ipynb) | 16.04.2024 |
| Würstchen | Architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models | - [Pablo Pernias](https://github.com/pabloppp)<br>- [Dominic Rampas](https://github.com/dome272)<br>- [Mats Richter](https://scholar.google.com/citations?user=xtlV5SAAAAAJ)<br>- [Christopher Pal](https://www.polymtl.ca/expertises/pal-christopher-j)<br>- [Marc Aubreville](https://lme.tf.fau.de/person/aubreville/) | [![](https://camo.githubusercontent.com/26a8d24a7f87c6b0e08f7e37dd9e2ebad6ec266085cc5ae919cdabc6db3ccfa8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646f6d653237322f7775657273746368656e3f7374796c653d736f6369616c)](https://github.com/dome272/wuerstchen)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.00637)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/blog/wuerstchen)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/16hsklt/w%C3%BCrstchen_is_here_a_game_changing_fastest/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ogJsCPqgFMk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/dome272/Wuerstchen/blob/main/w%C3%BCrstchen-stage-C.ipynb) | 06.04.2024 |
| AudioSep | Foundation model for open-domain audio source separation with natural language queries | - [Xubo Liu](https://liuxubo717.github.io/)<br>- [Qiuqiang Kong](https://qiuqiangkong.github.io/)<br>- [Yan Zhao](https://cliffzhao.github.io/)<br>- [Haohe Liu](https://haoheliu.github.io/)<br>others[Yi Yuan](https://www.surrey.ac.uk/people/yi-yuan)<br>[Yuzhuo Liu](https://github.com/redrabbit94)<br>[Rui Xia](https://scholar.google.co.uk/citations?user=26oErxwAAAAJ)<br>[Yuxuan Wang](https://scholar.google.com/citations?user=3RaOfJkAAAAJ)<br>[Mark Plumbley](https://www.surrey.ac.uk/people/mark-plumbley)<br>[Wenwu Wang](http://personal.ee.surrey.ac.uk/Personal/W.Wang/) | [![](https://camo.githubusercontent.com/07115de0e34a8e2cf9f2b5f5d8bdbf7cfccf1a1a2d07abc3afe2695e72fa51a5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f417564696f2d4147492f417564696f5365703f7374796c653d736f6369616c)](https://github.com/Audio-AGI/AudioSep) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2308.05037)<br>- [project](https://audio-agi.github.io/Separate-Anything-You-Describe/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Audio-AGI/AudioSep/blob/main/AudioSep_Colab.ipynb) | 15.03.2024 |
| AQLM | Extreme Compression of Large Language Models via Additive Quantization | - [Vage Egiazarian](https://github.com/Vahe1994)<br>- [Andrei Panferov](https://blog.panferov.org/)<br>- [Denis Kuznedelev](https://github.com/Godofnothing)<br>- [Elias Frantar](https://efrantar.github.io/)<br>others[Artem Babenko](https://scholar.google.com/citations?user=2Kv3JP0AAAAJ)<br>[Dan Alistarh](https://github.com/dalistarh) | [![](https://camo.githubusercontent.com/ed59cdda9087359b1373b556547188f806b2bdc9fe76910c50b8ef5257d153e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f56616865313939342f41514c4d3f7374796c653d736f6369616c)](https://github.com/Vahe1994/AQLM)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2401.06118)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/datasets/main/en/cache#cache-directory), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/Vahe1994/AQLM)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/LearningMachines/comments/1atvrnl/240106118_extreme_compression_of_large_language/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Qx8PNk4OkUA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hAHBKAXO-88) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Vahe1994/AQLM/blob/main/notebooks/colab_example.ipynb) | 08.03.2024 |
| YOLOv9 | Learning What You Want to Learn Using Programmable Gradient Information | - [Chien-Yao Wang](https://scholar.google.com/citations?user=DkQh4M4AAAAJ)<br>- [I-Hau Yeh](https://ieeexplore.ieee.org/author/37088448531)<br>- [Hong-Yuan Mark Liao](https://homepage.iis.sinica.edu.tw/pages/liao/index_zh.html) | [![](https://camo.githubusercontent.com/552a3c7c05050b066878c86eb77a49337c9ab68cbbc50a6bd0bf40958c5ae54c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f576f6e674b696e5969752f796f6c6f76393f7374796c653d736f6369616c)](https://github.com/WongKinYiu/yolov9)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2402.13616), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.16921)<br>- [blog post](https://learnopencv.com/yolov9-advancing-the-yolo-legacy/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/WongKinYiu/yolor), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/VDIGPKU/DynamicDet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DingXiaoH/RepVGG)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/kadirnar/Yolov9), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/merve/yolov9)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@Mert.A/how-to-use-yolov9-for-object-detection-93598ad88d7d)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/XHT2c8jT3Bc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3iLJ6YWPg28), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dccf_sJF0Gg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb) | 05.03.2024 |
| Multi-LoRA Composition | LoRA Switch and LoRA Composite, approaches that aim to surpass traditional techniques in terms of accuracy and image quality, especially in complex compositions | - [Ming Zhong](https://maszhongming.github.io/)<br>- [Yelong Shen](https://scholar.google.com/citations?user=S6OFEFEAAAAJ)<br>- [Shuohang Wang](https://www.microsoft.com/en-us/research/people/shuowa/)<br>- [Yadong Lu](https://adamlu123.github.io/)<br>others[Yizhu Jiao](https://yzjiao.github.io/)<br>[Siru Ouyang](https://ozyyshr.github.io/)<br>[Donghan Yu](https://plusross.github.io/)<br>[Jiawei Han](https://hanj.cs.illinois.edu/)<br>[Weizhu Chen](https://www.microsoft.com/en-us/research/people/wzchen/) | [![](https://camo.githubusercontent.com/fc026bd2ae70992f66b0bc1fe188094dd4dff48905d5ca54c6df8448865faa2e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d61737a686f6e676d696e672f4d756c74692d4c6f52412d436f6d706f736974696f6e3f7374796c653d736f6369616c)](https://github.com/maszhongming/Multi-LoRA-Composition) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2402.16843)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@letscodeai/multi-lora-composition-for-image-generation-f2706528c590)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/ninjasaid13/comments/1b13q8s/multilora_composition_for_image_generation/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://x.com/MingZhong_/status/1762347881812443575?s=20)<br>- [website](https://maszhongming.github.io/Multi-LoRA-Composition/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1eSTj6qGOtSY5NaazwwN3meXOzEZxgaZq) | 03.03.2024 |
| AMARETTO | Multiscale and multimodal inference of regulatory networks to identify cell circuits and their drivers shared and distinct within and across biological systems of human disease | - [Nathalie Pochet](http://portals.broadinstitute.org/pochetlab/)<br>- [Olivier Gevaert](https://profiles.stanford.edu/olivier-gevaert)<br>- [Mohsen Nabian](https://github.com/monabiyan)<br>- [Jayendra Shinde](https://jayendrashinde91.github.io/)<br>others[Celine Everaert](http://www.crig.ugent.be/en/node/510)<br>[Thorin Tabor](http://thorin.tabcreations.com/) | [![](https://camo.githubusercontent.com/5e63cf03cc47692df43eba033dcefb2c44e7de3542afec2dcbca441a81e3f0d7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676576616572746c61622f414d41524554544f3f7374796c653d736f6369616c)](https://github.com/gevaertlab/AMARETTO) <br>- [bioconductor](https://bioconductor.org/packages/release/bioc/html/AMARETTO.html)<br>- [project](http://portals.broadinstitute.org/pochetlab/amaretto.html) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1JfnRoNgTVX_7VEGAAmjGjwP_yX2tdDxs) | 28.02.2024 |
| LIDA | Tool for generating grammar-agnostic visualizations and infographics | [Victor Dibia](https://victordibia.com/) | [![](https://camo.githubusercontent.com/2cc6cd49bab6c69da9d7f7d4e5a0654f908f248707b040560a6cc087db50a808/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f323032332e61636c2d64656d6f2e3131)](https://doi.org/10.18653/v1/2023.acl-demo.11)[![](https://camo.githubusercontent.com/97009854e8fe8709a5b552db9156d94338fb6cd0aa2b0120ce710a63850fc020/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f6c6964613f7374796c653d736f6369616c)](https://github.com/microsoft/lida)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.02927)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/victordibia/llmx), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lida-project/lida-streamlit)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@c17hawke/lida-automatically-generate-visualization-and-with-llms-the-future-of-data-visualization-6bc556876b46)<br>- [project](https://microsoft.github.io/lida/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/exYi9W-dhME), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/U9K1Cu45nMQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/6xcCwlDx6f8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/microsoft/lida/blob/main/notebooks/tutorial.ipynb) | 06.02.2024 |
| ViT | Vision Transformer and MLP-Mixer Architectures | - [Alexey Dosovitskiy](https://scholar.google.com/citations?user=FXNJRDoAAAAJ)<br>- [Lucas Beyer](http://lucasb.eyer.be/)<br>- [Alexander Kolesnikov](https://github.com/akolesnikoff)<br>- [Dirk Weissenborn](https://github.com/dirkweissenborn)<br>others[Xiaohua Zhai](https://github.com/xiaohuazhai)<br>[Thomas Unterthiner](https://github.com/untom)<br>[Mostafa Dehghani](https://www.mostafadehghani.com/)<br>[Matthias Minderer](https://matthias.minderer.net/)<br>[Georg Heigold](https://scholar.google.com/citations?user=WwqlChAAAAAJ)<br>[Sylvain Gelly](https://scholar.google.com/citations?user=m7LvuTkAAAAJ)<br>[Jakob Uszkoreit](https://scholar.google.com/citations?user=mOG0bwsAAAAJ)<br>[Neil Houlsby](https://neilhoulsby.github.io/) | [![](https://camo.githubusercontent.com/9d0a1e9645ecddd42eaf41162563e137931b28fdad9d7493f0ab17bc88cf079f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f766973696f6e5f7472616e73666f726d65723f7374796c653d736f6369616c)](https://github.com/google-research/vision_transformer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.11929), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.01601), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.01601), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.10270), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.01548), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.07991), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.08065)<br>- [blog post](https://blog.research.google/2022/04/locked-image-tuning-adding-language.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/huggingface/pytorch-image-models), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google/flaxformer)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/models)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@weiwen21/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-957f88e53726)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TrdevFK_am4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/HZ4j_U3FC94), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7K4Z8RqjWIk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/oDtcobGQ7xU?si=C2EgZTESzhTXFSq6), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/v6xj_DG-UEo) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb) | 06.02.2024 |
| Qwen | Comprehensive language model series that encompasses distinct models with varying parameter counts | [qwenlm](https://qwenlm.github.io/) | [![](https://camo.githubusercontent.com/bed1c796d00202633d29383062ecebcc8e3881693db098ca081d924b679bd01a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5177656e4c4d2f5177656e3f7374796c653d736f6369616c)](https://github.com/QwenLM/Qwen)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.16609), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.09685), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.14314), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2307.11088)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/CV4E9rpNSD)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/qwenllm/qwen)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/QwenLM/Qwen2.5), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/QwenLM/Qwen-Agent), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/QwenLM/qwen.cpp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Dao-AILab/flash-attention), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AutoGPTQ/AutoGPTQ), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/DeepSpeed)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/Qwen)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/docs/stable/elastic/run.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/y6Wh4SpRoao), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bJmx_fAOW78), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ALArhCnz8rY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/QwenLM/Qwen/blob/master/recipes/quickstart/qwen.ipynb) | 30.01.2024 |
| 3D Ken Burns | A reference implementation of 3D Ken Burns Effect from a Single Image using PyTorch - given a single input image, it animates this still image with a virtual camera scan and zoom subject to motion parallax | [Manuel Romero](https://mrm8488.github.io/) | [![](https://camo.githubusercontent.com/2d13b07c4e0756d2bb2758d737c80e17c6a03c2006e7bbe1a667e1044566d601/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333335353038392e33333536353238)](https://doi.org/10.1145/3355089.3356528)[![](https://camo.githubusercontent.com/7e395c344a30b94c8f00ad45b408e94bdc7bbb9cdf343cf7edfbf2bd905bab06/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f736e696b6c6175732f33642d6b656e2d6275726e733f7374796c653d736f6369616c)](https://github.com/sniklaus/3d-ken-burns)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.05483)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=WrajxHHfRBA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/3D_Ken_Burns.ipynb) | 24.01.2024 |
| VALL-E X | Cross-lingual neural codec language model for cross-lingual speech synthesis | - [Ziqiang Zhang](https://github.com/onisac-K)<br>- [Long Zhou](https://long-zhou.github.io/)<br>- [Chengyi Wang](https://cywang97.github.io/)<br>- [Sanyuan Chen](https://sanyuan-chen.github.io/)<br>others[Yu Wu](https://www.microsoft.com/en-us/research/people/yuwu1/)<br>[Shujie Liu](https://www.microsoft.com/en-us/research/people/shujliu/)<br>[Zhuo Chen](https://www.microsoft.com/en-us/research/people/zhuc/)<br>[Yanqing Liu](https://scholar.google.com/citations?user=dIJFz4UAAAAJ)<br>[Huaming Wang](https://scholar.google.com/citations?user=aJDLg5IAAAAJ)<br>[Jinyu Li](https://www.microsoft.com/en-us/research/people/jinyli/)<br>[Lei He](https://scholar.google.com/citations?user=EKl9yY8AAAAJ)<br>[Sheng Zhao](https://scholar.google.com/citations?user=689bIIwAAAAJ)<br>[Furu Wei](https://www.microsoft.com/en-us/research/people/fuwei/) | [![](https://camo.githubusercontent.com/cc023fb6ecf2dbdc00be79ed84ff9319513f45751755a75a5b18f69e47e6cfba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f506c6163687461612f56414c4c2d452d583f7374796c653d736f6369616c)](https://github.com/Plachtaa/VALL-E-X)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.03926), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.02111), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.03143)<br>- [demo](https://plachtaa.github.io/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/qCBRmAnTxg)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lifeiteng/vall-e)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/Plachta/VALL-E-X)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/syncedreview/speak-a-foreign-language-in-your-own-voice-1dafa42f78d9)<br>- [project](https://www.microsoft.com/en-us/research/project/vall-e-x)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7qgfoVFQmvk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1yyD_sz531QntLKowMHo-XxorsFBCfKul) | 19.01.2024 |
| PhotoMaker | Efficient personalized text-to-image generation method, which mainly encodes an arbitrary number of input ID images into a stack ID embedding for preserving ID information | - [Zhen Li](https://paper99.github.io/)<br>- [Mingdeng Cao](https://github.com/ljzycmd)<br>- [Xintao Wang](https://xinntao.github.io/)<br>- [Zhongang Qi](https://scholar.google.com/citations?user=zJvrrusAAAAJ)<br>others[Ming-Ming Cheng](https://mmcheng.net/cmm/)<br>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ) | [![](https://camo.githubusercontent.com/b67ec39c7b9329bc46ad14236676c837adf4195fdd585027117da2e7d1a58179/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f54656e63656e744152432f50686f746f4d616b65723f7374796c653d736f6369616c)](https://github.com/TencentARC/PhotoMaker)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2312.04461)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/bmaltais/PhotoMaker), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sdbds/PhotoMaker-for-windows), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ZHO-ZHO-ZHO/ComfyUI-PhotoMaker), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mit-han-lab/fastcomposer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TencentARC/T2I-Adapter), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tencent-ailab/IP-Adapter)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/TencentARC/PhotoMaker)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@christopheverdier/photomaker-the-art-of-ai-consistent-characters-generation-cf2cd037bc3e)<br>- [project](https://photo-maker.github.io/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/197bfj9/tencentarc_releases_photomaker/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/NWIdzTEk5O4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ZTck128jfFY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/TencentARC/PhotoMaker/blob/main/photomaker_demo.ipynb) | 18.01.2024 |
| DDColor | End-to-end method with dual decoders for image colorization | - [Xiaoyang Kang](https://piddnad.github.io/xiaoyangkang)<br>- [Tao Yang](https://cg.cs.tsinghua.edu.cn/people/~tyang/)<br>- [Wenqi Ouyang](https://vicky0522.github.io/Wenqi-Ouyang/)<br>- [Peiran Ren](https://scholar.google.com/citations?user=x5dEuxsAAAAJ)<br>others[Lingzhi Li](https://lingzhili.com/)<br>[Xuansong Xie](https://github.com/xungie) | [![](https://camo.githubusercontent.com/bd23d10babb4b1d180ae86f280ea953e9e9b470bd537cf204b41e8d05d713a88/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f706964646e61642f4444436f6c6f723f7374796c653d736f6369616c)](https://github.com/piddnad/DDColor)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.11613)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jixiaozhong/ColorFormer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/KIMGEONUNG/BigColor) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/DDColor-colab/blob/main/DDColor_colab.ipynb) | 15.01.2024 |
| PASD | Pixel-aware stable diffusion network to achieve robust Real-ISR as well as personalized stylization | - [Tao Yang](https://cg.cs.tsinghua.edu.cn/people/~tyang)<br>- [Peiran Ren](http://renpr.org/)<br>- [Xuansong Xie](https://github.com/xungie)<br>- [Lei Zhang](https://www4.comp.polyu.edu.hk/~cslzhang) | [![](https://camo.githubusercontent.com/23aa4dab9f5c75f161b0e4dcd9f4c393e823414bec0fdef75f7187ec1eba18fe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f79616e6778792f504153443f7374796c653d736f6369616c)](https://github.com/yangxy/PASD)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2308.14469)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/runwayml/stable-diffusion-v1-5), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/nitrosocke/mo-di-diffusion)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/18qxe5q/pixelaware_stable_diffusion_for_realistic_image/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1lZ_-rSGcmreLCiRniVT973x6JLjFiC-b) | 12.01.2024 |
| HandRefiner | Refining Malformed Hands in Generated Images by Diffusion-based Conditional Inpainting | - [Wenquan Lu](https://github.com/wenquanlu)<br>- [Yufei Xu](https://scholar.google.com/citations?user=hlYWxX8AAAAJ)<br>- [Jing Zhang](https://scholar.google.com/citations?user=9jH5v74AAAAJ)<br>- [Chaoyue Wang](https://wang-chaoyue.github.io/)<br>- [Dacheng Tao](https://scholar.google.com/citations?user=RwlJNLcAAAAJ) | [![](https://camo.githubusercontent.com/296a2a5a47952884d6e98d0eb4a600b5a7bce9b80a5114e035facd9ebe781115/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77656e7175616e6c752f48616e64526566696e65723f7374796c653d736f6369616c)](https://github.com/wenquanlu/HandRefiner)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2311.17957)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Fannovel16/comfyui_controlnet_aux), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Mikubill/sd-webui-controlnet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/MeshGraphormer)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1881z4v/handrefiner_refining_malformed_hands_in_generated/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Tt-Fyn1RA6c) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/HandRefiner-colab/blob/main/HandRefiner_colab.ipynb) | 08.01.2024 |
| ESM | Evolutionary Scale Modeling: Pretrained language models for proteins | - [Zeming Lin](https://research.facebook.com/people/lin-zeming/)<br>- [Roshan Rao](https://rmrao.github.io/)<br>- [Brian Hie](https://brianhie.com/)<br>- [Zhongkai Zhu](https://www.linkedin.com/in/zhongkai-zhu-03a27424)<br>others[Allan dos Santos Costa](https://scholar.google.com/citations?user=Zb4RsFsAAAAJ)<br>[Maryam Fazel-Zarandi](https://www.maryamfazel.com/)<br>[Tom Sercu](https://tom.sercu.me/)<br>[Salvatore Candido](https://scholar.google.com/citations?user=BDgbhmEAAAAJ)<br>[Alexander Rives](https://scholar.google.com/citations?user=vqb78-gAAAAJ)<br>[Joshua Meier](https://scholar.google.com/citations?user=2M0OltAAAAAJ)<br>[Robert Verkuil](https://dblp.org/pid/296/8930.html)<br>[Jason Liu](https://www.linkedin.com/in/liujiayi/)<br>[Chloe Hsu](https://chloe-hsu.com/)<br>[Adam Lerer](https://scholar.google.com/citations?user=Ad6O4-0AAAAJ) | [![](https://camo.githubusercontent.com/ba4f5726827d71e7282323b1866872225c40f5b98990e11ecc7d7b3b5699b3c3/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130312f363232383033)](https://doi.org/10.1101/622803) [![](https://camo.githubusercontent.com/d77145c2ced19152d9fd4a8c4e8db5d11e2368a8af8247d13f046a95b0fbd363/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f65736d3f7374796c653d736f6369616c)](https://github.com/facebookresearch/esm) <br>- [ESM Atlas](https://esmatlas.com/)<br>- [FSDP](https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html)<br>- [ICML](https://proceedings.mlr.press/v139/rao21a.html)<br>- [data](https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2018_03/uniref/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sokrypton/ColabFold)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/model_doc/esm)<br>- [paper](https://doi.org/10.1101/2022.07.20.500902), [paper](https://doi.org/10.1101/2021.07.09.450648), [paper](https://doi.org/10.1101/2022.04.10.487779), [paper](https://doi.org/10.1101/2022.12.21.521521)<br>- [pubmed](https://pubmed.ncbi.nlm.nih.gov/33876751/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/N-eisTvUYrk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/GHoE4VkDehY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/ESMFold.ipynb) | 28.12.2023 |
| LLaVA | Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding | - [Haotian Liu](https://hliu.cc/)<br>- [Chunyuan Li](https://chunyuan.li/)<br>- [Qingyang Wu](https://qywu.github.io/)<br>- [Yong Jae Lee](https://pages.cs.wisc.edu/~yongjaelee/)<br>- [Yuheng Li](https://yuheng-li.github.io/) | [![](https://camo.githubusercontent.com/4874aab4cd4a7666bfc46b91ff38a53b7d07279deffa86c89b7b589accf738df/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68616f7469616e2d6c69752f4c4c6156413f7374796c653d736f6369616c)](https://github.com/haotian-liu/LLaVA)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2304.08485), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2310.03744), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.00890), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.09958), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.14895)<br>- [demo](https://llava.hliu.cc/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ggerganov/llama.cpp/pull/3436), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/LLaVA-Med), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lm-sys/FastChat), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Luodian/Otter), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/liuhaotian/LLaVA-Pretrained-Projectors)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://xthemadgenius.medium.com/how-to-use-llava-large-language-and-vision-assistant-732c666b5ed0)<br>- [project](https://llava-vl.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/mkI7EPD1vp8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/kx1VpI6JzsY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/RxBSmbdJ1I8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/mdYycY4lsuE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/t7I46dxfmWs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/KRAQkJC-XJU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/LLaVA-colab/blob/main/LLaVA_13b_4bit_vanilla_colab.ipynb) | 22.12.2023 |
| Background Matting V2 | Real-time, high-resolution background replacement technique which operates at 30fps in 4K resolution, and 60fps for HD on a modern GPU | - [Shanchuan Lin](https://github.com/PeterL1n)<br>- [Andrey Ryabtsev](https://github.com/andreyryabtsev)<br>- [Soumyadip Sengupta](https://github.com/senguptaumd)<br>- [Brian Curless](https://homes.cs.washington.edu/~curless/)<br>others[Steve Seitz](https://www.smseitz.com/)<br>[Ira Kemelmacher-Shlizerman](https://www.irakemelmacher.com/) | [![](https://camo.githubusercontent.com/4a07479ed3c1e7dabc269b3615d52bae7b8363feca853c296afe474d861efc38/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3030383635)](https://doi.org/10.1109/CVPR46437.2021.00865)[![](https://camo.githubusercontent.com/4a4306fc29385653eccc5860afd9c5541cb5fe01ed0e223c699c7ab912694c8b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f50657465724c316e2f4261636b67726f756e644d617474696e6756323f7374796c653d736f6369616c)](https://github.com/PeterL1n/BackgroundMattingV2)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.07810)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/senguptaumd/Background-Matting), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/andreyryabtsev/BGMv2-webcam-plugin-linux)<br>- [project](https://grail.cs.washington.edu/projects/background-matting-v2/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/oMfPTeYDF9g), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/b7ps21MVyTA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1cTxFq1YuoJ5QPqaTcnskwlHDolnjBkB9) | 22.12.2023 |
| Gaussian Splatting | State-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 100 fps) novel-view synthesis at 1080p resolution | - [Bernhard Kerbl](https://www.cg.tuwien.ac.at/staff/BernhardKerbl)<br>- [Georgios Kopanas](https://grgkopanas.github.io/)<br>- [Thomas Leimkühler](https://people.mpi-inf.mpg.de/~tleimkue/)<br>- [George Drettakis](http://www-sop.inria.fr/members/George.Drettakis/) | [![](https://camo.githubusercontent.com/93a5d019e7b54e05da79b0fd8c1f346a35ad402f854723361819d208f335b2e6/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f33353932343333)](https://doi.org/10.1145/3592433) [![](https://camo.githubusercontent.com/b94c5090fbcb34935f6e34b42b0f1375f03174e634f760bf7c408063f9975e9c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67726170686465636f2d696e7269612f676175737369616e2d73706c617474696e673f7374796c653d736f6369616c)](https://github.com/graphdeco-inria/gaussian-splatting) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2308.04079)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/camenduru/gaussian-splatting)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/axinc-ai/3d-gaussian-splatting-real-time-rendering-of-photorealistic-scenes-f7f1a47f060)<br>- [project](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/singularity/comments/163jeqa/3d_gaussian_splatting_for_realtime_radiance_field/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/T_kXY43VZnk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/UXtuigy_wYc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/HVv_IQKlafQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/w43KV79LsFw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TLK3TDDcJFU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/kShNYOuDnlI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/juRMRej2d5c) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/gaussian-splatting-colab/blob/main/gaussian_splatting_colab.ipynb) | 19.12.2023 |
| SMPLer-X | Scaling up EHPS towards the first generalist foundation model, with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources | - [Zhongang Cai](https://caizhongang.github.io/)<br>- [Wanqi Yin](https://scholar.google.com/citations?user=zlIJwBEAAAAJ)<br>- [Ailing Zeng](https://ailingzeng.site/)<br>- [Chen Wei](https://github.com/Wei-Chen-hub)<br>others[Qingping Sun](https://github.com/ttxskk)<br>[Yanjun Wang](https://github.com/WYJSJTU)<br>[Hui En Pang](https://pangyyyyy.github.io/)<br>[Haiyi Mei](https://haiyi-mei.com/)<br>[Mingyuan Zhang](https://mingyuan-zhang.github.io/)<br>[Lei Zhang](https://www.leizhang.org/)<br>[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)<br>[Lei Yang](https://scholar.google.com/citations?user=jZH2IPYAAAAJ)<br>[Ziwei Liu](https://liuziwei7.github.io/) | [![](https://camo.githubusercontent.com/da57eec7fa62d8f66fca00f9ca10ecfe4fc21ecb84c4f234ef42a926fca441ad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6361697a686f6e67616e672f534d504c65722d583f7374796c653d736f6369616c)](https://github.com/caizhongang/SMPLer-X)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.17448)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmhuman3d/blob/main/docs/human_data.md), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mks0601/Hand4Whole_RELEASE), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IDEA-Research/OSX)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://neurips.cc/virtual/2023/poster/73473)<br>- [project](https://caizhongang.com/projects/SMPLer-X/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/machinelearningnews/comments/176c5z7/this_ai_research_proposes_smplerx_a_generalist/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/DepTqbPpVzY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/aFTGFInUnM4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/SMPLer-X-colab/blob/main/SMPLer_X_colab.ipynb) | 18.12.2023 |
| DeepCache | Training-free paradigm that accelerates diffusion models from the perspective of model architecture | - [Xinyin Ma](https://horseee.github.io/)<br>- [Gongfan Fang](https://fangggf.github.io/)<br>- [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/) | [![](https://camo.githubusercontent.com/cc46d8cc5530e9ece1b2fdda8c50bac18c42105bca5dc182b5efc81510bc643a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f686f72736565652f4465657043616368653f7374796c653d736f6369616c)](https://github.com/horseee/DeepCache) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2312.00858)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/diffusers/v0.24.0/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)<br>- [project](https://horseee.github.io/Diffusion_DeepCache/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/18b40hh/deepcache_accelerating_diffusion_models_for_free/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/DeepCache-colab/blob/main/DeepCache_colab.ipynb) | 18.12.2023 |
| MagicAnimate | Diffusion-based framework that aims at enhancing temporal consistency, preserving reference image faithfully, and improving animation fidelity | - [Zhongcong Xu](https://scholar.google.com/citations?user=-4iADzMAAAAJ)<br>- [Jianfeng Zhang](http://jeff95.me/)<br>- [Jun Hao Liew](https://scholar.google.com/citations?user=8gm-CYYAAAAJ)<br>- [Hanshu Yan](https://hanshuyan.github.io/)<br>others[Jiawei Liu](https://jia-wei-liu.github.io/)<br>[Chenxu Zhang](https://zhangchenxu528.github.io/)<br>[Jiashi Feng](https://sites.google.com/site/jshfeng/home)<br>[Mike Shou](https://sites.google.com/view/showlab) | [![](https://camo.githubusercontent.com/1c3fc6675f7847ec0c3a0c6caebc5a69d4f96da1fe05678fcd7491b4cb3d5ea6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d616769632d72657365617263682f6d616769632d616e696d6174653f7374796c653d736f6369616c)](https://github.com/magic-research/magic-animate)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2311.16498)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/zcxu-eric/MagicAnimate), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/runwayml/stable-diffusion-v1-5), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/sd-vae-ft-mse)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@AIWorldBlog/revolutionizing-image-animation-with-magicanimate-technology-78cc94151915)<br>- [project](https://showlab.github.io/magicanimate/)<br>- [website](https://www.magicanimate.org/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/td27SyA9M80), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1pATjLFvNtY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/HeXknItbMM8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/MagicAnimate-colab/blob/main/MagicAnimate_colab.ipynb) | 18.12.2023 |
| DiffBIR | Towards Blind Image Restoration with Generative Diffusion Prior | - [Xinqi Lin](https://github.com/0x3f3f3f3fun)<br>- [Jingwen He](https://github.com/hejingwenhejingwen)<br>- [Ziyan Chen](https://github.com/ziyannchen)<br>- [Zhaoyang Lyu](https://zhaoyanglyu.github.io/)<br>others[Ben Fei](https://scholar.google.com/citations?user=skQROj8AAAAJ)<br>[Bo Dai](http://daibo.info/)<br>[Wanli Ouyang](https://wlouyang.github.io/)<br>[Yu Qiao](https://mmlab.siat.ac.cn/yuqiao)<br>[Chao Dong](http://xpixel.group/2010/01/20/chaodong.html) | [![](https://camo.githubusercontent.com/540dbf4aa83c7ac60eaa8df55c2ef02054a298d0b83cb53f6358fde7659c0aa7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f58506978656c47726f75702f446966664249523f7374796c653d736f6369616c)](https://github.com/XPixelGroup/DiffBIR) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2308.15070)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/albarji/mixture-of-diffusers)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)<br>- [project](https://0x3f3f3f3fun.github.io/projects/diffbir/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rGnrpxWjBOg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MIRiJGuGqsg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/DiffBIR-colab/blob/main/DiffBIR_colab.ipynb) | 18.12.2023 |
| AudioLDM | Text-to-audio system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining latents | - [Haohe Liu](https://haoheliu.github.io/)<br>- [Zehua Chen](https://github.com/zehuachenImperial)<br>- [Yi Yuan](https://www.surrey.ac.uk/people/yi-yuan)<br>- [Xinhao Mei](https://xinhaomei.github.io/)<br>others[Xubo Liu](https://liuxubo717.github.io/)<br>[Danilo Mandic](https://www.imperial.ac.uk/people/d.mandic)<br>[Wenwu Wang](http://personal.ee.surrey.ac.uk/Personal/W.Wang/)<br>[Mark Plumbley](https://www.surrey.ac.uk/people/mark-plumbley) | [![](https://camo.githubusercontent.com/781f89336ffc08a375ce5594f40f19d65ec93ec66445c3d0c35246b01dd6b09d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68616f68656c69752f417564696f4c444d3f7374796c653d736f6369616c)](https://github.com/haoheliu/AudioLDM)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.12503)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/LAION-AI/CLAP), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CompVis/stable-diffusion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/toshas/torch-fidelity)<br>- [project](https://audioldm.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_0VTltNYhao) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/olaviinha/NeuralTextToAudio/blob/main/AudioLDM_pub.ipynb) | 02.12.2023 |
| TabPFN | Neural network that learned to do tabular data prediction | - [Noah Hollmann](https://github.com/noahho)<br>- [Samuel Müller](https://scholar.google.com/citations?user=pevYEjAAAAAJ)<br>- [Katharina Eggensperger](https://github.com/KEggensperger)<br>- [Frank Hutter](https://ml.informatik.uni-freiburg.de/profile/hutter/) | [![](https://camo.githubusercontent.com/bb23b472deef1236ff48f173cfcd5072a66d2ea02639a7eff1112314fa94c912/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6175746f6d6c2f54616250464e3f7374796c653d736f6369616c)](https://github.com/automl/TabPFN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.01848), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.11189), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.01342), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.03253), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.11189), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10510)<br>- [blog post](https://www.automl.org/tabpfn-a-transformer-that-solves-small-tabular-classification-problems-in-a-second/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/tunguz/status/1578730907711655937)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BGTO5N5-ack) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/194mCs6SEPEW6C0rcP7xWzcEtt1RBc8jJ) | 29.11.2023 |
| Concept Sliders | Plug-and-play low rank adaptors applied on top of pretrained models | - [Rohit Gandikota](https://rohitgandikota.github.io/)<br>- [Joanna Materzyńska](https://joaanna.github.io/)<br>- [Tingrui Zhou](https://www.p1at.dev/)<br>- [Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/)<br>- [David Bau](https://baulab.info/) | [![](https://camo.githubusercontent.com/b301fe93775ddd35aa9003af4d8754dd97aa86ead9fcc1ddd5215dbc0bb28235/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726f68697467616e64696b6f74612f736c69646572733f7374796c653d736f6369616c)](https://github.com/rohitgandikota/sliders)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2311.12092), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.12598)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@furkangozukara/concept-sliders-lora-adaptors-for-precise-control-in-diffusion-models-b7f6b36fabee)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2020/hash/49856ed476ad01fcff881d57e161d73f-Abstract.html)<br>- [project](https://sliders.baulab.info/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/180zon7/concept_sliders_lora_adaptors_for_precise_control/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/rohitgandikota/sliders/blob/main/demo_concept_sliders.ipynb) | 26.11.2023 |
| Qwen-VL | Set of large-scale vision-language models designed to perceive and understand both text and images | - [Jinze Bai](https://github.com/jinze1994)<br>- [Shuai Bai](https://github.com/ShuaiBai623)<br>- [Shusheng Yang](https://shushengyang.com/)<br>- [Shijie Wang](https://scholar.google.com/citations?user=DuAqyTwAAAAJ)<br>others[Sinan Tan](https://www.tinytangent.com/)<br>[Peng Wang](https://scholar.google.com/citations?user=7fjqA0YAAAAJ)<br>[Junyang Lin](https://justinlin610.github.io/)<br>[Chang Zhou](https://scholar.google.com/citations?user=QeSoG3sAAAAJ)<br>[Jingren Zhou](http://www.cs.columbia.edu/~jrzhou/) | [![](https://camo.githubusercontent.com/8a4484244796707bd7ef083205869e57889e271734702eb3a0ba7b050812d202/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5177656e4c4d2f5177656e2d564c3f7374796c653d736f6369616c)](https://github.com/QwenLM/Qwen-VL)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2308.12966), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.09685), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.14314)<br>- [demo](https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/z3GAxXZ9Ce)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/OFA-Sys/TouchStone), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PanQiWei/AutoGPTQ)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/Qwen/Qwen-VL)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ElrSJDg23Po), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/E3MS8GfGWj4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ju09YaO7BGA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/Qwen-VL-Chat-colab/blob/main/Qwen_VL_Chat_colab.ipynb) | 24.11.2023 |
| AnimeGANv3 | Double-tail generative adversarial network for fast photo animation | - [Gang Liu](https://github.com/lg0061408)<br>- [Xin Chen](https://github.com/TachibanaYoshino) | [![](https://camo.githubusercontent.com/49d3e35684b7d28563c7f34565263e32f7e551db2e16107d59aa67df6dcf9ea2/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313538372f7472616e73696e662e3230323345445037303631)](http://doi.org/10.1587/transinf.2023EDP7061) [![](https://camo.githubusercontent.com/3f8ac224ecc361750f0bf5a180a3432d56a31f46c4dd825c744f2f539e7fc6d2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f546163686962616e61596f7368696e6f2f416e696d6547414e76333f7374796c653d736f6369616c)](https://github.com/TachibanaYoshino/AnimeGANv3) <br>- [project](https://tachibanayoshino.github.io/AnimeGANv3/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/EosubeJmAnE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5qLUflWb45E), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/iFjiaPlhVm4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/vJqQQMRYKh0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0KaScDxgyBw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/6WXhjXb5a-o) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1XYNWwM8Xq-U7KaTOqNap6A-Yq1f-V-FB) | 23.11.2023 |
| Ithaca | First Deep Neural Network for the textual restoration, geographical and chronological attribution of ancient Greek inscriptions | - [Yannis Assael](https://www.assael.gr/)<br>- [Thea Sommerschield](https://theasommerschield.it/)<br>- [Brendan Shillingford](https://github.com/bshillingford)<br>- [Mahyar Bordbar](https://scholar.google.com/citations?user=KB3ldSQAAAAJ)<br>others[John Pavlopoulos](https://ipavlopoulos.github.io/)<br>[Marita Chatzipanagiotou](https://gr.linkedin.com/in/marita-chatzipanagiotou-b0611a1a2)<br>[Ion Androutsopoulos](https://pages.aueb.gr/users/ion/)<br>[Jonathan Prag](https://www.classics.ox.ac.uk/people/dr-jonathan-prag)<br>[Nando de Freitas](https://www.cs.ox.ac.uk/people/nando.defreitas/) | [![](https://camo.githubusercontent.com/a685ff7ab7c906fe3143f6a13c8c38816fb439df94ad8d099c920aa68ffe8af2/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313033382f7334313538362d3032322d30343434382d7a)](https://doi.org/10.1038/s41586-022-04448-z)[![](https://camo.githubusercontent.com/5b314d2ac773d646ec66a735e0d2af1f531e990e733c8d85e93dad7cd0bb8c53/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d646565706d696e642f6974686163613f7374796c653d736f6369616c)](https://github.com/google-deepmind/ithaca)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.06262)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sommerschield/iphi)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://odsc.medium.com/deep-neural-networks-could-be-key-to-ancient-text-restoration-and-attribution-research-shows-81a2d89d9413), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/syncedreview/ithaca-paper-published-in-nature-the-first-dnn-designed-for-textual-restoration-and-geographical-ef395d56697e)<br>- [project](https://ithaca.deepmind.com/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/tgeo0q/r_restoring_and_attributing_ancient_texts_using/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/ithaca/blob/master/colabs/ithaca_inference.ipynb) | 21.11.2023 |
| PixArt-Σ | Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation | - [Junsong Chen](https://lawrence-cj.github.io/)<br>- [Chongjian Ge](https://chongjiange.github.io/)<br>- [Enze Xie](https://xieenze.github.io/)<br>- [Yue Wu](https://yuewuhkust.github.io/)<br>others[Lewei Yao](https://scholar.google.com/citations?user=hqDyTg8AAAAJ)<br>[Xiaozhe Ren](https://scholar.google.com/citations?user=3t2j87YAAAAJ)<br>[Zhongdao Wang](https://zhongdao.github.io/)<br>[Ping Luo](http://luoping.me/)<br>[Huchuan Lu](https://scholar.google.com/citations?user=D3nE0agAAAAJ)<br>[Zhenguo Li](https://scholar.google.com/citations?user=XboZC1AAAAAJ) | [![](https://camo.githubusercontent.com/eefd54bd90b9b2dcee4a4537e08cf8f9e0a1a15d1e4f9044f35c5109a464edc5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5069784172742d616c7068612f5069784172742d7369676d613f7374796c653d736f6369616c)](https://github.com/PixArt-alpha/PixArt-sigma)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2403.04692), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2310.00426), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2401.05252)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/rde6eaE5Ta)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/PixArt-alpha/PixArt-LCM)<br>- [project](https://pixart-alpha.github.io/PixArt-sigma-project/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/PixArtSigma/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1jZ5UZXk7tcpTfVwnX33dDuefNMcnW9ME) | 07.11.2023 |
| Zero123++ | Image-conditioned diffusion model for generating 3D-consistent multi-view images from a single input view | - [Ruoxi Shi](https://rshi.top/)<br>- [Hansheng Chen](https://lakonik.github.io/)<br>- [Zhuoyang Zhang](https://github.com/zhuoyang20)<br>- [Minghua Liu](https://cseweb.ucsd.edu/~mil070/)<br>others[Chao Xu](https://chaoxu.xyz/)<br>[Xinyue Wei](https://sarahweiii.github.io/)<br>[Linghao Chen](https://ootts.github.io/)<br>[Chong Zeng](https://www.chong-zeng.com/)<br>[Hao Su](https://cseweb.ucsd.edu/~haosu/) | [![](https://camo.githubusercontent.com/2af2713de0438cb127d945222003c1bdd549ea1a4b3123494b72edf357dce7dd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5355444f2d41492d33442f7a65726f313233706c75733f7374796c653d736f6369616c)](https://github.com/SUDO-AI-3D/zero123plus)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2310.15110)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/One-2-3-45/One-2-3-45), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cvlab-columbia/zero123)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/sudo-ai/zero123plus-demo-space), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/ysharma/Zero123PlusDemo)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://xthemadgenius.medium.com/zero123-your-guide-to-single-view-to-multi-view-3d-image-transformation-b4346b0e6615)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/17f4c6p/zero123_a_single_image_to_consistent_multiview/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/V9AR-81pAgk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1_5ECnTOosRuAsm2tUp0zvBG0DppL-F3V) | 26.10.2023 |
| UniFormerV2 | Unified Transformer for Efficient Spatiotemporal Representation Learning | - [Kunchang Li](https://github.com/Andy1621)<br>- [Yali Wang](https://scholar.google.com/citations?user=hD948dkAAAAJ)<br>- [Yinan He](https://github.com/yinanhe)<br>- [Yizhuo Li](http://liyizhuo.com/)<br>others[Yi Wang](https://scholar.google.com/citations?user=Xm2M8UwAAAAJ)<br>[Limin Wang](http://wanglimin.github.io/)<br>[Yu Qiao](http://mmlab.siat.ac.cn/yuqiao/index.html) | [![](https://camo.githubusercontent.com/10ee65f4433905686231749e71486160800909b0f63f8e931931f86f8837548c/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435635313037302e323032332e3030313537)](https://doi.org/10.1109/ICCV51070.2023.00157)[![](https://camo.githubusercontent.com/13e966eb862f7b9bb0d529a04b17f0ec1556c384d07f5d91818a9f491af2b69b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e47564c61622f556e69466f726d657256323f7374796c653d736f6369616c)](https://github.com/OpenGVLab/UniFormerV2)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.09552)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/innat/UniFormerV2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://huggingface.co/spaces/Andy1621/uniformerv2_demo), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/SlowFast)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/Andy1621/uniformerv2_demo)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/action-classification-on-activitynet?p=uniformerv2-spatiotemporal-learning-by-arming), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/action-recognition-on-hacs?p=uniformerv2-spatiotemporal-learning-by-arming), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/action-classification-on-moments-in-time?p=uniformerv2-spatiotemporal-learning-by-arming), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/action-recognition-in-videos-on-something-1?p=uniformerv2-spatiotemporal-learning-by-arming), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/action-classification-on-kinetics-700?p=uniformerv2-spatiotemporal-learning-by-arming) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Z6RzLcno_eLGD_E96mlWoyGieGbKxPQr) | 20.10.2023 |
| Show-1 | Hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation | - [David Junhao Zhang](https://junhaozhang98.github.io/)<br>- [Jay Zhangjie Wu](https://zhangjiewu.github.io/)<br>- [Jiawei Liu](https://jia-wei-liu.github.io/)<br>- [Rui Zhao](https://ruizhaocv.github.io/)<br>others[Lingmin Ran](https://siacorplab.nus.edu.sg/people/ran-lingmin/)<br>[Yuchao Gu](https://ycgu.site/)<br>[Difei Gao](https://scholar.google.com/citations?user=No9OsocAAAAJ)<br>[Mike Zheng Shou](https://sites.google.com/view/showlab/home) | [![](https://camo.githubusercontent.com/49e2971c6747290599ea509b4b72066b3813054c7db73553f508829afeeaa73c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73686f776c61622f53686f772d313f7374796c653d736f6369616c)](https://github.com/showlab/Show-1)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.15818)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/showlab/show-1-base), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/showlab/show-1-interpolation), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/showlab/show-1-sr1), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/showlab/show-1-sr2), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/cerspense/zeroscope_v2_576w)<br>- [project](https://showlab.github.io/Show-1/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/Show-1-colab/blob/main/Show_1_steps_colab.ipynb) | 15.10.2023 |
| DA-CLIP | Degradation-aware vision-language model to better transfer pretrained vision-language models to low-level vision tasks as a universal framework for image restoration | - [Ziwei Luo](https://algolzw.github.io/)<br>- [Fredrik Gustafsson](http://www.fregu856.com/)<br>- [Zheng Zhao](https://zz.zabemon.com/)<br>- [Jens Sjölund](https://github.com/jsjol)<br>- [Thomas Schön](https://user.it.uu.se/~thosc112/index.html) | [![](https://camo.githubusercontent.com/9de36ca47de0324d238483e67373f7d3c589f39dddd364b8d6aceb9f87d7e9eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f416c676f6c7a772f6461636c69702d7569723f7374796c653d736f6369616c)](https://github.com/Algolzw/daclip-uir) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2310.01018)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Algolzw/image-restoration-sde)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/weblzw/daclip-uir-ViT-B-32-irsde)<br>- [project](https://algolzw.github.io/daclip-uir/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/daclip-uir-colab/blob/main/daclip_uir_gradio_colab.ipynb) | 11.10.2023 |
| SadTalker | Generates 3D motion coefficients of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation | - [Wenxuan Zhang](https://github.com/Winfredy)<br>- [Xiaodong Cun](https://vinthony.github.io/academic/)<br>- [Xuan Wang](https://xuanwangvc.github.io/)<br>- [Yong Zhang](https://yzhang2016.github.io/)<br>others[Xi Shen](https://xishen0220.github.io/)<br>[Yu Guo](https://yuguo-xjtu.github.io/)<br>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)<br>[Fei Wang](http://gr.xjtu.edu.cn/zh/web/feynmanw) | [![](https://camo.githubusercontent.com/8af3dd298ad55c88bfec38532c1e38d0f11776d334b800ddac0e84dcd2ba352a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323732392e323032332e3030383336)](https://doi.org/10.1109/CVPR52729.2023.00836)[![](https://camo.githubusercontent.com/c7ff4b69668a3190c74fc241f55f80bd41ab91b345f4d61fc4f053479acb3e3d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e54616c6b65722f53616454616c6b65723f7374796c653d736f6369616c)](https://github.com/OpenTalker/SadTalker)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.12194)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/rrayYqZ4tf)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/OpenTalker/DPE), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/RenYurui/PIRender), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/Deep3DFaceReconstruction), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/facexlib), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Zz-ww/SadTalker-Video-Lip-Sync), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/FeiiYin/SPI)<br>- [project](https://sadtalker.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/AoIzJWnQw1M), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/fDgQcDL-qOc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BkSnM9cxkcM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7u0FYVPQ5rc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/OpenTalker/SadTalker/blob/main/quick_demo.ipynb) | 10.10.2023 |
| Musika | Music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU | - [Marco Pasini](https://github.com/marcoppasini)<br>- [Jan Schlüter](https://www.ofai.at/~jan.schlueter/) | [![](https://camo.githubusercontent.com/c08585c0f51da7d932569bc0445fcbc20a6b03ea140f90daf535ece960ca76c8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6172636f70706173696e692f6d7573696b613f7374796c653d736f6369616c)](https://github.com/marcoppasini/musika)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2208.08706), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.08526)<br>- [data](https://magenta.tensorflow.org/datasets/maestro)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hendriks73/tempo-cnn), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CPJKU/madmom)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/marcop/musika)<br>- [project](https://marcoppasini.github.io/musika)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/QBl8y2Z_i7Y), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0l7OSM-bFvc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb) | 09.10.2023 |
| YOLOv6 | Single-stage object detection framework dedicated to industrial applications | - [Kaiheng Weng](https://github.com/khwengXU)<br>- [Meng Cheng](https://github.com/MTChengMeng)<br>- [Yiduo Li](https://github.com/yili123123)<br>- [Xiangxiang Chu](https://scholar.google.com/citations?&user=jn21pUsAAAAJ)<br>- [Xiaolin Wei](https://scholar.google.com/citations?user=s5b7lU4AAAAJ) | [![](https://camo.githubusercontent.com/f4fd0ceb4b638a82059e96ccf13d37d0f4d14c402857b8b68df56010c9c8209c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d65697475616e2f594f4c4f76363f7374796c653d736f6369616c)](https://github.com/meituan/YOLOv6)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.02976), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.05586)<br>- [blog post](https://learnopencv.com/yolov6-object-detection/)<br>- [data](https://cocodataset.org/#download)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://yolov6-docs.readthedocs.io/zh_CN/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/FeiGeChuanShu/ncnn-android-yolov6), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/ort/cv/yolov6.cpp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Linaom1214/TensorRT-For-YOLO-Series), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zhiqwang/yolov5-rt-stack/tree/main/deployment/tensorrt-yolov6)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3OpwcGU7VvE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/GJ0lVOE3a7c), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3hqkbqJ5ag8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/fFCWrMFH2UY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/meituan/YOLOv6/blob/master/turtorial.ipynb) | 08.10.2023 |
| DreamGaussian | Algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details | - [Jiaxiang Tang](https://me.kiui.moe/)<br>- [Jiawei Ren](https://jiawei-ren.github.io/)<br>- [Hang Zhou](https://hangz-nju-cuhk.github.io/)<br>- [Ziwei Liu](https://liuziwei7.github.io/)<br>- [Gang Zeng](http://www.cis.pku.edu.cn/info/1177/1378.htm) | [![](https://camo.githubusercontent.com/60681972f927d5beb1c81df9525111676e22023e998eada9f0eb2c6e3b2b4b03/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f647265616d676175737369616e2f647265616d676175737369616e3f7374796c653d736f6369616c)](https://github.com/dreamgaussian/dreamgaussian)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.16653)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/graphdeco-inria/diff-gaussian-rasterization), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/nvdiffrast), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hoffstadt/DearPyGui)<br>- [project](https://dreamgaussian.github.io/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1sLpYmmLS209-e5eHgcuqdryFRRO6ZhFS) | 04.10.2023 |
| ICON | Given a set of images, method estimates a detailed 3D surface from each image and then combines these into an animatable avatar | - [Yuliang Xiu](https://xiuyuliang.cn/)<br>- [Jinlong Yang](https://is.mpg.de/~jyang)<br>- [Dimitrios Tzionas](https://ps.is.mpg.de/~dtzionas)<br>- [Michael Black](https://ps.is.mpg.de/~black) | [![](https://camo.githubusercontent.com/13fb5d4477ba5b4c63a22d673b0e7908fa9d2b902a372a398efca59706a7f169/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031323934)](https://doi.org/10.1109/CVPR52688.2022.01294)[![](https://camo.githubusercontent.com/7e25a95f2345f7a23fac1fcc4abd4eb14a40e40c9e74592ab88c51fec80a5337/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f79756c69616e677869752f69636f6e3f7374796c653d736f6369616c)](https://github.com/yuliangxiu/icon)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.09127)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/KeypointNeRF), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/YadiraF/PIXIE), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/YuliangXiu/bvh-distance-queries), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Project-Splinter/MonoPortDataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ZhengZerong/PaMIR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Project-Splinter/MonoPort), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/shunsukesaito/SCANimate), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google/aistplusplus_api)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/Yuliang/ICON)<br>- [project](https://icon.is.tue.mpg.de/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hZd6AYin2DE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1-AWeWhPvCTBX0KfMtgtMk10uPU05ihoA) | 31.08.2023 |
| DINOv2 | Produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning | - [Maxime Oquab](https://scholar.google.com/citations?user=5vteYV8AAAAJ)<br>- [Timothée Darcet](https://github.com/TimDarcet)<br>- [Théo Moutakanni](https://github.com/TheoMoutakanni)<br>- [Huy Vo](https://huyvvo.github.io/)<br>others[Marc Szafraniec](https://github.com/MarcSzafraniec/)<br>[Vasil Khalidov](https://scholar.google.com/citations?user=tjazz3AAAAAJ)<br>[Pierre Fernandez](https://pierrefdz.github.io/)<br>[Daniel Haziza](https://scholar.google.com/citations?user=2eSKdFMAAAAJ)<br>[Francisco Massa](https://github.com/fmassa)<br>[Alaaeldin El-Nouby](https://aelnouby.github.io/)<br>[Mahmoud Assran](http://www.midoassran.ca/)<br>[Nicolas Ballas](https://scholar.google.com/citations?user=euUV4iUAAAAJ)<br>[Wojciech Galuba](https://scholar.google.com/citations?user=jyaTX64AAAAJ)<br>[Russell Howes](http://www.russellhowes.net/)<br>[Po-Yao Huang](https://berniebear.github.io/)<br>[Shang-Wen Li](https://swdanielli.github.io/)<br>[Ishan Misra](http://imisra.github.io/)<br>[Michael Rabbat](https://scholar.google.com/citations?user=cMPKe9UAAAAJ)<br>[Vasu Sharma](https://vasusharma.github.io/)<br>[Gabriel Synnaeve](https://syhw.github.io/)<br>[Hu Xu](https://howardhsu.github.io/)<br>[Hervé Jegou](https://github.com/jegou)<br>[Julien Mairal](http://thoth.inrialpes.fr/people/mairal/)<br>[Patrick Labatut](https://github.com/patricklabatut)<br>[Armand Joulin](https://scholar.google.com/citations?user=kRJkDakAAAAJ)<br>[Piotr Bojanowski](https://github.com/piotr-bojanowski) | [![](https://camo.githubusercontent.com/ea675de31c93ce6439f45c2e76f470c83a72611f8f527394625f0069e3e018ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f64696e6f76323f7374796c653d736f6369616c)](https://github.com/facebookresearch/dinov2) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2304.07193)<br>- [blog post](https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/)<br>- [demo](https://dinov2.metademolab.com/)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/main/model_doc/dinov2)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://purnasaigudikandula.medium.com/dinov2-image-classification-visualization-and-paper-review-745bee52c826), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/meta-ais-another-revolutionary-large-scale-model-dinov2-for-image-feature-extraction-1114b287eadd)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/csEgtSh7jV4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/KSZiJ4k28b4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/RZEkdOc3szU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/dinov2/blob/main/notebooks/semantic_segmentation.ipynb) | 31.08.2023 |
| OWL-ViT | Simple Open-Vocabulary Object Detection with Vision Transformers | - [Matthias Minderer](http://matthias.minderer.net/)<br>- [Alexey Gritsenko](https://github.com/AlexeyG)<br>- [Austin Stone](https://github.com/AustinCStone)<br>- [Maxim Neumann](https://github.com/maximneumann)<br>others[Dirk Weissenborn](https://github.com/dirkweissenborn)<br>[Alexey Dosovitskiy](https://scholar.google.com/citations?user=FXNJRDoAAAAJ)<br>[Aravindh Mahendran](https://github.com/aravindhm)<br>[Anurag Arnab](https://github.com/anuragarnab)<br>[Mostafa Dehghani](https://mostafadehghani.com/)<br>[Zhuoran Shen](https://cmsflash.github.io/)<br>[Xiao Wang](https://scholar.google.com/citations?user=ukyXqzMAAAAJ)<br>[Xiaohua Zhai](https://github.com/xiaohuazhai)<br>[Thomas Kipf](https://tkipf.github.io/)<br>[Neil Houlsby](https://neilhoulsby.github.io/) | [![](https://camo.githubusercontent.com/0981abaa5364497e0ac87cb97a47f3d2ba48a680ded55e73b9281f38692cdcae/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d32303038302d395f3432)](https://doi.org/10.1007/978-3-031-20080-9_42)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.06230)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/model_doc/owlvit) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb) | 21.08.2023 |
| StyleGAN3 | Alias-Free Generative Adversarial Networks | - [Tero Karras](https://research.nvidia.com/person/tero-karras)<br>- [Miika Aittala](https://research.nvidia.com/person/Miika-Aittala)<br>- [Samuli Laine](https://research.nvidia.com/person/Samuli-Laine)<br>- [Erik Härkönen](https://github.com/harskish)<br>others[Janne Hellsten](https://research.nvidia.com/person/Janne-Hellsten)<br>[Jaakko Lehtinen](https://users.aalto.fi/~lehtinj7/)<br>[Timo Aila](https://research.nvidia.com/person/timo-aila) | [![](https://camo.githubusercontent.com/01ad2171d88513b6356d5f5f164f96b088734a2b3e86be033a69fde79bc035e5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e566c6162732f7374796c6567616e333f7374796c653d736f6369616c)](https://github.com/NVlabs/stylegan3)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.12423), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.08500), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1801.01401), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1904.06991), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1812.04948), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1606.03498)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan3-detector), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/metfaces-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2-ada-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2-ada)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html)<br>- [project](https://nvlabs.github.io/stylegan3) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1BXNHZBai-pXtP-ncliouXo_kUiG1Pq7M) | 13.08.2023 |
| FateZero | Zero-shot text-based editing method on real-world videos without per-prompt training or use-specific mask | - [Chenyang Qi](https://chenyangqiqi.github.io/)<br>- [Xiaodong Cun](https://vinthony.github.io/academic/)<br>- [Yong Zhang](https://yzhang2016.github.io/)<br>- [Chenyang Lei](https://chenyanglei.github.io/)<br>others[Xintao Wang](https://xinntao.github.io/)<br>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)<br>[Qifeng Chen](https://cqf.io/) | [![](https://camo.githubusercontent.com/06849bd848f2b85e9d8a96da3badb92c05ca6e9305fff5cc94d421bbba8b3ff9/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435635313037302e323032332e3031343630)](https://doi.org/10.1109/ICCV51070.2023.01460)[![](https://camo.githubusercontent.com/c533a1aaa12bb2717b791cb3b3e519c0d1e489addb7a09adbf6965940480e3da/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4368656e79616e67516951692f466174655a65726f3f7374796c653d736f6369616c)](https://github.com/ChenyangQiQi/FateZero)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.09535)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/bryandlee/Tune-A-Video), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google/prompt-to-prompt)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/chenyangqi/FateZero), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/chenyangqi/jeep_tuned_200)<br>- [project](https://fate-zero-edit.github.io/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/11uzioo/r_fatezero_fusing_attentions_for_zeroshot/)<br>- [video](https://hkustconnect-my.sharepoint.com/personal/cqiaa_connect_ust_hk/_layouts/15/stream.aspx?id=%2Fpersonal%2Fcqiaa%5Fconnect%5Fust%5Fhk%2FDocuments%2Fdiffusion%2Fweb%5Fvideo%2Emp4&ga=1&referrer=StreamWebApp%2EWeb&referrerScenario=AddressBarCopied%2Eview%2E9b85614a%2D5af9%2D4485%2Dbcb1%2Db39f90e8d381) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ChenyangQiQi/FateZero/blob/main/colab_fatezero.ipynb) | 13.08.2023 |
| Big GAN | Large Scale GAN Training for High Fidelity Natural Image Synthesis | - [Andrew Brock](https://github.com/ajbrock)<br>- [Jeff Donahue](https://jeffdonahue.com/)<br>- [Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1809.11096) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) | 03.08.2023 |
| LaMa | Resolution-robust Large Mask Inpainting with Fourier Convolutions | - [Roman Suvorov](https://github.com/windj007)<br>- [Elizaveta Logacheva](https://github.com/elimohl)<br>- [Anton Mashikhin](https://www.linkedin.com/in/heyt0ny/)<br>- [Anastasia Remizova](https://github.com/feathernox)<br>others[Arsenii Ashukha](https://ashukha.com/)<br>[Aleksei Silvestrov](https://www.linkedin.com/in/%D0%B0%D0%BB%D0%B5%D0%BA%D1%81%D0%B5%D0%B9-%D1%81%D0%B8%D0%BB%D1%8C%D0%B2%D0%B5%D1%81%D1%82%D1%80%D0%BE%D0%B2-141b99b6/)<br>[Naejin Kong](https://github.com/naejin-kong)<br>[Harshith Goka](https://github.com/h9399-goka)<br>[Kiwoong Park](https://github.com/kyoong-park)<br>[Victor Lempitsky](http://sites.skoltech.ru/compvision/members/vilem/) | [![](https://camo.githubusercontent.com/d4161641bf6f62c1c869c3e5eab505e1b6c4bcbadea8429dd0a79ab5e5e364f3/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5741435635313435382e323032322e3030333233)](https://doi.org/10.1109/WACV51458.2022.00323)[![](https://camo.githubusercontent.com/a324b229d8c09009bcaf6fd47a0b4398f0506965613ff67c4a5c0934c529743d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f736169632d6d64616c2f6c616d613f7374796c653d736f6369616c)](https://github.com/saic-mdal/lama)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.07161)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/andy971022/auto-lama), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/richzhang/PerceptualSimilarity), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Po-Hsun-Su/pytorch-ssim), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mseitzer/pytorch-fid)<br>- [project](https://saic-mdal.github.io/lama-project/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/saic-mdal/lama/blob/master/colab/LaMa_inpainting.ipynb) | 02.08.2023 |
| MakeItTalk | A method that generates expressive talking-head videos from a single facial image with audio as the only input | - [Yang Zhou](https://people.umass.edu/~yangzhou/)<br>- [Xintong Han](http://users.umiacs.umd.edu/~xintong/)<br>- [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)<br>- [Jose Echevarria](http://www.jiechevarria.com/)<br>others[Evangelos Kalogerakis](https://people.cs.umass.edu/~kalo/)<br>[Dingzeyu Li](https://dingzeyu.li/) | [![](https://camo.githubusercontent.com/9035bcd973a26662a3d4b34a3ff76037a484a11d01a768930fcfa477d13d370b/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333431343638352e33343137373734)](https://doi.org/10.1145/3414685.3417774) [![](https://camo.githubusercontent.com/e118c8edf40714330bbc09355586eebd2e970472537dfacbf66b9d8a0fdfc8ce/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f797a686f753335392f4d616b65497454616c6b3f7374796c653d736f6369616c)](https://github.com/yzhou359/MakeItTalk) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.12992)<br>- [data](https://drive.google.com/drive/folders/1EwuAy3j1b9Zc1MsidUfxG_pJGc_cV60O)<br>- [project](https://people.umass.edu/~yangzhou/MakeItTalk/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=vUMGKASgbf8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/iboyles/makeittalknow/blob/main/working_quick_demo_of_makeittalk_07_2023.ipynb) | 27.07.2023 |
| HiDT | A generative image-to-image model and a new upsampling scheme that allows to apply image translation at high resolution | - [Denis Korzhenkov](https://github.com/denkorzh)<br>- [Gleb Sterkin](https://github.com/belkakari)<br>- [Sergey Nikolenko](https://logic.pdmi.ras.ru/~sergey/)<br>- [Victor Lempitsky](http://sites.skoltech.ru/compvision/members/vilem/) | [![](https://camo.githubusercontent.com/0b5b7b733ae00106c2a9b8640e9f5a43b1fe47060840ecc3174b76b4d90695ec/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030373531)](https://doi.org/10.1109/CVPR42600.2020.00751) [![](https://camo.githubusercontent.com/af03130f73ae7a5b157b3e6efe77685abc80b8388c48f1a294fc58edff01a8ff/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f736169632d6d64616c2f486944543f7374796c653d736f6369616c)](https://github.com/saic-mdal/HiDT) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.08791)<br>- [project](https://saic-mdal.github.io/HiDT/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLuvGzlEQXT1KQuKrfBBEWh2f3PToxyeM5), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=EWKAgwgqXB4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/saic-mdal/hidt/blob/master/notebooks/HighResolutionDaytimeTranslation.ipynb) | 24.07.2023 |
| CutLER | Simple approach for training unsupervised object detection and segmentation models | - [Xudong Wang](https://people.eecs.berkeley.edu/~xdwang/)<br>- [Rohit Girdhar](https://rohitgirdhar.github.io/)<br>- [Stella Yu](https://www1.icsi.berkeley.edu/~stellayu/)<br>- [Ishan Misra](https://imisra.github.io/) | [![](https://camo.githubusercontent.com/8fce07f2c472e8d576b5c6b989b0220a20d077bd10a006097149130616207235/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f4375744c45523f7374796c653d736f6369616c)](https://github.com/facebookresearch/CutLER)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.11320), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.02677)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html)<br>- [project](http://people.eecs.berkeley.edu/~xdwang/projects/CutLER/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1NgEyFHvOfuA2MZZnfNPWg1w5gSr3HOBb) | 24.07.2023 |
| Recognize Anything & Tag2Text | Vision language pre-training framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features | - [Xinyu Huang](https://xinyu1205.github.io/)<br>- [Youcai Zhang](https://github.com/Coler1994)<br>- [Jinyu Ma](https://github.com/majinyu666)<br>- [Zhaoyang Li](https://github.com/ZhaoyangLi-nju)<br>others[Yanchun Xie](https://scholar.google.com/citations?user=T0xk9-wAAAAJ)<br>[Yuzhuo Qin](https://scholar.google.com/citations?user=5ZG65AkAAAAJ)<br>[Tong Luo](https://ieeexplore.ieee.org/author/37089387319)<br>[Yaqian Li](https://openreview.net/profile?id=~Yaqian_Li1)<br>[Yandong Guo](http://www.lsl.zone/)<br>[Yandong Guo](https://scholar.google.com/citations?user=fWDoWsQAAAAJ)<br>[Lei Zhang](https://www.leizhang.org/) | [![](https://camo.githubusercontent.com/4c3b1ef3514befd4e09ed2b40350a8851a0ce8d418b4634a63d36f4c3db71b62/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f78696e7975313230352f7265636f676e697a652d616e797468696e673f7374796c653d736f6369616c)](https://github.com/xinyu1205/recognize-anything)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.03514), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.05657)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/OpenGVLab/Ask-Anything), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/positive666/Prompt-Can-Anything)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://artgor.medium.com/paper-review-recognize-anything-a-strong-image-tagging-model-9e5e1c6dd0af)<br>- [project](https://recognize-anything.github.io/), [project](https://recognize-anything.github.io/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mhd-medfa/recognize-anything/blob/main/recognize_anything_demo.ipynb) | 09.07.2023 |
| Thin-Plate Spline Motion Model | End-to-end unsupervised motion transfer framework | - [Jian Zhao](https://scholar.google.com/citations?user=OKm5CQYAAAAJ)<br>- [Hui Zhang](https://scholar.google.com/citations?user=w3mzCiwAAAAJ) | [![](https://camo.githubusercontent.com/82e3b9741a4929b59c403e1fd44294f242c5690d9e0a58c175f2df19b556edef/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030333634)](https://doi.org/10.1109/CVPR52688.2022.00364)[![](https://camo.githubusercontent.com/3e380c1d578cf25b7ede9feacd47e5a3444ca6859ae8e05baf8ea51e1c0ddc68/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f796f796f2d6e622f5468696e2d506c6174652d53706c696e652d4d6f74696f6e2d4d6f64656c3f7374796c653d736f6369616c)](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.14367)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AliaksandrSiarohin/monkey-net), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AliaksandrSiarohin/video-preprocessing), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AliaksandrSiarohin/pose-evaluation), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TalkUHulk/Image-Animation-Turbo-Boost)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/CVPR/Image-Animation-using-Thin-Plate-Spline-Motion-Model)<br>- [supp](https://cloud.tsinghua.edu.cn/f/f7b8573bb5b04583949f/?dl=1) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1DREfdpnaBhqISg0fuQlAAIwyGVn1loH_) | 07.07.2023 |
| MobileSAM | Towards Lightweight SAM for Mobile Applications | - [Chaoning Zhang](https://github.com/ChaoningZhang)<br>- [Dongshen Han](https://github.com/dongshenhan)<br>- [Yu Qiao](https://github.com/qiaoyu1002)<br>- [Jung Uk Kim](https://visualai.khu.ac.kr/)<br>others[Sung-Ho Bae](https://scholar.google.com/citations?user=EULut5oAAAAJ)<br>[Seungkyu Lee](https://scholar.google.com/citations?user=3Pf6C6cAAAAJ)<br>[Choong Seon Hong](https://scholar.google.com/citations?user=oKANWloAAAAJ) | [![](https://camo.githubusercontent.com/7415688672d81525c3738ff9f3f2cc497dbbe98bd551478796bf71ba49c3e299/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4368616f6e696e675a68616e672f4d6f62696c6553414d3f7374796c653d736f6369616c)](https://github.com/ChaoningZhang/MobileSAM)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.14289)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jolibrain/joliGEN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/akbartus/MobileSAM-in-the-Browser), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/qiaoyu1002/Inpaint-Anything), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/qiaoyu1002/Personalize-SAM), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Jumpat/SegmentAnythingin3D), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vietanhdev/anylabeling), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/wangsssky/SonarSAM), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/continue-revolution/sd-webui-segment-anything)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/_akhaliq/status/1674410573075718145)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/eTEfq_kWabQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ChaoningZhang/MobileSAM/blob/master/notebooks/predictor_example.ipynb) | 30.06.2023 |
| Grounding DINO | Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | - [Shilong Liu](https://github.com/SlongLiu)<br>- [Zhaoyang Zeng](https://scholar.google.com/citations?user=U_cvvUwAAAAJ)<br>- [Tianhe Ren](https://rentainhe.github.io/)<br>- [Feng Li](https://scholar.google.com/citations?user=ybRe9GcAAAAJ)<br>others[Hao Zhang](https://scholar.google.com/citations?user=B8hPxMQAAAAJ)<br>[Jie Yang](https://yangjie-cv.github.io/)<br>[Chunyuan Li](https://scholar.google.com/citations?user=Zd7WmXUAAAAJ)<br>[Jianwei Yang](https://jwyang.github.io/)<br>[Hang Su](https://www.suhangss.me/)<br>[Jun Zhu](https://scholar.google.com/citations?user=axsP38wAAAAJ)<br>[Lei Zhang](https://www.leizhang.org/) | [![](https://camo.githubusercontent.com/c6a434d03c84765d1ca6368c83522ccc7b0b5178349b2d0e2de3ef764d9be71e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f494445412d52657365617263682f47726f756e64696e6744494e4f3f7374796c653d736f6369616c)](https://github.com/IDEA-Research/GroundingDINO)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.05499)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IDEA-Research/DINO), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/UX-Decoder/Semantic-SAM), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/OptimalScale/DetGPT), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IDEA-Research/OpenSeeD), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/X-Decoder/tree/xgpt), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IDEA-Research/detrex)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/zero-shot-object-detection-on-mscoco?p=grounding-dino-marrying-dino-with-grounded), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/zero-shot-object-detection-on-odinw?p=grounding-dino-marrying-dino-with-grounded), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/object-detection-on-coco-minival?p=grounding-dino-marrying-dino-with-grounded), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/object-detection-on-coco?p=grounding-dino-marrying-dino-with-grounded)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/wxWDt5UiwY8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/cMa77r3YrDk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/C4NqaRBz_Kw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/oEQYStnF2l8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) | 28.06.2023 |
| T5X | Modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models at many scales | - [Adam Roberts](https://github.com/adarob)<br>- [Hyung Won Chung](https://github.com/hwchung27)<br>- [Anselm Levskaya](https://anselmlevskaya.com/)<br>- [Gaurav Mishra](https://research.google/people/GauravMishra/)<br>others[James Bradbury](https://github.com/jekbradbury)<br>[Daniel Andor](https://github.com/andorardo)<br>[Sharan Narang](https://github.com/sharannarang)<br>[Brian Lester](https://blester125.com/)<br>[Colin Gaffney](https://github.com/cpgaffney1)<br>[Afroz Mohiuddin](https://github.com/afrozenator)<br>[Curtis Hawthorne](https://github.com/cghawthorne)<br>[Aitor Lewkowycz](https://scholar.google.com/citations?user=Yum1ah0AAAAJ)<br>[Alex Salcianu](https://scholar.google.com/citations?user=HSrT1wsAAAAJ)<br>[Marc van Zee](https://github.com/marcvanzee)<br>[Jacob Austin](https://jacobaustin123.github.io/)<br>[Sebastian Goodman](https://github.com/0x0539)<br>[Livio Baldini Soares](https://liviosoares.github.io/)<br>[Haitang Hu](https://hthu.github.io/)<br>[Sasha Tsvyashchenko](https://endl.ch/)<br>[Aakanksha Chowdhery](http://www.achowdhery.com/)<br>[Jasmijn Bastings](https://jasmijn.ninja/)<br>[Jannis Bulian](http://bulian.org/)<br>[Xavier Garcia](https://scholar.google.com/citations?user=Y2Hio6MAAAAJ)<br>[Jianmo Ni](https://nijianmo.github.io/)<br>[Kathleen Kenealy](https://scholar.google.com/citations?&user=HgRBC5gAAAAJ)<br>[Jonathan Clark](http://www.cs.cmu.edu/~jhclark/)<br>[Dan Garrette](http://www.dhgarrette.com/)<br>[James Lee-Thorp](https://scholar.google.com/citations?user=qsPv098AAAAJ)<br>[Colin Raffel](https://colinraffel.com/)<br>[Noam Shazeer](https://scholar.google.com/citations?user=wsGvgA8AAAAJ)<br>[Marvin Ritter](https://scholar.google.com/citations?user=arcf5FgAAAAJ)<br>[Maarten Bosma](https://scholar.google.com/citations?user=wkeFQPgAAAAJ)<br>[Alexandre Passos](https://www.ic.unicamp.br/~tachard/)<br>[Jeremy Maitin-Shepard](https://research.google/people/JeremyMaitinShepard/)<br>[Noah Fiedel](https://scholar.google.com/citations?user=XWpV9DsAAAAJ)<br>[Brennan Saeta](https://github.com/saeta)<br>[Ryan Sepassi](https://ryansepassi.com/)<br>[Alexander Spiridonov](https://research.google/people/AlexanderSpiridonov/)<br>[Joshua Newlan](https://github.com/joshnewlan)<br>[Andrea Gesmundo](https://github.com/agesmundo) | [![](https://camo.githubusercontent.com/e60096690aa08b31cbd51e2cafc02e6269a8758b9e4506e923516aeff1ab96d3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f7435783f7374796c653d736f6369616c)](https://github.com/google-research/t5x)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.17189), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.10683)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://t5x.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tensorflow/mesh), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tensorflow/serving)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/data), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tensorboard) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-research/t5x/blob/main/t5x/notebooks/introduction.ipynb) | 27.06.2023 |
| CodeTalker | Cast speech-driven facial animation as a code query task in a finite proxy space of the learned codebook, which effectively promotes the vividness of the generated motions by reducing the cross-modal mapping uncertainty | - [Jinbo Xing](https://doubiiu.github.io/)<br>- [Menghan Xia](https://menghanxia.github.io/)<br>- [Yuechen Zhang](https://julianjuaner.github.io/)<br>- [Xiaodong Cun](https://vinthony.github.io/academic/)<br>others[Jue Wang](https://juewang725.github.io/)<br>[Tien-Tsin Wong](https://ttwong12.github.io/myself.html) | [![](https://camo.githubusercontent.com/763cfbdda36671103e28bd086efc7492619a450ea15d36dda3a759a8eb48d782/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323732392e323032332e3031323239)](https://doi.org/10.1109/CVPR52729.2023.01229)[![](https://camo.githubusercontent.com/4d7d38850746ff8d9de80ab47c069766743050980e4a451dac4d13ea727b585c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f446f75626969752f436f646554616c6b65723f7374796c653d736f6369616c)](https://github.com/Doubiiu/CodeTalker)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.02379), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.09797)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/MPI-IS/mesh), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TimoBolkart/voca/tree/master/template), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/EvelynFan/FaceFormer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/RenYurui/PIRender), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/OpenTalker/StyleHEAT), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Meta-Portrait/MetaPortrait)<br>- [project](https://doubiiu.github.io/projects/codetalker/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/J2RngmuYrG4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Doubiiu/CodeTalker/blob/main/demo.ipynb) | 16.06.2023 |
| First Order Motion Model for Image Animation | Transferring facial movements from video to image | [Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/) | [![](https://camo.githubusercontent.com/0f8138627e5a3e0338baa331f0f708937f4f1ea9f033241e88f87a5d6d7d6db7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f416c69616b73616e6472536961726f68696e2f66697273742d6f726465722d6d6f64656c3f7374796c653d736f6369616c)](https://github.com/AliaksandrSiarohin/first-order-model) <br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2019/hash/31c0b36aef265d9221af80872ceb62f9-Abstract.html)<br>- [project](https://aliaksandrsiarohin.github.io/first-order-model-website/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=u-0cQ-grXBQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb) | 04.06.2023 |
| Parallel WaveGAN | State-of-the-art non-autoregressive models to build your own great vocoder | [Tomoki Hayashi](https://kan-bayashi.github.io/) | [![](https://camo.githubusercontent.com/f494223e8cc85c4ed78e848652555a50f73b9e9ad9155bae9222f6ad1c0688f2/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f49434153535034303737362e323032302e39303533373935)](https://doi.org/10.1109/ICASSP40776.2020.9053795)[![](https://camo.githubusercontent.com/d4d2e7acdf07faea1293bc9890060d46d5803834e2a8acd7e63e25cfddc2e45d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b616e2d626179617368692f506172616c6c656c5761766547414e3f7374796c653d736f6369616c)](https://github.com/kan-bayashi/ParallelWaveGAN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.11480), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.06711), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.05106)<br>- [demo](https://kan-bayashi.github.io/ParallelWaveGAN/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/tacotron2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/espnet/espnet) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb) | 01.06.2023 |
| ECON | designed for "Human digitization from a color image", which combines the best properties of implicit and explicit representations, to infer high-fidelity 3D clothed humans from in-the-wild images, even with loose clothing or in challenging poses | - [Yuliang Xiu](https://xiuyuliang.cn/)<br>- [Jinlong Yang](https://is.mpg.de/~jyang)<br>- [Xu Cao](https://xucao-42.github.io/homepage/)<br>- [Dimitrios Tzionas](https://ps.is.mpg.de/~dtzionas)<br>- [Michael Black](https://ps.is.mpg.de/~black) | [![](https://camo.githubusercontent.com/c303f30c81e9583739cc56cdba6968a962389ffc3f10f002a035c36a0e012b33/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323732392e323032332e3030303537)](https://doi.org/10.1109/CVPR52729.2023.00057)[![](https://camo.githubusercontent.com/f5c823c64bee403a2c95311babf194cb7feaf5441ba93b54c0f687564732b92d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f59756c69616e675869752f45434f4e3f7374796c653d736f6369616c)](https://github.com/YuliangXiu/ECON)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.07422)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/Vqa7KBGRyk)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://github.com/YuliangXiu/ECON/blob/master/docs/installation-docker.md)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kwan3854/CEB_ECON), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xucao-42/bilateral_normal_integration), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Project-Splinter/MonoPortDataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/huangyangyi/TeCH), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/huangyangyi/TeCH), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vchoutas/smplx), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yfeng95/PIXIE)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1451sjr/econ_explicit_clothed_humans_optimized_via_normal/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/yuliangxiu)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/sbWZbTf6ZYk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/SDVfCeaI4AY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5PEd_p90kS0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MDFvV7y5Qgk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1YRgwoRCZIrSB2e7auEWFyG10Xzjbrbno) | 31.05.2023 |
| MMS | The Massively Multilingual Speech project expands speech technology from about 100 languages to over 1000 by building a single multilingual speech recognition model supporting over 1100 languages, language identification models able to identify over 4000 languages, pretrained models supporting over 1400 languages, and text-to-speech models for over 1100 languages | - [Vineel Pratap](https://github.com/vineelpratap)<br>- [Andros Tjandra](https://github.com/androstj)<br>- [Bowen Shi](https://scholar.google.com/citations?user=xqyoorYAAAAJ)<br>- [Paden Tomasello](https://scholar.google.com/citations?user=sBtWMGYAAAAJ)<br>others[Arun Babu](https://scholar.google.com/citations?user=oJfoTakAAAAJ)<br>[Sayani Kundu](https://www.linkedin.com/in/sayani-kundu)<br>[Ali Elkahky](https://scholar.google.com/citations?user=KB3S8RoAAAAJ)<br>[Zhaoheng Ni](https://scholar.google.com/citations?user=SYFMSNsAAAAJ)<br>[Apoorv Vyas](https://apoorv2904.github.io/)<br>[Maryam Fazel-Zarandi](https://www.maryamfazel.com/)<br>[Alexei Baevski](https://github.com/alexeib)<br>[Yossi Adi](https://www.cs.huji.ac.il/~adiyoss/)<br>[Xiaohui Zhang](https://github.com/xiaohui-zhang)<br>[Wei-Ning Hsu](https://wnhsu.github.io/)<br>[Alexis Conneau](https://github.com/aconneau)<br>[Michael Auli](https://github.com/michaelauli) | [![](https://camo.githubusercontent.com/12713baaa78572a511fbed8b2614003e623ed65b00d51871ac20a8e2c8b8a117/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f666169727365713f7374796c653d736f6369616c)](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.13516)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/main/en/model_doc/mms), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/mms-cclms/), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/blog/mms_adapters)<br>- [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://ai.facebook.com/blog/multilingual-model-speech-recognition/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/GEzxHxWys2s), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/g06agCmxS7I) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/fairseq/blob/main/examples/mms/asr/tutorial/MMS_ASR_Inference_Colab.ipynb) | 26.05.2023 |
| FAB | Flow AIS Bootstrap uses AIS to generate samples in regions where the flow is a poor approximation of the target, facilitating the discovery of new modes | - [Laurence Midgley](https://lollcat.github.io/laurence-midgley/)<br>- [Vincent Stimper](https://is.mpg.de/person/vstimper)<br>- [Gregor N. C. Simm](https://www.gncs.me/)<br>- [Bernhard Schölkopf](https://scholar.google.com/citations?user=DZ-fHPgAAAAJ)<br>- [José Miguel Hernández-Lobato](https://jmhl.org/) | [![](https://camo.githubusercontent.com/12f3aa3c1ba19299d5009400c1759d0d2a51b714dc44d4516295d7d8bb6b292f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6f6c6c6361742f6661622d746f7263683f7374796c653d736f6369616c)](https://github.com/lollcat/fab-torch)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2208.01893)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lollcat/fab-jax-old), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/annealed_flow_transport)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/xQQXvOWu9nE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/lollcat/fab-torch/blob/master/experiments/gmm/fab_gmm.ipynb) | 29.04.2023 |
| CodeFormer | Transformer-based prediction network to model global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded | - [Shangchen Zhou](https://shangchenzhou.com/)<br>- [Kelvin Chan](https://ckkelvinchan.github.io/)<br>- [Chongyi Li](https://li-chongyi.github.io/)<br>- [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) | [![](https://camo.githubusercontent.com/9826e89dc7efc38bec28ae30b8791e8b4e999f7f76d798a54ee19e646003b5ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73637a686f752f436f6465466f726d65723f7374796c653d736f6369616c)](https://github.com/sczhou/CodeFormer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2206.11253)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/samb-t/unleashing-transformers), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepcam-cn/yolov5-face), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/facexlib)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c573258c38d0a3919d8c1364053c45df-Abstract-Conference.html)<br>- [project](https://shangchenzhou.com/projects/CodeFormer/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/d3VDpkXlueI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PtwWu-FugbA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ORtYP8NW4T0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/xc5lKOKBCcg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb) | 21.04.2023 |
| Text2Video-Zero | Text-to-Image Diffusion Models are Zero-Shot Video Generators | - [Levon Khachatryan](https://github.com/lev1khachatryan)<br>- [Andranik Movsisyan](https://github.com/19and99)<br>- [Vahram Tadevosyan](https://www.linkedin.com/in/vtadevosian)<br>- [Roberto Henschel](https://github.com/rob-hen)<br>others[Zhangyang Wang](https://www.ece.utexas.edu/people/faculty/atlas-wang)<br>[Shant Navasardyan](https://scholar.google.com/citations?user=VJSh59sAAAAJ)<br>[Humphrey Shi](https://www.humphreyshi.com/) | [![](https://camo.githubusercontent.com/38c9155b1887ebe5e1fa39a857f7f62c736e798cb8be52d5e04fd32d805a8b24/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435635313037302e323032332e3031343632)](https://doi.org/10.1109/ICCV51070.2023.01462)[![](https://camo.githubusercontent.com/fda81bc3e82b9c9fa8aa8bb1badaa15062a9f227a040c16a19bc0bd1873da511/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f506963736172742d41492d52657365617263682f5465787432566964656f2d5a65726f3f7374796c653d736f6369616c)](https://github.com/Picsart-AI-Research/Text2Video-Zero)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.13439), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.01341), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.17604)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/dbolya/tomesd), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/JiauZhang/Text2Video-Zero), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/camenduru/text2video-zero-colab), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/SHI-Labs/Text2Video-Zero-sd-webui)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero)<br>- [project](https://text2video-zero.github.io/)<br>- [video](https://www.dropbox.com/s/uv90mi2z598olsq/Text2Video-Zero.MP4)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/beeDJJz-Q0A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/97-1GYPtz0M) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/text2video-zero-colab/blob/main/text2video_all.ipynb) | 11.04.2023 |
| Segment Anything | The Segment Anything Model produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image | - [Alexander Kirillov](https://alexander-kirillov.github.io/)<br>- [Eric Mintun](https://ericmintun.github.io/)<br>- [Nikhila Ravi](https://nikhilaravi.com/)<br>- [Hanzi Mao](https://hanzimao.me/)<br>others[Chloé Rolland](https://scholar.google.com/citations?user=n-SnMhoAAAAJ)<br>[Laura Gustafson](https://scholar.google.com/citations?user=c8IpF9gAAAAJ)<br>[Tete Xiao](https://tetexiao.com/)<br>[Spencer Whitehead](https://www.spencerwhitehead.com/)<br>[Alex Berg](http://acberg.com/)<br>[Wan-Yen Lo](https://github.com/wanyenlo)<br>[Piotr Dollár](https://pdollar.github.io/)<br>[Ross Girshick](https://www.rossgirshick.info/) | [![](https://camo.githubusercontent.com/fda9f7bfbe5462db4c5ea420348ca3592c669a0d5b92eca51d8bb1044b5c11b5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7365676d656e742d616e797468696e673f7374796c653d736f6369616c)](https://github.com/facebookresearch/segment-anything) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2304.02643)<br>- [data](https://ai.facebook.com/datasets/segment-anything/)<br>- [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://ai.facebook.com/research/publications/segment-anything/), [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)<br>- [website](https://segment-anything.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/2O_vecl28OA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/fVeW9a6wItM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/FjYE0tKWOiY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb) | 10.04.2023 |
| FollowYourPose | Two-stage training scheme that can utilize image pose pair and pose-free video datasets and the pre-trained text-to-image model to obtain the pose-controllable character videos | - [Yue Ma](https://mayuelala.github.io/)<br>- [Yingqing He](https://yingqinghe.github.io/)<br>- [Xiaodong Cun](https://vinthony.github.io/academic/)<br>- [Xintao Wang](https://xinntao.github.io/)<br>others[Siran Chen](https://github.com/Sranc3)<br>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)<br>[Xiu Li](https://scholar.google.com/citations?user=Xrh1OIUAAAAJ)<br>[Qifeng Chen](https://cqf.io/) | [![](https://camo.githubusercontent.com/c225831aded2d6761b043bd7cb27e619d98055f66fae754d8925fbd28a5365b3/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313630392f616161692e76333869352e3238323036)](https://doi.org/10.1609/aaai.v38i5.28206)[![](https://camo.githubusercontent.com/0fbd24580dd459ef76938ddc62bdd0d2ede6c6e6efddbaefc4b102a9d2380fbb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d617975656c616c612f466f6c6c6f77596f7572506f73653f7374796c653d736f6369616c)](https://github.com/mayuelala/FollowYourPose)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2304.01186), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10752)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/bryandlee/Tune-A-Video)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/YueMafighting/FollowYourPose_v1/tree/main), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/CompVis/stable-diffusion-v1-4)<br>- [project](https://follow-your-pose.github.io/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://github.com/mayuelala)<br>- [video](https://underline.io/lecture/91712-follow-your-pose-pose-guided-text-to-video-generation-using-pose-free-videos) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mayuelala/FollowYourPose/blob/main/quick_demo.ipynb) | 07.04.2023 |
| EVA3D | High-quality unconditional 3D human generative model that only requires 2D image collections for training | - [Fangzhou Hong](https://hongfz16.github.io/)<br>- [Zhaoxi Chen](https://frozenburning.github.io/)<br>- [Yushi Lan](https://github.com/NIRVANALAN)<br>- [Liang Pan](https://github.com/paul007pl)<br>- [Ziwei Liu](https://liuziwei7.github.io/) | [![](https://camo.githubusercontent.com/76508eaaae172d4fc88056405514d67b822df70fa313668752227d23058fc34e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f686f6e67667a31362f45564133443f7374796c653d736f6369616c)](https://github.com/hongfz16/EVA3D) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2210.04888)<br>- [project](https://hongfz16.github.io/projects/EVA3D.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/JNV0FJ0aDWM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/M-kyvzTQrBI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/hongfz16/EVA3D/blob/main/notebook/EVA3D_Demo.ipynb) | 06.04.2023 |
| Stable Dreamfusion | Using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis | - [Jiaxiang Tang](https://me.kiui.moe/)<br>- [Ben Poole](https://cs.stanford.edu/~poole/)<br>- [Ajay Jain](https://ajayj.com/)<br>- [Jon Barron](https://jonbarron.info/)<br>- [Ben Mildenhall](https://bmild.github.io/) | [![](https://camo.githubusercontent.com/bcaf163c3a6d0974775e14021f271223d235ea0e984306eaaa0ba635108c5744/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61736861776b65792f737461626c652d647265616d667573696f6e3f7374796c653d736f6369616c)](https://github.com/ashawkey/stable-dreamfusion)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.14988)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ashawkey/torch-ngp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hoffstadt/DearPyGui)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/runwayml/stable-diffusion-v1-5)<br>- [project](https://dreamfusion3d.github.io/)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uM5NPodZZ1U?t=219), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/zWD5ZR5GtJM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/L3G0dx1Q0R8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dIgDbBTztUM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUUvTKwUkrrlCHpF) | 04.04.2023 |
| PIFuHD | Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization | - [Shunsuke Saito](https://shunsukesaito.github.io/)<br>- [Tomas Simon](http://www.cs.cmu.edu/~tsimon/)<br>- [Jason Saragih](https://scholar.google.com/citations?user=ss-IvjMAAAAJ)<br>- [Hanbyul Joo](https://jhugestar.github.io/) | [![](https://camo.githubusercontent.com/9c988fdaa465cdcf8d799f696f16ddfa28bed66e101a0c8cc7d72a56e2326512/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030303136)](https://doi.org/10.1109/CVPR42600.2020.00016)[![](https://camo.githubusercontent.com/0ba2ae07adad18de2820a3955419a2603f1826f83be00e3a8acb51e7b02cfb0a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7069667568643f7374796c653d736f6369616c)](https://github.com/facebookresearch/pifuhd)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.00452)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uEDqCxvF5yc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=8qnwbbDS8xk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt) | 26.03.2023 |
| VideoReTalking | System to edit the faces of a real-world talking head video according to input audio, producing a high-quality and lip-syncing output video even with a different emotion | - [Kun Cheng](https://github.com/kunncheng)<br>- [Xiaodong Cun](https://vinthony.github.io/)<br>- [Yong Zhang](https://yzhang2016.github.io/)<br>- [Menghan Xia](https://menghanxia.github.io/)<br>others[Fei Yin](https://feiiyin.github.io/)<br>[Mingrui Zhu](https://web.xidian.edu.cn/mrzhu/en/index.html)<br>[Xuan Wang](https://xuanwangvc.github.io/)<br>[Jue Wang](https://juewang725.github.io/)<br>[Nannan Wang](https://web.xidian.edu.cn/nnwang/en/index.html) | [![](https://camo.githubusercontent.com/926c0badff80a19840913c3241a4bf761a3db92e2012875150a4e869bc927cb3/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333535303436392e33353535333939)](https://doi.org/10.1145/3550469.3555399)[![](https://camo.githubusercontent.com/8669df6747cbc591b8f5892af7a5dddf01605dc4191a745aa4e6e5c3cc04f56a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4f70656e54616c6b65722f766964656f2d726574616c6b696e673f7374796c653d736f6369616c)](https://github.com/OpenTalker/video-retalking)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.14758)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/donydchen/ganimation_replicate), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/RenYurui/PIRender), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/OpenTalker/StyleHEAT), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/FeiiYin/SPI)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://xthemadgenius.medium.com/making-videos-talk-right-syncing-lips-with-sound-using-videoretalking-611428084bbc)<br>- [project](https://opentalker.github.io/video-retalking/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/178krha/videoretalking/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/pttsTrQ-fko), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/2Lkw8AmmRn0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/RJ8YK_K4Ne0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/vinthony/video-retalking/blob/main/quick_demo.ipynb) | 19.03.2023 |
| Visual ChatGPT | Connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting | - [Chenfei Wu](https://github.com/chenfei-wu)<br>- [Shengming Yin](https://github.com/shengming-yin)<br>- [Weizhen Qi](https://github.com/WeizhenQ)<br>- [Xiaodong Wang](https://wang-xiaodong1899.github.io/)<br>others[Zecheng Tang](https://github.com/CODINNLG)<br>[Nan Duan](https://nanduan.github.io/) | [![](https://camo.githubusercontent.com/d65d13f6190269b204396f9fe0d885826b0be26936a67dbb737ac59d1e89188a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f76697375616c2d636861746770743f7374796c653d736f6369616c)](https://github.com/microsoft/visual-chatgpt)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.04671)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hwchase17/langchain), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lllyasviel/ControlNet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/timothybrooks/instruct-pix2pix), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/timojl/clipseg)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0UfXlFUwLms), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7YEiEyfPF5U) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/11BtP3h-w0dZjA-X8JsS9_eo8OeGYvxXB) | 15.03.2023 |
| Tune-A-Video | One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation | - [Jay Zhangjie Wu](https://zhangjiewu.github.io/)<br>- [Yixiao Ge](https://geyixiao.com/)<br>- [Xintao Wang](https://xinntao.github.io/)<br>- [Stan Weixian Lei](https://github.com/StanLei52)<br>others[Yuchao Gu](https://ycgu.site/)<br>[Yufei Shi](https://scholar.google.com/citations?user=rpnlkwEAAAAJ)<br>[Wynne Hsu](https://www.comp.nus.edu.sg/~whsu/)<br>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)<br>[Xiaohu Qie](https://scholar.google.com/citations?user=mk-F69UAAAAJ)<br>[Mike Zheng Shou](https://sites.google.com/view/showlab) | [![](https://camo.githubusercontent.com/797ee7d64068e9b1fe4cda5a2a3844e9e8727e82490c6d1c50d8fdbfd702e7c6/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435635313037302e323032332e3030373031)](https://doi.org/10.1109/ICCV51070.2023.00701)[![](https://camo.githubusercontent.com/c964821c0991e2325b4db1374be3e1f1feeb03ffba7f1b27caead1d886235880/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73686f776c61622f54756e652d412d566964656f3f7374796c653d736f6369616c)](https://github.com/showlab/Tune-A-Video)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.11565), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10752)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/Tune-A-Video-library), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/stable-diffusion-2-1), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/sd-dreambooth-library)<br>- [project](https://tuneavideo.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uzF6CTtjn-g), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uUlp1_ExsGQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/showlab/Tune-A-Video/blob/main/notebooks/Tune-A-Video.ipynb) | 23.02.2023 |
| GPEN | GAN Prior Embedded Network for Blind Face Restoration in the Wild | - [Tao Yang](https://cg.cs.tsinghua.edu.cn/people/~tyang/)<br>- [Peiran Ren](https://scholar.google.com/citations?&user=x5dEuxsAAAAJ)<br>- [Xuansong Xie](https://scholar.google.com/citations?user=M0Ei1zkAAAAJ)<br>- [Lei Zhang](http://www4.comp.polyu.edu.hk/~cslzhang/) | [![](https://camo.githubusercontent.com/77f93928c9513d84f3665bba7283e322faad867f1556b3839360c09bc20a3aed/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f79616e6778792f4750454e3f7374796c653d736f6369616c)](https://github.com/yangxy/GPEN) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.06070)<br>- [demo](https://vision.aliyun.com/experience/detail?spm=a211p3.14020179.J_7524944390.17.66cd4850wVDkUQ&tagName=facebody&children=EnhanceFace)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/biubug6/Pytorch_Retinaface), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/yangxy/GPEN/blob/main/GPEN.ipynb) | 15.02.2023 |
| PyMAF-X | Кegression-based approach to recovering parametric full-body models from monocular images | - [Hongwen Zhang](https://hongwenzhang.github.io/)<br>- [Yating Tian](https://github.com/tinatiansjz)<br>- [Yuxiang Zhang](https://zhangyux15.github.io/)<br>- [Mengcheng Li](https://github.com/Dw1010)<br>others[Liang An](https://anl13.github.io/)<br>[Zhenan Sun](http://www.cbsr.ia.ac.cn/users/znsun/)<br>[Yebin Liu](https://www.liuyebin.com/) | [![](https://camo.githubusercontent.com/40de3479ada4150ddd399f72342755f854845a8017ba3b89e0e55152e5bce934/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5450414d492e323032332e33323731363931)](https://doi.org/10.1109/TPAMI.2023.3271691)[![](https://camo.githubusercontent.com/b4549e71de16b98bc325a3fe769edf59f28419e3e3647175f71a66d04030020e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f486f6e6777656e5a68616e672f50794d41462d583f7374796c653d736f6369616c)](https://github.com/HongwenZhang/PyMAF-X)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.06400)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/HongwenZhang/DaNet-DensePose2SMPL), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/DensePose), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Microsoft/human-pose-estimation.pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/MeshGraphormer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch)<br>- [project](https://www.liuyebin.com/pymaf-x/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ylOB0wCeV34) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/13Iytx1Hb0ZryEwbJdpXBW9ggDxs2Y-tL) | 14.02.2023 |
| Disco Diffusion | A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations | - [Max Ingham](https://github.com/somnai-dreams)<br>- [Adam Letts](https://linktr.ee/gandamu)<br>- [Daniel Russell](https://github.com/russelldc)<br>- [Chigozie Nri](https://github.com/chigozienri) | [![](https://camo.githubusercontent.com/550a1da78a772416060cf9121184866a6cd000304e4c4fe116abd6a4dc8c4549/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616c656d626963732f646973636f2d646966667573696f6e3f7374796c653d736f6369616c)](https://github.com/alembics/disco-diffusion)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/guided-diffusion)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_DtWfh9oS54), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gWxmtdZL8FE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/yVJB6oD0_gM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb) | 11.02.2023 |
| GrooVAE | Some applications of machine learning for generating and manipulating beats and drum performances | - [Jon Gillick](https://www.jongillick.com/)<br>- [Adam Roberts](https://github.com/adarob)<br>- [Jesse Engel](https://github.com/jesseengel) | [![](https://camo.githubusercontent.com/caa390a03471ee7a1f74b1153d656a7135c49848d73123b6c300b22c610c84f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6167656e74612f6d6167656e74613f7374796c653d736f6369616c)](https://github.com/magenta/magenta/tree/main/magenta/models/music_vae) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1905.06118)<br>- [blog post](https://g.co/magenta/groovae)<br>- [data](https://g.co/magenta/groove-datasets)<br>- [web app](https://groove-drums.glitch.me/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=x2YLmXzovDo) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/magenta-demos/blob/master/colab-notebooks/GrooVAE.ipynb) | 02.02.2023 |
| Multitrack MusicVAE | The models in this notebook are capable of encoding and decoding single measures of up to 8 tracks, optionally conditioned on an underlying chord | - [Ian Simon](https://github.com/iansimon)<br>- [Adam Roberts](https://github.com/adarob)<br>- [Colin Raffel](https://colinraffel.com//)<br>- [Jesse Engel](https://github.com/jesseengel)<br>others[Curtis Hawthorne](https://github.com/cghawthorne)<br>[Douglas Eck](https://github.com/douglaseck) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1806.00195)<br>- [blog post](http://g.co/magenta/multitrack) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) | 02.02.2023 |
| MusicVAE | A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music | - [Adam Roberts](https://github.com/adarob)<br>- [Jesse Engel](https://github.com/jesseengel)<br>- [Colin Raffel](https://colinraffel.com//)<br>- [Curtis Hawthorne](https://github.com/cghawthorne)<br>- [Douglas Eck](https://github.com/douglaseck) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1803.05428)<br>- [blog post](https://g.co/magenta/music-vae)<br>- [project](https://magenta.tensorflow.org/music-vae)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLBUMAYA6kvGU8Cgqh709o5SUvo-zHGTxr) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb) | 02.02.2023 |
| Learning to Paint | Learning to Paint With Model-based Deep Reinforcement Learning | [Manuel Romero](https://mrm8488.github.io/) | [![](https://camo.githubusercontent.com/ffd6c6429955b8381459ffd368ff939b9ea2faa139f6de9dab3de8171af0d4fa/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343562e323031392e3030383830)](https://doi.org/10.1109/ICCV.2019.00880)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1903.04411)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/reinforcementlearning/comments/b5lpfl/learning_to_paint_with_modelbased_deep/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=YmOgKZ5oipk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/custom_learningtopaint.ipynb) | 01.02.2023 |
| Instant-NGP | Instant Neural Graphics Primitives with a Multiresolution Hash Encoding | - [Thomas Müller](https://tom94.net/)<br>- [Alex Evans](https://research.nvidia.com/person/alex-evans)<br>- [Christoph Schied](https://research.nvidia.com/person/christoph-schied)<br>- [Alexander Keller](https://research.nvidia.com/person/alex-keller) | [![](https://camo.githubusercontent.com/fa902589fe27bac1966816cda6751feaee4572a1bf1e1be1727bf23a21c7f5a0/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333532383232332e33353330313237)](https://doi.org/10.1145/3528223.3530127) [![](https://camo.githubusercontent.com/b3687c4b443d2a6cca20e7166bfe8ca58b016b64bd37aa7123c9c4c5ed6cc9a1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e566c6162732f696e7374616e742d6e67703f7374796c653d736f6369616c)](https://github.com/NVlabs/instant-ngp) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.05989)<br>- [blog post](https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/tiny-cuda-nn), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IDLabMedia/large-lightfields-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nickponline/dd-nerf-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ocornut/imgui), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nothings/stb)<br>- [project](https://nvlabs.github.io/instant-ngp/)<br>- [tutorial](https://www.nvidia.com/en-us/on-demand/session/siggraph2022-sigg22-s-16/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/j8tMk-GE8hY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8GbENSmdVeE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/DJ2hcC1orc4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/z3-fjYzd0BA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/NVlabs/instant-ngp/blob/master/notebooks/instant_ngp.ipynb) | 18.01.2023 |
| Fourier Feature Networks | Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains | - [Matthew Tancik](https://www.matthewtancik.com/)<br>- [Pratul Srinivasan](https://pratulsrinivasan.github.io/)<br>- [Ben Mildenhall](https://bmild.github.io/)<br>- [Sara Fridovich-Keil](https://people.eecs.berkeley.edu/~sfk/)<br>others[Nithin Raghavan](https://cseweb.ucsd.edu//~n2raghavan/)<br>[Utkarsh Singhal](https://scholar.google.com/citations?user=lvA86MYAAAAJ)<br>[Ravi Ramamoorthi](https://cseweb.ucsd.edu//~ravir/)<br>[Jon Barron](https://jonbarron.info/)<br>[Ren Ng](https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html) | [![](https://camo.githubusercontent.com/f0d478a24e2913c1ee653d6d03936ecebc0469aee48c7dfcecaac0a090a958b1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74616e63696b2f666f75726965722d666561747572652d6e6574776f726b733f7374796c653d736f6369616c)](https://github.com/tancik/fourier-feature-networks)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1806.07572)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2020/hash/55053683268957697aa39fba6f231c68-Abstract.html), [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html)<br>- [project](https://bmild.github.io/fourfeat/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nVA6K6Sn2S4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tancik/fourier-feature-networks/blob/master/Demo.ipynb) | 17.01.2023 |
| AlphaPose | Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time | - [Hao-Shu Fang](https://fang-haoshu.github.io/)<br>- [Jiefeng Li](https://jeffli.site/)<br>- [Hongyang Tang](https://github.com/tang-hy)<br>- [Chao Xu](https://www.isdas.cn/)<br>others[Haoyi Zhu](https://www.haoyizhu.site/)<br>[Yuliang Xiu](https://xiuyuliang.cn/)<br>[Yong-Lu Li](https://dirtyharrylyl.github.io/)<br>[Cewu Lu](https://scholar.google.com/citations?user=QZVQEWAAAAAJ) | [![](https://camo.githubusercontent.com/8ad25e6585759c2f4f882bf7d70fb67b97b875d19125c470a6d8660b43629151/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5450414d492e323032322e33323232373834)](https://doi.org/10.1109/TPAMI.2022.3222784)[![](https://camo.githubusercontent.com/2a1986a250a5f12a795c26315de9730fb0494d4107efe0a69fb7c35c630cca1b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d5649472d534a54552f416c706861506f73653f7374796c653d736f6369616c)](https://github.com/MVIG-SJTU/AlphaPose)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.03375)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tycoer/AlphaPose_jittor), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Fang-Haoshu/Halpe-FullBody)<br>- [project](https://www.mvig.org/research/alphapose.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uze6chg-YeU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Z2WPd59pRi8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qW4lb9tnA3I), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_qtNzylm1XI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1_3Wxi4H3QGVC28snL3rHIoeMAwI2otMR) | 07.01.2023 |
| HybrIK | Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation | - [Jiefeng Li](https://jeffli.site/)<br>- [Chao Xu](https://www.isdas.cn/)<br>- [Zhicun Chen](https://github.com/chenzhicun)<br>- [Siyuan Bian](https://github.com/biansy000)<br>others[Lixin Yang](https://lixiny.github.io/)<br>[Cewu Lu](https://www.mvig.org/) | [![](https://camo.githubusercontent.com/25fc0c7044c4abb53cd2b481e9f6abbe7850f517e4352dc89701c5cbd4361b92/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3030333339)](https://doi.org/10.1109/CVPR46437.2021.00339) [![](https://camo.githubusercontent.com/b098d62f5a771e5aee711424d68cb6a1d8fe0b00595f4aacb9070a3ce96fb1be/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4a6566662d736a74752f48796272494b3f7374796c653d736f6369616c)](https://github.com/Jeff-sjtu/HybrIK) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2011.14672)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mks0601/3DMPPE_POSENET_RELEASE)<br>- [project](https://jeffli.site/HybrIK/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/3d-human-pose-estimation-on-3dpw?p=hybrik-a-hybrid-analytical-neural-inverse)<br>- [supp](https://openaccess.thecvf.com/content/CVPR2021/supplemental/Li_HybrIK_A_Hybrid_CVPR_2021_supplemental.zip)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/tvwnXXH7xIw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1n41l7I2NxWseuruVQEU8he2XqzSXhu2f) | 01.01.2023 |
| Score Jacobian Chaining | Apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instantiate to be a voxel radiance field | - [Haochen Wang](https://whc.is/)<br>- [Xiaodan Du](https://xiaodan.io/)<br>- [Jiahao Li](https://jiahao.ai/)<br>- [Raymond Yeh](https://raymond-yeh.com/)<br>- [Greg Shakhnarovich](https://home.ttic.edu/~gregory/) | [![](https://camo.githubusercontent.com/802b0a452b110616ebf030368d82b3264f401e5f875f59ba58e4b19fc97689f9/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323732392e323032332e3031323134)](https://doi.org/10.1109/CVPR52729.2023.01214)[![](https://camo.githubusercontent.com/d9c711dbe7564f699165a32444f9848018af5fcab7fa6cb9c3d25c3c9ac0bbc4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f70616c732d747469632f736a633f7374796c653d736f6369616c)](https://github.com/pals-ttic/sjc)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.00774), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2206.00364)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/MirageML/sjc)<br>- [project](https://pals.ttic.edu/p/score-jacobian-chaining)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/zac8z4/score_jacobian_chaining_lifting_pretrained_2d/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MmDSLc6CjoI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1oeruRLKoiU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1zixo66UYGl70VOPy053o7IV_YkQt5lCZ) | 05.12.2022 |
| Demucs | Hybrid Spectrogram and Waveform Source Separation | [Alexandre Défossez](https://ai.honu.io/) | [![](https://camo.githubusercontent.com/e683a39dadad13cb67a1b092190361dfa0e4f0d536be6582a6cfd026f41b7b51/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f64656d7563733f7374796c653d736f6369616c)](https://github.com/facebookresearch/demucs)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.03600), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.01733), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.05418), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1805.02410)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/adefossez/mdx21_demucs), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CarlGao4/Demucs-Gui), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kuielab/mdx-net-submission), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/f90/Wave-U-Net) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1dC9nVxk3V_VPjUADsnFu8EiT-xnU1tGH) | 21.11.2022 |
| StyleCLIP | Text-Driven Manipulation of StyleGAN Imager | - [Or Patashnik](https://orpatashnik.github.io/)<br>- [Zongze Wu](https://github.com/betterze)<br>- [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)<br>- [Daniel Cohen-Or](https://danielcohenor.com/)<br>- [Dani Lischinski](https://pages.cs.huji.ac.il/danix/) | [![](https://camo.githubusercontent.com/8f4248b48726939c58a2cdf7ad3c7a10e1af2ab93e4a9ed5bf6e9834aa323d29/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435634383932322e323032312e3030323039)](https://doi.org/10.1109/ICCV48922.2021.00209)[![](https://camo.githubusercontent.com/add01861c92fabea05baec3f9b3fd4ee75bc03f8c6845dedbcef5d164c54bb11/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f727061746173686e696b2f5374796c65434c49503f7374796c653d736f6369616c)](https://github.com/orpatashnik/StyleCLIP)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.17249), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2011.12799)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5icI0NgALnQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PhR1gpXDu0w), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/d1OET63Ulwc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/RAXrwPskNso) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global_torch.ipynb) | 30.10.2022 |
| MotionDiffuse | The first diffusion model-based text-driven motion generation framework, which demonstrates several desired properties over existing methods | - [Mingyuan Zhang](https://mingyuan-zhang.github.io/)<br>- [Zhongang Cai](https://caizhongang.github.io/)<br>- [Liang Pan](https://github.com/paul007pl)<br>- [Fangzhou Hong](https://hongfz16.github.io/)<br>others[Xinying Guo](https://gxyes.github.io/)<br>[Lei Yang](https://scholar.google.com/citations?user=jZH2IPYAAAAJ)<br>[Ziwei Liu](https://liuziwei7.github.io/) | [![](https://camo.githubusercontent.com/37b9f9570e37b9b5147f443a5bbbe2801cdf7089e3540ed5d3d5c38416303a93/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d696e677975616e2d7a68616e672f4d6f74696f6e446966667573653f7374796c653d736f6369616c)](https://github.com/mingyuan-zhang/MotionDiffuse) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2208.15001)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/mingyuan/MotionDiffuse)<br>- [project](https://mingyuan-zhang.github.io/projects/MotionDiffuse.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/U5PTnw490SA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Dp6VsZp2ozKuu9ccMmsDjyij_vXfCYb3) | 13.10.2022 |
| VToonify | Leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details | - [Shuai Yang](https://williamyang1991.github.io/)<br>- [Liming Jiang](https://liming-jiang.com/)<br>- [Ziwei Liu](https://liuziwei7.github.io/)<br>- [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) | [![](https://camo.githubusercontent.com/13794e6452dc6f856b216ea69ac3e6879d5d06176758513e35b63bbf42862f37/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333535303435342e33353535343337)](https://doi.org/10.1145/3550454.3555437)[![](https://camo.githubusercontent.com/8aca517d608cc1951b7fa5fa3795bfd913328ad158b1032f3cd29b90fc66edb8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77696c6c69616d79616e67313939312f56546f6f6e6966793f7374796c653d736f6369616c)](https://github.com/williamyang1991/VToonify)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.11224), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2001.02890)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zllrunning/face-parsing.PyTorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zhujiapeng/LowRankGAN)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/PKUWilliamYang/VToonify), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/PKUWilliamYang/VToonify/tree/main/models)<br>- [project](https://www.mmlab-ntu.com/project/vtoonify/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0_OmVhDgYuY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](http://colab.research.google.com/github/williamyang1991/VToonify/blob/master/notebooks/inference_playground.ipynb) | 07.10.2022 |
| PyMAF | Pyramidal Mesh Alignment Feedback loop in regression network for well-aligned body mesh recovery and extend it for the recovery of expressive full-body models | - [Hongwen Zhang](https://hongwenzhang.github.io/)<br>- [Yating Tian](https://github.com/tinatiansjz)<br>- [Yuxiang Zhang](https://zhangyux15.github.io/)<br>- [Mengcheng Li](https://github.com/Dw1010)<br>others[Liang An](https://anl13.github.io/)<br>[Zhenan Sun](http://www.cbsr.ia.ac.cn/users/znsun/)<br>[Yebin Liu](https://www.liuyebin.com/) | [![](https://camo.githubusercontent.com/94843095288d5c0e166d96f84f7e5a38e2c4c42d74d24a025249b608128af238/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f486f6e6777656e5a68616e672f50794d41463f7374796c653d736f6369616c)](https://github.com/HongwenZhang/PyMAF)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.06400), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.16507)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/eft), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/HongwenZhang/DaNet-DensePose2SMPL), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/DensePose), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Microsoft/human-pose-estimation.pytorch)<br>- [project](https://www.liuyebin.com/pymaf-x/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/yqEmznSKjYI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ylOB0wCeV34) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/11RXLsH9BdoSCwY6G-IX7KgqDxVoImu6K) | 06.10.2022 |
| AlphaTensor | Discovering faster matrix multiplication algorithms with reinforcement learning | - [Alhussein Fawzi](http://www.alhusseinfawzi.info/)<br>- [Matej Balog](http://matejbalog.eu/)<br>- [Aja Huang](https://en.wikipedia.org/wiki/Aja_Huang)<br>- [Thomas Hubert](https://scholar.google.com/citations?user=WXG0QfMAAAAJ)<br>others[Bernardino Romera-Paredes](https://sites.google.com/site/romeraparedes/)<br>[Mohammadamin Barekatain](http://barekatain.me/)<br>[Alexander Novikov](https://scholar.google.com/citations?user=jMUkLqwAAAAJ)<br>[Francisco Ruiz](https://franrruiz.github.io/)<br>[Julian Schrittwieser](https://www.furidamu.org/)<br>[Grzegorz Swirszcz](https://sites.google.com/site/grzegorzswirszcz/home)<br>[David Silver](https://www.davidsilver.uk/)<br>[Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis)<br>[Pushmeet Kohli](https://sites.google.com/site/pushmeet/) | [![](https://camo.githubusercontent.com/b15aac575292c70af1275fc4cdb14d8a24805e4ae8c42dc555b9ec64b6b6327a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313033382f7334313538362d3032322d30353137322d34)](https://doi.org/10.1038/s41586-022-05172-4)[![](https://camo.githubusercontent.com/af2c07817b9d843750a486eb613986de2c14f03d10f7a218e9fa603f78ff7e68/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d646565706d696e642f616c70686174656e736f723f7374796c653d736f6369616c)](https://github.com/google-deepmind/alphatensor)<br>- [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3N3Bl5AA5QU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gpYnDls4PdQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/IYgZS2EvnLI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8ILk4Wjo5rc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/alphatensor/blob/master/nonequivalence/inspect_factorizations_notebook.ipynb) | 04.10.2022 |
| Swin2SR | Novel Swin Transformer V2, to improve SwinIR for image super-resolution, and in particular, the compressed input scenario | - [Marcos Conde](https://mv-lab.github.io/)<br>- [Ui-Jin Choi](https://github.com/Choiuijin1125)<br>- [Maxime Burchi](https://scholar.google.com/citations?user=7S_l2eAAAAAJ)<br>- [Radu Timofte](https://www.informatik.uni-wuerzburg.de/computervision/home/) | [![](https://camo.githubusercontent.com/df44874b77ce8c1fb1d2e488239699a953f4ec35338737c24b5687832dc38c13/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d32353036332d375f3432)](https://doi.org/10.1007/978-3-031-25063-7_42)[![](https://camo.githubusercontent.com/198775d6db48cb4517924318db01e21fe713c7a63ed12cc445112de0e0f47d93/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d762d6c61622f7377696e3273723f7374796c653d736f6369616c)](https://github.com/mv-lab/swin2sr)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.11345), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.10257), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2208.11184), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.09883)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cszn/KAIR/), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mv-lab/AISP), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/Swin-Transformer)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/jjourney1125/swin2sr)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/jesucristo/super-resolution-demo-swin2sr-official/), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/datasets/jesucristo/super-resolution-benchmarks), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/jinssaa/official-swin2sr-demo-results/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1paPrt62ydwLv2U2eZqfcFsePI4X4WRR1) | 03.10.2022 |
| Functa | From data to functa: Your data point is a function and you can treat it like one | - [Emilien Dupont](https://emiliendupont.github.io/)<br>- [Hyunjik Kim](https://hyunjik11.github.io/)<br>- [Ali Eslami](http://arkitus.com/)<br>- [Danilo Rezende](https://danilorezende.com/about/)<br>- [Dan Rosenbaum](https://danrsm.github.io/) | [![](https://camo.githubusercontent.com/aba04de920740a3ff21caa50c89a938a84b125ca87520fc3e3df125f866a3625/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f66756e6374613f7374796c653d736f6369616c)](https://github.com/deepmind/functa)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.12204)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sxyu/pixel-nerf), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/jaxline)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/celeb_a_hq) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/functa/blob/main/modulation_visualization_colab.ipynb) | 24.09.2022 |
| Whisper | Automatic speech recognition system trained on 680,000 hours of multilingual and multitask supervised data collected from the web | - [Alec Radford](http://newmu.github.io/)<br>- [Jong Wook Kim](https://jongwook.kim/)<br>- [Tao Xu](https://github.com/bayesian)<br>- [Greg Brockman](https://gregbrockman.com/)<br>others[Christine McLeavey](http://christinemcleavey.com/)<br>[Ilya Sutskever](http://www.cs.toronto.edu/~ilya/) | [![](https://camo.githubusercontent.com/51657ac9ca7fad1589f7b58645272d798ce880a002b5360f2d84b43c58112cbe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e61692f776869737065723f7374796c653d736f6369616c)](https://github.com/openai/whisper) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.04356)<br>- [blog post](https://openai.com/research/whisper)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kkroening/ffmpeg-python)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/OCBZtgQGt1I), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8SQV-B83tPU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nE5iVtwKerA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb) | 21.09.2022 |
| DeOldify (video) | Colorize your own videos! | [Jason Antic](https://github.com/jantic) | [![](https://camo.githubusercontent.com/1f508eb9aa76825c7e64effcbdf050cb17442515b02160cbd7d984e28af86ced/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a616e7469632f44654f6c646966793f7374796c653d736f6369616c)](https://github.com/jantic/DeOldify)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1805.08318), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.08500)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42)<br>- [model](https://data.deepai.org/deoldify/ColorizeVideo_gen.pth)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/Nickelodeons/), [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/silentmoviegifs/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/DeOldify)<br>- [website](https://deoldify.ai/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](http://www.youtube.com/watch?v=l3UXXid04Ys), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](http://www.youtube.com/watch?v=EXn-n2iqEjI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb) | 19.09.2022 |
| DeOldify (photo) | Colorize your own photos! | - [Jason Antic](https://github.com/jantic)<br>- [Matt Robinson](https://github.com/mc-robinson)<br>- [María Benavente](https://github.com/mariabg) | [![](https://camo.githubusercontent.com/1f508eb9aa76825c7e64effcbdf050cb17442515b02160cbd7d984e28af86ced/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a616e7469632f44654f6c646966793f7374796c653d736f6369616c)](https://github.com/jantic/DeOldify)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1805.08318), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.08500)<br>- [model](https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/TheWayWeWere/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/DeOldify)<br>- [website](https://deoldify.ai/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb) | 19.09.2022 |
| Real-ESRGAN | Extend the powerful ESRGAN to a practical restoration application, which is trained with pure synthetic data | - [Xintao Wang](https://xinntao.github.io/)<br>- [Liangbin Xie](https://liangbinxie.github.io/)<br>- [Chao Dong](https://scholar.google.com/citations?user=OSDCB0UAAAAJ)<br>- [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ) | [![](https://camo.githubusercontent.com/27227b86825f3aaa07ea756e093ab595fc4930487cb1ea1f6591ed07e282c8da/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343565735343132302e323032312e3030323137)](https://doi.org/10.1109/ICCVW54120.2021.00217)[![](https://camo.githubusercontent.com/731141c47070692f63d7e670f8f81d5324a8c707f74996888d84a17d265ce4c6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f78696e6e74616f2f5265616c2d45535247414e3f7374796c653d736f6369616c)](https://github.com/xinntao/Real-ESRGAN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2107.10833)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/ESRGAN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/facexlib), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/HandyView), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Tencent/ncnn), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nihui/waifu2x-ncnn-vulkan) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo) | 18.09.2022 |
| IDE-3D | Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis | - [Jingxiang Sun](https://mrtornado24.github.io/)<br>- [Xuan Wang](https://xuanwangvc.github.io/)<br>- [Yichun Shi](https://seasonsh.github.io/)<br>- [Lizhen Wang](https://lizhenwangt.github.io/)<br>others[Jue Wang](https://juewang725.github.io/)<br>[Yebin Liu](http://www.liuyebin.com/) | [![](https://camo.githubusercontent.com/a0c3f9fd909be1183d0bab7bf8a9ae7060d478f27744f0a0ad6fecc964dfe8cb/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333535303435342e33353535353036)](https://doi.org/10.1145/3550454.3555506)[![](https://camo.githubusercontent.com/eceec5c03fc9b4486ae4760984e4a4e643a9e475cfad86015ea49f1614bda9de/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d72546f726e61646f32342f4944452d33443f7374796c653d736f6369616c)](https://github.com/MrTornado24/IDE-3D)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://arxiv.org/abs/2205.15517), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/eg3d), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan3)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Kj5XY_J2Alk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/MrTornado24/IDE-3D/blob/main/inversion/notebooks/inference_playground.ipynb) | 08.09.2022 |
| Decision Transformers | An architecture that casts the problem of RL as conditional sequence modeling | - [Lili Chen](http://www.lilichen.me/)<br>- [Kevin Lu](https://kzl.github.io/)<br>- [Aravind Rajeswaran](https://aravindr93.github.io/)<br>- [Kimin Lee](https://sites.google.com/view/kiminlee)<br>others[Aditya Grover](https://aditya-grover.github.io/)<br>[Michael Laskin](https://www.mishalaskin.com/)<br>[Pieter Abbeel](http://people.eecs.berkeley.edu/~pabbeel/)<br>[Aravind Srinivas](https://github.com/aravindsrinivas)<br>[Igor Mordatch](https://scholar.google.com/citations?user=Vzr1RukAAAAJ) | [![](https://camo.githubusercontent.com/1fb7349154ca6043cdb9ff6c433646766f39a5db6d54bd8c92c7090a8c4b5d78/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b7a6c2f6465636973696f6e2d7472616e73666f726d65723f7374796c653d736f6369616c)](https://github.com/kzl/decision-transformer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.01345)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/models?other=gym-continous-control), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/edbeeching/decision-transformer-gym-hopper-expert), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/model_doc/decision_transformer)<br>- [project](https://sites.google.com/berkeley.edu/decision-transformer)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Autoregressive_model)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/k08N5a0gG0A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-buULmf7dec), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/83QN9S-0I84), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/w4Bw8WYL8Ps) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1K3UuajwoPY1MzRKNkONNRS3gS5DxZ-qF) | 06.09.2022 |
| textual-inversion | An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion | - [Rinon Gal](https://rinongal.github.io/)<br>- [Yuval Alaluf](https://yuval-alaluf.github.io/)<br>- [Yuval Atzmon](https://research.nvidia.com/person/yuval-atzmon)<br>- [Or Patashnik](https://orpatashnik.github.io/)<br>others[Amit Bermano](https://www.cs.tau.ac.il/~amberman/)<br>[Gal Chechik](https://research.nvidia.com/person/gal-chechik)<br>[Daniel Cohen-Or](https://danielcohenor.com/) | [![](https://camo.githubusercontent.com/b6f26ffc7e488355d96ba11546d61089ce514302ed634383a32cf53ccac84bb1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f72696e6f6e67616c2f7465787475616c5f696e76657273696f6e3f7374796c653d736f6369616c)](https://github.com/rinongal/textual_inversion) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2208.01618)<br>- [project](https://textual-inversion.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/f3oXa7_SYek), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/opD_H9bED9Y) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/rinongal/textual_inversion/blob/master/scripts/latent_imagenet_diffusion.ipynb) | 21.08.2022 |
| StyleGAN-Human | A Data-Centric Odyssey of Human Generation | - [Jianglin Fu](https://github.com/arleneF)<br>- [Shikai Li](https://github.com/leeskyed)<br>- [Yuming Jiang](https://yumingj.github.io/)<br>- [Kwan-Yee Lin](https://kwanyeelin.github.io/)<br>others[Chen Qian](https://scholar.google.com/citations?user=AerkT0YAAAAJ)<br>[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)<br>[Wayne Wu](https://wywu.github.io/)<br>[Ziwei Liu](https://liuziwei7.github.io/) | [![](https://camo.githubusercontent.com/6e128f2534945b4d033f5aac0bc3e34f6ad627d49c4ebf20fb85531bad9db237/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d31393738372d315f31)](https://doi.org/10.1007/978-3-031-19787-1_1)[![](https://camo.githubusercontent.com/a8bfc5a46cf920b21f13df01eadda576ef3654b94b227fafc89203b7375636a8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7374796c6567616e2d68756d616e2f7374796c6567616e2d68756d616e3f7374796c653d736f6369616c)](https://github.com/stylegan-human/stylegan-human)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.11823)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2-ada-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan3)<br>- [project](https://stylegan-human.github.io/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/dataset/market-1501)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nIrb9hwsdcI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/86b49sCz0Gg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/g3nmM6MdxwY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/p2uwqh_SFL8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1sgxoDM55iM07FS54vz9ALg1XckiYA2On) | 19.08.2022 |
| Make-A-Scene | Scene-Based Text-to-Image Generation with Human Priors | - [Oran Gafni](https://github.com/ogafni)<br>- [Adam Polyak](https://scholar.google.com/citations?user=CP62OTMAAAAJ)<br>- [Oron Ashual](https://scholar.google.com/citations?user=CUA9JCkAAAAJ)<br>- [Shelly Sheynin](https://github.com/shellysheynin)<br>others[Devi Parikh](https://faculty.cc.gatech.edu/~parikh/)<br>[Yaniv Taigman](https://ytaigman.github.io/) | [![](https://camo.githubusercontent.com/bb523f7423bf5034bf7127c25a340ffc089036229a46881f4a3613010d456fc9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f43617375616c47414e5061706572732f4d616b652d412d5363656e653f7374796c653d736f6369616c)](https://github.com/CasualGANPapers/Make-A-Scene)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.13131)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ZM06MjPdoxw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1SPyQ-epTsAOAu8BEohUokN4-b5RM_TnE) | 12.08.2022 |
| StyleGAN-NADA | Zero-Shot non-adversarial domain adaptation of pre-trained generators | - [Rinon Gal](https://rinongal.github.io/)<br>- [Or Patashnik](https://orpatashnik.github.io/)<br>- [Haggai Maron](https://haggaim.github.io/)<br>- [Gal Chechik](https://research.nvidia.com/person/gal-chechik)<br>- [Daniel Cohen-Or](https://danielcohenor.com/) | [![](https://camo.githubusercontent.com/c15a540b3d807275e54b5ab18a48df303fa3f2b557b515d6822178cbf0829cb0/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333532383232332e33353330313634)](https://doi.org/10.1145/3528223.3530164)[![](https://camo.githubusercontent.com/800b274a8cb3ad1ce5a348f151aeff1e984df523b3b97cca5c33b6fc11d201f9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f72696e6f6e67616c2f5374796c6547414e2d6e6164613f7374796c653d736f6369616c)](https://github.com/rinongal/StyleGAN-nada)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.00946), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.17249), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.02699)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch/), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2-ada)<br>- [project](https://stylegan-nada.github.io/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/rinongal/stylegan-nada/blob/main/stylegan_nada.ipynb) | 09.08.2022 |
| YOLOv7 | Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors | - [Chien-Yao Wang](https://scholar.google.com/citations?user=DkQh4M4AAAAJ)<br>- [Alexey Bochkovskiy](http://www.alexeyab.com/)<br>- [Mark Liao](https://www.iis.sinica.edu.tw/pages/liao/) | [![](https://camo.githubusercontent.com/92e74c40ebf900c86bd5f6f6912709d0ceea64a356c2ade1c048df317c8283bd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f576f6e674b696e5969752f796f6c6f76373f7374796c653d736f6369616c)](https://github.com/WongKinYiu/yolov7) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.02696)<br>- [data](http://images.cocodataset.org/annotations/annotations_trainval2017.zip), [data](http://images.cocodataset.org/zips/train2017.zip), [data](http://images.cocodataset.org/zips/val2017.zip), [data](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/coco2017labels-segments.zip)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/WongKinYiu/yolor), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/WongKinYiu/PyTorch_YOLOv4), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/WongKinYiu/ScaledYOLOv4), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Megvii-BaseDetection/YOLOX), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DingXiaoH/RepVGG), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/JUGGHM/OREPA_CVPR2022), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=yolov7-trainable-bag-of-freebies-sets-new)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PL_Nji0JOuXg2QMohGK7wfzgJ-MavzXRHW), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-QWxJ0j9EY8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/WongKinYiu/yolov7/blob/main/tools/compare_YOLOv7_vs_YOLOv5m6_half.ipynb) | 09.08.2022 |
| GLIP | Grounded language-image pre-training model for learning object-level, language-aware, and semantic-rich visual representations | - [Liunian Harold Li](https://liunian-harold-li.github.io/)<br>- [Pengchuan Zhang](https://pzzhang.github.io/pzzhang/)<br>- [Haotian Zhang](https://haotian-zhang.github.io/)<br>- [Jianwei Yang](https://jwyang.github.io/)<br>others[Chunyuan Li](https://chunyuan.li/)<br>[Yiwu Zhong](https://pages.cs.wisc.edu/~yiwuzhong/)<br>[Lijuan Wang](https://github.com/LijuanWang)<br>[Lu Yuan](https://scholar.google.com/citations?user=k9TsUVsAAAAJ)<br>[Lei Zhang](https://www.leizhang.org/)<br>[Jenq-Neng Hwang](https://people.ece.uw.edu/hwang/)<br>[Kai-Wei Chang](http://web.cs.ucla.edu/~kwchang/)<br>[Jianfeng Gao](https://www.microsoft.com/en-us/research/people/jfgao/) | [![](https://camo.githubusercontent.com/e979ecdfb47e425654465c2186bd25ae0e442fa789708c8ef8ddef3e96772b0c/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031303639)](https://doi.org/10.1109/CVPR52688.2022.01069)[![](https://camo.githubusercontent.com/a05401264b578f3f6800e1c605b461ea7ffb05e2b4eab430747f9a25d5557065/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f474c49503f7374796c653d736f6369616c)](https://github.com/microsoft/GLIP)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.03857), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2206.05836), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.01066), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.08790)<br>- [blog post](https://www.microsoft.com/en-us/research/project/project-florence-vl/articles/object-detection-in-the-wild-via-grounded-language-image-pre-training/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/gligen/GLIGEN)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/harold/GLIP)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://sh-tsang.medium.com/glip-grounded-language-image-pre-training-2be2483295b3), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/zu1BGQBI4dU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/12x7v-_miN7-SRiziK3Cx4ffJzstBJNqb) | 30.07.2022 |
| Anycost GAN | Interactive natural image editing | - [Ji Lin](http://linji.me/)<br>- [Richard Zhang](https://richzhang.github.io/)<br>- [Frieder Ganz](https://scholar.google.com/citations?user=u9ySZkUAAAAJ)<br>- [Song Han](https://songhan.mit.edu/)<br>- [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/) | [![](https://camo.githubusercontent.com/a4e2bc7f30dd6769310477bc374103ef4cc61c01e3caf4e79673fc6e586aab33/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031343734)](https://doi.org/10.1109/CVPR46437.2021.01474)[![](https://camo.githubusercontent.com/70bd39fb523ea72fc2dab5998bb34acea0f4eebcb53093b657c5d99877f3f381/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d69742d68616e2d6c61622f616e79636f73742d67616e3f7374796c653d736f6369616c)](https://github.com/mit-han-lab/anycost-gan)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.03243)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/switchablenorms/CelebAMask-HQ), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fyu/lsun)<br>- [project](https://hanlab.mit.edu/projects/anycost-gan/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=_yEziPl9AkM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mit-han-lab/anycost-gan/blob/master/notebooks/intro_colab.ipynb) | 20.07.2022 |
| GFPGAN | Towards Real-World Blind Face Restoration with Generative Facial Prior | - [Xintao Wang](https://xinntao.github.io/)<br>- [Yu Li](https://yu-li.github.io/)<br>- [Honglun Zhang](https://scholar.google.com/citations?user=KjQLROoAAAAJ)<br>- [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ) | [![](https://camo.githubusercontent.com/d3697da3883889c6317212c023eba4583814d0b3bf62eade1980b83cd880a861/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3030393035)](https://doi.org/10.1109/CVPR46437.2021.00905)[![](https://camo.githubusercontent.com/ac0e568d14ba24fe240806ead658e4e6c6df6e51e104ea8eb7e3a01e06522070/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f54656e63656e744152432f47465047414e3f7374796c653d736f6369616c)](https://github.com/TencentARC/GFPGAN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2101.04061)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/facexlib), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/HandyView), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset)<br>- [project](https://xinntao.github.io/projects/gfpgan) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) | 13.07.2022 |
| EPro-PnP | Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation | - [Hansheng Chen](https://lakonik.github.io/)<br>- [Pichao Wang](https://wangpichao.github.io/)<br>- [Fan Wang](https://scholar.google.com/citations?user=WCRGTHsAAAAJ)<br>- [Wei Tian](https://scholar.google.com/citations?user=aYKQn88AAAAJ)<br>others[Lu Xiong](https://ieeexplore.ieee.org/author/37401835800)<br>[Hao Li](https://scholar.google.com/citations?user=pHN-QIwAAAAJ) | [![](https://camo.githubusercontent.com/0bae14b38f2b4a85d19f97e1faa3b3096c056bd15071e5e1425a6aea9ba57ddf/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5450414d492e323032342e33333534393937)](https://doi.org/10.1109/TPAMI.2024.3354997)[![](https://camo.githubusercontent.com/52633885a37da512d015cee7ff30a45e2b637aa73321164acae6913d0a406374/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f746a6969762d637072672f4550726f2d506e503f7374796c653d736f6369616c)](https://github.com/tjiiv-cprg/EPro-PnP)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.13254)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/megvii-research/petr), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/HuangJunJie2017/BEVDet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fudan-zvg/PolarFormer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zhiqi-li/BEVFormer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmdetection3d)<br>- [nuScenes](https://www.nuscenes.org/object-detection?externalData=no&mapData=no&modalities=Camera)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TonBodQ6EUU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tjiiv-cprg/EPro-PnP/blob/main/demo/fit_identity.ipynb) | 12.07.2022 |
| Text2Human | Text-driven controllable framework for a high-quality and diverse human generation | - [Yuming Jiang](https://yumingj.github.io/)<br>- [Shuai Yang](https://williamyang1991.github.io/)<br>- [Haonan Qiu](http://haonanqiu.com/)<br>- [Wayne Wu](https://wywu.github.io/)<br>others[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)<br>[Ziwei Liu](https://liuziwei7.github.io/) | [![](https://camo.githubusercontent.com/67d0190b7c9fad7e6cee546b99f8ac7d3a105ccd433b3ab320650c495d3ff6ee/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333532383232332e33353330313034)](https://doi.org/10.1145/3528223.3530104)[![](https://camo.githubusercontent.com/2cdff360ecaa0a2b2134bb80eb26ae9ff39893c0f013414f4843a4e333d16117/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f79756d696e676a2f546578743248756d616e3f7374796c653d736f6369616c)](https://github.com/yumingj/Text2Human)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.15996)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yumingj/DeepFashion-MultiModal), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/samb-t/unleashing-transformers)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/hysts/Text2Human), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/CVPR/drawings-to-human)<br>- [project](https://yumingj.github.io/projects/Text2Human.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/yKh4VORA_E0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/RV-g5BlH3Zg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1AVwbqLwMp_Gz3KTCgBTtnGVtXIlCZDPk) | 04.07.2022 |
| VQ-Diffusion | Based on a VQ-VAE whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model | - [Shuyang Gu](https://github.com/cientgu)<br>- [Dong Chen](http://www.dongchen.pro/)<br>- [Jianmin Bao](https://jianminbao.github.io/)<br>- [Fang Wen](https://www.microsoft.com/en-us/research/people/fangwen/)<br>others[Bo Zhang](https://bo-zhang.me/)<br>[Dongdong Chen](http://www.dongdongchen.bid/)<br>[Lu Yuan](https://scholar.google.com/citations?&user=k9TsUVsAAAAJ)<br>[Baining Guo](https://scholar.google.com/citations?user=h4kYmRYAAAAJ)<br>[Shuyang Gu](https://github.com/cientgu)<br>[Zhicong Tang](https://github.com/zzctan) | [![](https://camo.githubusercontent.com/ec97594cd899a31e2cee57dcc54fa18f7bc1a4162e8f6e6eb65108a5247cd7fa/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031303433)](https://doi.org/10.1109/CVPR52688.2022.01043)[![](https://camo.githubusercontent.com/b44e8e1c5da2dfcbe7c4729bb86875459cf941adc6b5c8202e1cd30ba02e46ae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f56512d446966667573696f6e3f7374796c653d736f6369616c)](https://github.com/microsoft/VQ-Diffusion)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.14822), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.16007)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ehoogeboom/multinomial_diffusion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/improved-diffusion) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Ws0_wK2cnsWEnfB7HtmPT4bjCPElb40C) | 30.06.2022 |
| OPT | Open Pre-trained Transformers is a family of NLP models trained on billions of tokens of text obtained from the internet | - [Susan Zhang](https://github.com/suchenzang)<br>- [Stephen Roller](https://stephenroller.com/)<br>- [Naman Goyal](https://github.com/ngoyal2707)<br>- [Mikel Artetxe](https://github.com/artetxem)<br>others[Moya Chen](https://moyachen.com/)<br>[Christopher Dewan](https://github.com/m3rlin45)<br>[Mona Diab](https://scholar.google.com/citations?user=-y6SIhQAAAAJ)<br>[Xi Victoria Lin](http://victorialin.net/)<br>[Todor Mihaylov](https://github.com/tbmihailov)<br>[Myle Ott](https://myleott.com/)<br>[Sam Shleifer](https://github.com/sshleifer)<br>[Kurt Shuster](https://github.com/klshuster)<br>[Daniel Simig](https://scholar.google.com/citations?user=TtWU9fsAAAAJ)<br>[Punit Singh Koura](https://github.com/punitkoura)<br>[Anjali Sridhar](https://www.linkedin.com/in/anjalisridhar/)<br>[Tianlu Wang](https://tianlu-wang.github.io/)<br>[Luke Zettlemoyer](https://www.cs.washington.edu/people/faculty/lsz/) | [![](https://camo.githubusercontent.com/05977366ecddb44d294f731afa9d8fa91237305a68f3e9c8cb6bfcc166b8118d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f6d6574617365713f7374796c653d736f6369616c)](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.01068), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1906.02243), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.10350), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.11990)<br>- [blog post](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/Megatron-LM)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Ejg0OunCi9U) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/14wnxMvD9zsiBQo2FtTpxn6w2cpXCcb-7) | 29.06.2022 |
| Customizing a Transformer Encoder | We will learn how to customize the encoder to employ new network architectures | [Chen Chen](https://github.com/chenGitHuber) | [![](https://camo.githubusercontent.com/a05c6a10141aa5829d7218a8c698391289a603709741d593b3f9f8005a8ce9e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f6d6f64656c733f7374796c653d736f6369616c)](https://github.com/tensorflow/models/tree/master/official/nlp/modeling)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.03762)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/encoder_scaffold.py) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/nlp/customize_encoder.ipynb) | 22.06.2022 |
| MTTR | End-to-End Referring Video Object Segmentation with Multimodal Transformers | - [Adam Botach](https://www.linkedin.com/in/adam-botach)<br>- [Evgenii Zheltonozhskii](https://evgeniizh.com/)<br>- [Chaim Baskin](https://github.com/chaimbaskin) | [![](https://camo.githubusercontent.com/f538484a89751c9bb101ade650da123fdfa3f65ab0eee3d78d689014d662d016/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030343933)](https://doi.org/10.1109/CVPR52688.2022.00493)[![](https://camo.githubusercontent.com/b810f624ae18d989a1b4b9682e0e27cd6edde6685847e6d6a99652af3de2b97c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d747472323032312f4d5454523f7374796c653d736f6369616c)](https://github.com/mttr2021/MTTR)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.14821), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.11692), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.13230)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/SwinTransformer/Video-Swin-Transformer)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/MTTR/MTTR-Referring-Video-Object-Segmentation)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YqlhXgq6hcs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/12p0jpSx3pJNfZk-y_L44yeHZlhsKVra-) | 20.06.2022 |
| SwinIR | Image Restoration Using Swin Transformer | - [Jingyun Liang](https://jingyunliang.github.io/)<br>- [Jiezhang Cao](https://github.com/caojiezhang)<br>- [Guolei Sun](https://github.com/GuoleiSun)<br>- [Kai Zhang](https://cszn.github.io/)<br>others[Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ)<br>[Radu Timofte](https://www.informatik.uni-wuerzburg.de/computervision/home/) | [![](https://camo.githubusercontent.com/ece466f20be873aae4835f38d6cde52bca853acae72c65b006f8938fab791a4d/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343565735343132302e323032312e3030323130)](https://doi.org/10.1109/ICCVW54120.2021.00210)[![](https://camo.githubusercontent.com/ab2f1518a76a4d94552ac0b112e0b6fe38b33d382da4437247b520f809767f08/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4a696e6779756e4c69616e672f5377696e49523f7374796c653d736f6369616c)](https://github.com/JingyunLiang/SwinIR)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.10257), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2107.10833)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cszn/BSRGAN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/Swin-Transformer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cszn/KAIR) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/gist/JingyunLiang/a5e3e54bc9ef8d7bf594f6fee8208533/swinir-demo-on-real-world-image-sr.ipynb) | 17.06.2022 |
| VRT | A Video Restoration Transformer | - [Jingyun Liang](https://jingyunliang.github.io/)<br>- [Jiezhang Cao](https://github.com/caojiezhang)<br>- [Yuchen Fan](https://ychfan.github.io/)<br>- [Kai Zhang](https://cszn.github.io/)<br>others[Yawei Li](https://ofsoundof.github.io/)<br>[Radu Timofte](https://www.informatik.uni-wuerzburg.de/computervision/home/)<br>[Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ) | [![](https://camo.githubusercontent.com/d1d950750b1e94ce05c0e4baf1c0efb9e91bec942729e1525d4a303c9fb7b15c/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5449502e323032342e33333732343534)](https://doi.org/10.1109/TIP.2024.3372454)[![](https://camo.githubusercontent.com/5981b560becb2fea81ff40f84ee24fa0b37c00b2af9b7d0afc32ca68f0746874/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4a696e6779756e4c69616e672f5652543f7374796c653d736f6369616c)](https://github.com/JingyunLiang/VRT)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.12288)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cszn/KAIR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/SwinTransformer/Video-Swin-Transformer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmediting) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/gist/JingyunLiang/deb335792768ad9eb73854a8efca4fe0/vrt-demo-on-video-restoration.ipynb) | 15.06.2022 |
| Omnivore | A single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters | - [Rohit Girdhar](http://rohitgirdhar.github.io/)<br>- [Mannat Singh](https://scholar.google.com/citations?user=QOO8OCcAAAAJ)<br>- [Nikhila Ravi](https://nikhilaravi.com/)<br>- [Laurens Maaten](https://lvdmaaten.github.io/)<br>others[Armand Joulin](https://ai.facebook.com/people/armand-joulin/)<br>[Ishan Misra](https://imisra.github.io/) | [![](https://camo.githubusercontent.com/5856f1675c4a60184228398016ec37a709fa5c80ffd2f4e8d589c95c6a8bf0ea/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031353633)](https://doi.org/10.1109/CVPR52688.2022.01563)[![](https://camo.githubusercontent.com/2015572827be1031755975837b98ba85fa5e2fd8fc1a1fe7a59e1487294859f0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f6f6d6e69766f72653f7374796c653d736f6369616c)](https://github.com/facebookresearch/omnivore)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.08377), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2206.08356)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/omnivore)<br>- [project](https://facebookresearch.github.io/omnivore/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/dataset/epic-kitchens-100) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/omnivore/blob/main/inference_tutorial.ipynb) | 14.06.2022 |
| Dream Fields | Zero-Shot Text-Guided Object Generation | - [Ajay Jain](https://ajayj.com/)<br>- [Ben Mildenhall](https://bmild.github.io/)<br>- [Jon Barron](https://jonbarron.info/)<br>- [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)<br>- [Ben Poole](https://cs.stanford.edu/~poole/) | [![](https://camo.githubusercontent.com/9c2fe04aaa1241dbf46a9631e3a1f909313cab67a913e6a25098e6f22653ff28/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030303934)](https://doi.org/10.1109/CVPR52688.2022.00094)[![](https://camo.githubusercontent.com/d133dccb193ebd202b15270c232696ecf695dfe76659dd32fb47e91659f29482/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f676f6f676c652d72657365617263683f7374796c653d736f6369616c)](https://github.com/google-research/google-research/tree/master/dreamfields)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.01455), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.00677), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.13415)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ajayjain/DietNeRF), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google/mipnerf)<br>- [project](https://ajayj.com/dreamfields)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1Fke6w46tv4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/17GtPqdUCbG5CsmTnQFecPpoq_zpNKX7A) | 10.06.2022 |
| Detic | Detecting Twenty-thousand Classes using Image-level Supervision | - [Xingyi Zhou](https://www.cs.utexas.edu/~zhouxy/)<br>- [Rohit Girdhar](https://rohitgirdhar.github.io/)<br>- [Armand Joulin](https://ai.facebook.com/people/armand-joulin/)<br>- [Philipp Krähenbühl](https://github.com/philkr)<br>- [Ishan Misra](https://imisra.github.io/) | [![](https://camo.githubusercontent.com/d498178eaf369a7aaa12e0d61b3a192808c21f9df947f812d47936b04e076036/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d32303037372d395f3231)](https://doi.org/10.1007/978-3-031-20077-9_21)[![](https://camo.githubusercontent.com/95ad4370e045e802804607068bb65654ba7a6c9f91f06b153501cb5e91c3eff7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f44657469633f7374796c653d736f6369616c)](https://github.com/facebookresearch/Detic)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.02605)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lvis-dataset/lvis-api) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1QtTW9-ukX2HKZGvt0QvVGqjuqEykoZKI) | 07.06.2022 |
| SimCTG | Contrastive training objective to calibrate the model's representation space, and a decoding method -- contrastive search -- to encourage diversity while maintaining coherence in the generated text | - [Yixuan Su](https://yxuansu.github.io/)<br>- [Tian Lan](https://github.com/gmftbyGMFTBY)<br>- [Yan Wang](https://libertywing.github.io/yanwang.github.io/)<br>- [Dani Yogatama](https://dyogatama.github.io/)<br>others[Lingpeng Kong](https://ikekonglp.github.io/)<br>[Nigel Collier](https://github.com/nhcollier) | [![](https://camo.githubusercontent.com/60abe8024e0a0d4deb049089f231807f0d1b2efc5275859442e9cc71ab15892d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f797875616e73752f53696d4354473f7374796c653d736f6369616c)](https://github.com/yxuansu/SimCTG)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.06417), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2210.14140)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yxuansu/Contrastive_Search_versus_Contrastive_Decoding), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/blog/introducing-csearch), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/joaogante/contrastive_search_generation), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/model_doc/opt)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/871cae8f599cb8bbfcb0f58fe1af95ad-Abstract-Conference.html)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/simctg/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1ImvR-ldHf9rJyFzOCMJ_zjAGK8n1iTW7) | 04.06.2022 |
| T0 | Multitask Prompted Training Enables Zero-Shot Task Generalization | - [Victor Sanh](https://github.com/VictorSanh)<br>- [Albert Webson](https://representation.ai/)<br>- [Colin Raffel](https://colinraffel.com//)<br>- [Stephen Bach](http://cs.brown.edu/people/sbach/)<br>others[Lintang Sutawika](https://github.com/lintangsutawika)<br>[Zaid Alyafeai](https://github.com/zaidalyafeai)<br>[Antoine Chaffin](https://antoine.chaffin.fr/)<br>[Arnaud Stiegler](https://github.com/arnaudstiegler)<br>[Teven Scao](https://scholar.google.com/citations?user=ik0_vxsAAAAJ)<br>[Arun Raja](https://www.arunraja.dev/)<br>[Manan Dey](https://github.com/manandey)<br>[M Saiful Bari](https://sbmaruf.github.io/)<br>[Canwen Xu](https://www.canwenxu.net/)<br>[Urmish Thakker](https://github.com/Urmish)<br>[Shanya Sharma](https://shanyas10.github.io/)<br>[Eliza Szczechla](https://elsanns.github.io/)<br>[Taewoon Kim](https://tae898.github.io/)<br>[Gunjan Chhablani](https://gchhablani.github.io/)<br>[Nihal Nayak](https://nihalnayak.github.io/)<br>[Debajyoti Datta](http://debajyotidatta.github.io/)<br>[Jonathan Chang](https://github.com/cccntu/)<br>[Mike Tian-Jian Jiang](https://github.com/tianjianjiang)<br>[Matteo Manica](https://github.com/drugilsberg)<br>[Sheng Shen](https://sincerass.github.io/)<br>[Zheng Xin Yong](https://yongzx.github.io/)<br>[Harshit Pandey](https://scholar.google.com/citations?user=BPIs78gAAAAJ)<br>[Rachel Bawden](https://rbawden.github.io/)<br>[Trishala Neeraj](https://github.com/trishalaneeraj)<br>[Jos Rozen](https://scholar.google.com/citations?user=OxEDKogAAAAJ)<br>[Abheesht Sharma](https://github.com/abheesht-sharma)<br>[Andrea Santilli](https://teelinsan.github.io/)<br>[Thibault Fevry](http://thibaultfevry.com/)<br>[Jason Alan Fries](https://web.stanford.edu/~jfries/)<br>[Ryan Teehan](https://github.com/rteehas)<br>[Stella Biderman](https://www.stellabiderman.com/)<br>[Leo Gao](https://github.com/leogao2)<br>[Tali Bers](https://github.com/tbers-coursera)<br>[Thomas Wolf](https://thomwolf.io/)<br>[Alexander Rush](https://scholar.google.com/citations?user=LIjnUGgAAAAJ) | [![](https://camo.githubusercontent.com/004da33f418a3b84ac2b292ef5b3e08bcd6757f27e5c0ec43b96f1f4fa4a8dda/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626967736369656e63652d776f726b73686f702f70726f6d7074736f757263653f7374796c653d736f6369616c)](https://github.com/bigscience-workshop/promptsource)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2110.08207)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/iJ0IVZgGjTM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YToXXfrIu6w) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1xx7SgdLaAu23YFBirXmaQViDr8caowX_) | 29.05.2022 |
| AvatarCLIP | A zero-shot text-driven framework for 3D avatar generation and animation | - [Fangzhou Hong](https://hongfz16.github.io/)<br>- [Mingyuan Zhang](https://scholar.google.com/citations?user=2QLD4fAAAAAJ)<br>- [Liang Pan](https://scholar.google.com/citations?user=lSDISOcAAAAJ)<br>- [Zhongang Cai](https://caizhongang.github.io/)<br>others[Lei Yang](https://scholar.google.com/citations?user=jZH2IPYAAAAJ)<br>[Ziwei Liu](https://liuziwei7.github.io/) | [![](https://camo.githubusercontent.com/99821eb97dbc546dd3e33f37d1c769afc84aaf11c268d4fe5a8ed5e0977d2443/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333532383232332e33353330303934)](https://doi.org/10.1145/3528223.3530094)[![](https://camo.githubusercontent.com/e21011f021f1aeebac84e97251b54a23d3d80cc3ed513b20e595157f8d768015/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f686f6e67667a31362f417661746172434c49503f7374796c653d736f6369616c)](https://github.com/hongfz16/AvatarCLIP)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.08535), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.01455), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.03221), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.05139), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.13333)<br>- [data](https://www.di.ens.fr/willow/research/surreal/data/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/daniilidis-group/neural_renderer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/GuyTevet/MotionCLIP), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Totoro97/NeuS), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vchoutas/smplx), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nghorbani/human_body_prior)<br>- [project](https://hongfz16.github.io/projects/AvatarCLIP.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-l2ZMeoASGY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1dfaecX7xF3nP6fyXc8XBljV5QY1lc1TR) | 15.05.2022 |
| Text2Mesh | Text-Driven Neural Stylization for Meshes | - [Oscar Michel](https://ojmichel.github.io/)<br>- [Roi Bar-On](https://github.com/roibaron)<br>- [Richard Liu](https://github.com/factoryofthesun)<br>- [Sagie Benaim](https://sagiebenaim.github.io/)<br>- [Rana Hanocka](http://people.cs.uchicago.edu/~ranahanocka/) | [![](https://camo.githubusercontent.com/301c15b8082f83bd60add2ede14bf473bf32728c63572491e3e8f760d17e2b47/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7468726565646c652f74657874326d6573683f7374796c653d736f6369616c)](https://github.com/threedle/text2mesh) <br>- [CLIP](https://openai.com/blog/clip/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.03221)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/neverix/text2mesh/notebook)<br>- [project](https://threedle.github.io/text2mesh/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/threedle/text2mesh/blob/master/colab_demo.ipynb) | 14.05.2022 |
| T5 | Text-To-Text Transfer Transformer | - [Colin Raffel](https://colinraffel.com/)<br>- [Noam Shazeer](https://scholar.google.com/citations?user=wsGvgA8AAAAJ)<br>- [Adam Roberts](https://github.com/adarob)<br>- [Katherine Lee](https://github.com/katelee168)<br>others[Sharan Narang](https://github.com/sharannarang)<br>[Michael Matena](https://scholar.google.com/citations?user=rN_9vroAAAAJ)<br>[Yanqi Zhou](https://zhouyanqi.github.io/)<br>[Wei Li](https://research.google/people/106528/)<br>[Peter J. Liu](https://scholar.google.com/citations?user=1EPxhywAAAAJ) | [![](https://camo.githubusercontent.com/660d6fc11cf187de8795b53eafa70ba1282f57d0e54e92e8e8d8ec00606d8755/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f746578742d746f2d746578742d7472616e736665722d7472616e73666f726d65723f7374796c653d736f6369616c)](https://github.com/google-research/text-to-text-transfer-transformer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.10683)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) | 11.05.2022 |
| XLS-R | Self-supervised Cross-lingual Speech Representation Learning at Scale | - [Arun Babu](https://github.com/arbabu123)<br>- [Changhan Wang](https://www.changhan.me/)<br>- [Andros Tjandra](https://github.com/androstj)<br>- [Kushal Lakhotia](https://about.me/hikushalhere)<br>others[Qiantong Xu](https://github.com/xuqiantong)<br>[Naman Goyal](https://github.com/ngoyal2707)<br>[Kritika Singh](https://scholar.google.com/citations?user=Ltk3SykAAAAJ)<br>[Patrick von Platen](https://github.com/patrickvonplaten)<br>[Yatharth Saraf](https://scholar.google.com/citations?user=KJTtNJwAAAAJ)<br>[Juan Pino](https://scholar.google.com/citations?user=weU_-4IAAAAJ)<br>[Alexei Baevski](https://github.com/alexeib)<br>[Alexis Conneau](https://github.com/aconneau)<br>[Michael Auli](https://github.com/michaelauli) | [![](https://camo.githubusercontent.com/12713baaa78572a511fbed8b2614003e623ed65b00d51871ac20a8e2c8b8a117/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f666169727365713f7374796c653d736f6369616c)](https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/xlsr/README.md) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.09296)<br>- [blog post](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/fairscale) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLS_R_on_Common_Voice.ipynb) | 10.05.2022 |
| MAGIC | Training-free framework, iMAge-Guided text generatIon with CLIP, for plugging in visual controls in the generation process and enabling LMs to perform multimodal tasks in a zero-shot manner | - [Yixuan Su](https://yxuansu.github.io/)<br>- [Tian Lan](https://github.com/gmftbyGMFTBY)<br>- [Yahui Liu](https://yhlleo.github.io/)<br>- [Fangyu Liu](https://fangyuliu.me/about)<br>others[Dani Yogatama](https://dyogatama.github.io/)<br>[Yan Wang](https://libertywing.github.io/yanwang.github.io/)<br>[Lingpeng Kong](https://www.cs.cmu.edu/~lingpenk/)<br>[Nigel Collier](https://sites.google.com/site/nhcollier/) | [![](https://camo.githubusercontent.com/ecbda0b1e9ff9f15cc27c3235b0e9ddcee36b803be456c537af1cbb4c8478293/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f797875616e73752f6d616769633f7374796c653d736f6369616c)](https://github.com/yxuansu/magic)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.02655) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1NDVkKpanbsaUwecHoRp_2kIpMztOFW25) | 02.05.2022 |
| DiffCSE | Unsupervised contrastive learning framework for learning sentence embeddings | - [Yung-Sung Chuang](https://people.csail.mit.edu/yungsung/)<br>- [Rumen Dangovski](http://super-ms.mit.edu/rumen.html)<br>- [Hongyin Luo](https://luohongyin.github.io/)<br>- [Yang Zhang](https://mitibmwatsonailab.mit.edu/people/yang-zhang/)<br>others[Shiyu Chang](https://code-terminator.github.io/)<br>[Marin Soljačić](http://www.mit.edu/~soljacic/marin.html)<br>[Shang-Wen Li](https://swdanielli.github.io/)<br>[Scott Wen-tau Yih](https://scottyih.org/)<br>[Yoon Kim](https://people.csail.mit.edu/yoonkim/)<br>[James Glass](http://groups.csail.mit.edu/sls/people/glass.shtml) | [![](https://camo.githubusercontent.com/c36c5074f7e8844bf41ed85ab93265bbe87853bb302b69aaa09385ce98afca5c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f766f696469736d2f646966666373653f7374796c653d736f6369616c)](https://github.com/voidism/diffcse)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.10298), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.08821), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.00899)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/princeton-nlp/SimCSE)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/voidism)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/YungSungChuang/status/1517518077902000129) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/voidism/DiffCSE/blob/master/diffcse_evaluation.ipynb) | 24.04.2022 |
| ViDT+ | An Extendable, Efficient and Effective Transformer-based Object Detector | - [Hwanjun Song](https://songhwanjun.github.io/)<br>- [Deqing Sun](https://deqings.github.io/)<br>- [Sanghyuk Chun](https://sanghyukchun.github.io/home/)<br>- [Varun Jampani](https://varunjampani.github.io/)<br>others[Dongyoon Han](https://sites.google.com/site/dyhan0920/)<br>[Byeongho Heo](https://sites.google.com/view/byeongho-heo/home)<br>[Wonjae Kim](https://wonjae.kim/)<br>[Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/) | [![](https://camo.githubusercontent.com/47cd254c017bef38ad222e78e80342babe66038f20e29ce1eaa6d8ee8f6d35fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e617665722d61692f766964743f7374796c653d736f6369616c)](https://github.com/naver-ai/vidt/tree/vidt-plus)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.07962), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2110.03921)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fundamentalvision/Deformable-DETR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/EherSenaw/ViDT_colab) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/EherSenaw/ViDT_colab/blob/main/vidt_colab.ipynb) | 20.04.2022 |
| BasicVSR++ | Redesign BasicVSR by proposing second-order grid propagation and flow-guided deformable alignment | - [Kelvin Chan](https://ckkelvinchan.github.io/)<br>- [Shangchen Zhou](https://shangchenzhou.com/)<br>- [Xiangyu Xu](https://xuxy09.github.io/)<br>- [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) | [![](https://camo.githubusercontent.com/ceac55c3b2fa82b60028abac737a8718c9d3ad675758537867115037354a0d5e/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030353838)](https://doi.org/10.1109/CVPR52688.2022.00588)[![](https://camo.githubusercontent.com/87c2638dcf4d0858316a3c98da53f87a32f5f75e29a6e302a2ba6f7dadb99641/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636b6b656c76696e6368616e2f42617369635653525f506c7573506c75733f7374796c653d736f6369616c)](https://github.com/ckkelvinchan/BasicVSR_PlusPlus)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.13371)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ckkelvinchan/BasicVSR-IconVSR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ckkelvinchan/offset-fidelity-loss)<br>- [project](https://ckkelvinchan.github.io/projects/BasicVSR++/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/iIDml09CUc4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1I0kZMM0DQyb4ueHZw5si8fMnRCJ_eUX3) | 18.04.2022 |
| NAFNet | Nonlinear Activation Free Network for Image Restoration | - [Liangyu Chen](https://github.com/mayorx)<br>- [Xiaojie Chu](https://github.com/chuxiaojie)<br>- [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ)<br>- [Jian Sun](http://www.jiansun.org/) | [![](https://camo.githubusercontent.com/6c5e2f6c4a606273afca1bad89bbb5dd7e4d4e05a7b70a259c0080f2f61d3156/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d32303037312d375f32)](https://doi.org/10.1007/978-3-031-20071-7_2)[![](https://camo.githubusercontent.com/255044f9f7d2134c24925dd7b58f13ea5539b9dfb10823c5bcccd1d2409283ac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d65677669692d72657365617263682f4e41464e65743f7374796c653d736f6369616c)](https://github.com/megvii-research/NAFNet)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.04676), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.08714)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/image-deblurring-on-gopro?p=simple-baselines-for-image-restoration), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/image-denoising-on-sidd?p=simple-baselines-for-image-restoration) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1dkO5AyktmBoWwxBwoKFUurIDn0m4qDXT) | 15.04.2022 |
| Panini-Net | GAN Prior based Degradation-Aware Feature Interpolation for Face Restoration | - [Yinhuai Wang](https://github.com/wyhuai)<br>- [Yujie Hu](https://villa.jianzhang.tech/people/yujie-hu/)<br>- [Jian Zhang](http://jianzhang.tech/) | [![](https://camo.githubusercontent.com/200070c4f7d5198dc84bf0a4060256d1ec8834988f9f685b59e212acb57b94af/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313630392f616161692e76333669332e3230313539)](https://doi.org/10.1609/aaai.v36i3.20159)[![](https://camo.githubusercontent.com/67e02bdc14a731c80fbe899d4aa72db1e0b814eae51aff9b9d9bde21145983da/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a69616e7a68616e6763732f70616e696e693f7374796c653d736f6369616c)](https://github.com/jianzhangcs/panini)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.08444)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tkarras/progressive_growing_of_gans) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/GeeveGeorge/Panini-Net-Colab/blob/main/PaniniNet_Working.ipynb) | 13.04.2022 |
| E2FGVI | An End-to-End framework for Flow-Guided Video Inpainting through elaborately designed three trainable modules, namely, flow completion, feature propagation, and content hallucination modules | - [Zhen Li](https://paper99.github.io/)<br>- [Cheng-Ze Lu](https://github.com/LGYoung)<br>- [Jianhua Qin](https://scholar.google.com/citations?&user=TAr7TU4AAAAJ)<br>- [Chun-Le Guo](https://scholar.google.com/citations?user=RZLYwR0AAAAJ)<br>- [Ming-Ming Cheng](https://mmcheng.net/) | [![](https://camo.githubusercontent.com/b37a362ce93b77f98b4cd30e7f2dd4b639f0d4fc27da14e15dd051888fe7638b/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031373034)](https://doi.org/10.1109/CVPR52688.2022.01704) [![](https://camo.githubusercontent.com/a285d0279dcfa5c14f01a241f90f93072cc8f6cd5037d13b307b10a29898e387/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d43472d4e4b552f4532464756493f7374796c653d736f6369616c)](https://github.com/MCG-NKU/E2FGVI) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.02663)<br>- [data](https://competitions.codalab.org/competitions/19544#participate-get-data), [data](https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/researchmm/STTN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/Focal-Transformer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ruiliu-ai/FuseFormer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/phoenix104104/fast_blind_video_consistency#evaluation)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/mlearning-ai/end-to-end-framework-for-flow-guided-video-inpainting-c5e2d8b61d20)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/N--qC3T2wc4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3eH3Fm6gOFk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/12rwY2gtG8jVWlNx9pjmmM8uGmh5ue18G) | 06.04.2022 |
| LDM | High-Resolution Image Synthesis with Latent Diffusion Models | - [Robin Rombach](https://github.com/rromb)<br>- [Andreas Blattmann](https://github.com/ablattmann)<br>- [Dominik Lorenz](https://github.com/qp-qp)<br>- [Patrick Esser](https://github.com/pesser)<br>- [Björn Ommer](https://ommer-lab.com/people/ommer/) | [![](https://camo.githubusercontent.com/8c4fa5252fc13ca4d91ee41af3758ba32cb46bdc152d22f93726c4eadd2e9e44/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031303432)](https://doi.org/10.1109/CVPR52688.2022.01042)[![](https://camo.githubusercontent.com/50d5f6cb61b901dc02a713203611024147fe02954bf9ec1d75b037e018af4dcc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6d705669732f6c6174656e742d646966667573696f6e3f7374796c653d736f6369616c)](https://github.com/CompVis/latent-diffusion)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10752), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.09778), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.02114)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fyu/lsun), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/guided-diffusion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/denoising-diffusion-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/x-transformers)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/multimodalart/latentdiffusion) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/CompVis/latent-diffusion/blob/master/scripts/latent_imagenet_diffusion.ipynb) | 04.04.2022 |
| GP-UNIT | Novel framework, Generative Prior-guided UNsupervised Image-to-image Translation, to improve the overall quality and applicability of the translation algorithm | - [Shuai Yang](https://williamyang1991.github.io/)<br>- [Liming Jiang](https://liming-jiang.com/)<br>- [Ziwei Liu](https://liuziwei7.github.io/)<br>- [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) | [![](https://camo.githubusercontent.com/3b74aec23588cfa5417ee190b766562c4913bc277c32b80433c7bf68f6a13687/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77696c6c69616d79616e67313939312f47502d554e49543f7374796c653d736f6369616c)](https://github.com/williamyang1991/GP-UNIT) <br>- [ImageNet](https://image-net.org/download.php)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.03641)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/clovaai/stargan-v2#datasets-and-pre-trained-networks), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/switchablenorms/CelebAMask-HQ), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/metfaces-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TreB1eN/InsightFace_Pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/SPADE), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nvlabs/imaginaire), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://doi.org/10.1109/CVPR52688.2022.01779)<br>- [project](https://www.mmlab-ntu.com/project/gpunit/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dDApWs_oDrM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/williamyang1991/GP-UNIT/blob/main/notebooks/inference_playground.ipynb) | 02.04.2022 |
| DualStyleGAN | More challenging exemplar-based high-resolution portrait style transfer by introducing a novel DualStyleGAN with flexible control of dual styles of the original face domain and the extended artistic portrait domain | - [Shuai Yang](https://williamyang1991.github.io/)<br>- [Liming Jiang](https://liming-jiang.com/)<br>- [Ziwei Liu](https://liuziwei7.github.io/)<br>- [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) | [![](https://camo.githubusercontent.com/92c92334027d0048ed11402cfeaf16633db91fb73514052c9bda5b0ecf9a04ee/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030373534)](https://doi.org/10.1109/CVPR52688.2022.00754) [![](https://camo.githubusercontent.com/11d1cd886705f2057993e3ed443e718acdb2a27104a5ebcf9ca590c4d7b40b90/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77696c6c69616d79616e67313939312f4475616c5374796c6547414e3f7374796c653d736f6369616c)](https://github.com/williamyang1991/DualStyleGAN) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.13248)<br>- [data](https://cs.nju.edu.cn/rl/WebCaricature.htm), [data](https://www.gwern.net/Crops#danbooru2019-portraits)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lowfuel/progrock-stable), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TreB1eN/InsightFace_Pytorch)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/Gradio-Blocks/DualStyleGAN), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/hysts/DualStyleGAN)<br>- [project](https://www.mmlab-ntu.com/project/dualstylegan/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/scZTu77jixI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/williamyang1991/DualStyleGAN/blob/master/notebooks/inference_playground.ipynb) | 24.03.2022 |
| CLIPasso | Semantically-Aware Object Sketching | - [Yael Vinker](https://yaelvi116.wixsite.com/mysite)<br>- [Ehsan Pajouheshgar](https://pajouheshgar.github.io/)<br>- [Jessica Y. Bo](https://jessica-bo.github.io/)<br>- [Roman Bachmann](https://roman-bachmann.github.io/)<br>others[Amit Bermano](https://www.cs.tau.ac.il/~amberman/)<br>[Daniel Cohen-Or](https://danielcohenor.com/)<br>[Amir Zamir](https://vilab.epfl.ch/zamir/)<br>[Ariel Shamir](https://faculty.runi.ac.il/arik/site/index.asp) | [![](https://camo.githubusercontent.com/706afc437e488a40f6dd0e85014e6e8d3827c43af9f111738069e85c704da7f3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7961656c2d76696e6b65722f434c49506173736f3f7374796c653d736f6369616c)](https://github.com/yael-vinker/CLIPasso)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.05822), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.14843)<br>- [demo](https://replicate.com/yael-vinker/clipasso)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BachiLi/diffvg)<br>- [project](https://clipasso.github.io/clipasso/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/yael-vinker/CLIPasso/blob/main/CLIPasso.ipynb) | 21.03.2022 |
| StyleSDF | A high resolution, 3D-consistent image and shape generation technique | - [Roy Or-El](https://homes.cs.washington.edu/~royorel/)<br>- [Xuan Luo](https://roxanneluo.github.io/)<br>- [Mengyi Shan](https://shanmy.github.io/)<br>- [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)<br>others[Jeong Joon Park](https://jjparkcv.github.io/)<br>[Ira Kemelmacher-Shlizerman](https://www.irakemelmacher.com/) | [![](https://camo.githubusercontent.com/581d0a65cc8ecae696bdf16ce998cb48cf7aa02085cbb1a7c719ac31a46bc26d/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031333134)](https://doi.org/10.1109/CVPR52688.2022.01314)[![](https://camo.githubusercontent.com/e7be994c5f7a8a71ab46811214c09f53c07846128c7d15bb9d2dd256ed9f05e7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726f796f72656c2f5374796c655344463f7374796c653d736f6369616c)](https://github.com/royorel/StyleSDF)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.11427)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yenchenlin/nerf-pytorch)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/SerdarHelli/StyleSDF-3D)<br>- [project](https://stylesdf.github.io/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/royorel/StyleSDF/blob/main/StyleSDF_demo.ipynb) | 05.03.2022 |
| Disentangled Lifespan Face Synthesis | LFS model is proposed to disentangle the key face characteristics including shape, texture and identity so that the unique shape and texture age transformations can be modeled effectively | - [Sen He](https://senhe.github.io/)<br>- [Wentong Liao](https://www.tnt.uni-hannover.de/en/staff/liao/)<br>- [Michael Yang](https://sites.google.com/site/michaelyingyang/)<br>- [Yi-Zhe Song](http://personal.ee.surrey.ac.uk/Personal/Y.Song/)<br>others[Bodo Rosenhahn](https://scholar.google.com/citations?user=qq3TxtcAAAAJ)<br>[Tao Xiang](http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html) | [![](https://camo.githubusercontent.com/0de9e40c9bfb83a0fb60b4b842818cd2b53fda703472a2f2648f607862aef43b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53656e48652f444c46533f7374796c653d736f6369616c)](https://github.com/SenHe/DLFS) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.02874)<br>- [project](https://senhe.github.io/projects/iccv_2021_lifespan_face/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=uklX03ns0m0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1fgVAoxCSaqPkj0rUK4RmBh7GTQRqLNpE) | 22.02.2022 |
| ClipCap | CLIP Prefix for Image Captioning | - [Ron Mokady](https://rmokady.github.io/)<br>- [Amir Hertz](https://github.com/amirhertz)<br>- [Amit Bermano](https://www.cs.tau.ac.il/~amberman/) | [![](https://camo.githubusercontent.com/0f90f812df093078bc01d3974b73463c638c5e1fcfe988831812bf9e36d1ade6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726d6f6b6164792f434c49505f7072656669785f63617074696f6e3f7374796c653d736f6369616c)](https://github.com/rmokady/CLIP_prefix_caption) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.09734)<br>- [data](https://cocodataset.org/)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/CLIP_prefix_captioning)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@uppalamukesh/clipcap-clip-prefix-for-image-captioning-3970c73573bc)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/VQDrmuccWDo) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/rmokady/CLIP_prefix_caption/blob/main/notebooks/clip_prefix_captioning_inference.ipynb#scrollTo=glBzYsgIwhwF) | 15.02.2022 |
| ROMP | Monocular, One-stage, Regression of Multiple 3D People | - [Yu Sun](https://www.yusun.work/)<br>- [Qian Bao](https://github.com/for-code0216)<br>- [Wu Liu](https://faculty.ustc.edu.cn/liuwu)<br>- [Yili Fu](https://ieeexplore.ieee.org/author/37286601800)<br>others[Michael Black](https://ps.is.mpg.de/~black)<br>[Tao Mei](https://taomei.me/) | [![](https://camo.githubusercontent.com/7094617c899c40b902947411aec3b07e02cc4e2f047081e0ff80ed13be63623b/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435634383932322e323032312e3031303939)](https://doi.org/10.1109/ICCV48922.2021.01099)[![](https://camo.githubusercontent.com/311c5b36213ff5908b9ace73696a919ecd966f909a5e101feac20dde73b56a72/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4172746875723135312f524f4d503f7374796c653d736f6369616c)](https://github.com/Arthur151/ROMP)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2008.12272), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.08274), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/2306.02850)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Arthur151/Relative_Human), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Arthur151/DynaCam), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yanchxx/MoPA)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hunBPJxnyBU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Q62fj_6AxRI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/l8aLHDXWQRw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1oz9E6uIbj4udOPZvA1Zi9pFx0SWH_UXg) | 11.02.2022 |
| Mask2Former | Masked-attention Mask Transformer for Universal Image Segmentation | - [Bowen Cheng](https://bowenc0221.github.io/)<br>- [Ishan Misra](https://imisra.github.io/)<br>- [Alexander Schwing](https://alexander-schwing.de/)<br>- [Alexander Kirillov](https://alexander-kirillov.github.io/)<br>- [Rohit Girdhar](https://rohitgirdhar.github.io/) | [![](https://camo.githubusercontent.com/3d5d7c73a37f166b3978ca5645d1d20c16e079e3f2487e90dec68f12a397a154/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030313335)](https://doi.org/10.1109/CVPR52688.2022.00135)[![](https://camo.githubusercontent.com/1a8f23adffc666de4b48f66a2f6ed7903a30b6d17007a1aabe6aaeec5efb4a6f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f4d61736b32466f726d65723f7374796c653d736f6369616c)](https://github.com/facebookresearch/Mask2Former)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.01527), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10764)<br>- [demo](https://replicate.com/facebookresearch/mask2former)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/MaskFormer)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/Mask2Former)<br>- [project](https://bowenc0221.github.io/mask2former/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1uIWE5KbGFSjrxey2aRd5pWkKNY1_SaNq) | 09.02.2022 |
| JoJoGAN | One Shot Face Stylization | - [Min Jin Chong](https://mchong6.github.io/)<br>- [David Forsyth](http://luthuli.cs.uiuc.edu/~daf/) | [![](https://camo.githubusercontent.com/b97e34f8dbb9891443f013a7ac8f6bd04b7a71e2ecfbb8f1721cb1c04e381bab/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d31393738372d315f38)](https://doi.org/10.1007/978-3-031-19787-1_8)[![](https://camo.githubusercontent.com/a3a6e97fd74de63c7aeb85029b34398dec03a43512e91e99ca08f611ab7d3800/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d63686f6e67362f4a6f4a6f47414e3f7374796c653d736f6369616c)](https://github.com/mchong6/JoJoGAN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.11641)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/replicate/cog) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/master/stylize.ipynb) | 02.02.2022 |
| Pose with Style | Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN | - [Badour AlBahar](https://badouralbahar.github.io/)<br>- [Jingwan Lu](https://research.adobe.com/person/jingwan-lu/)<br>- [Jimei Yang](https://github.com/jimeiyang)<br>- [Zhixin Shu](https://zhixinshu.github.io/)<br>others[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)<br>[Jia-Bin Huang](https://jbhuang0604.github.io/) | [![](https://camo.githubusercontent.com/b5aa2896e3717027307b2b50c48d01b709a21b23f89a0ecb4c78b7e6ec370707/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4261646f7572416c42616861722f706f73652d776974682d7374796c653f7374796c653d736f6369616c)](https://github.com/BadourAlBahar/pose-with-style) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.06166)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch)<br>- [project](https://pose-with-style.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/d_ETeAVLilw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/HomeStylist.ipynb) | 19.01.2022 |
| ConvNeXt | A pure ConvNet model constructed entirely from standard ConvNet modules | - [Zhuang Liu](https://liuzhuang13.github.io/)<br>- [Hanzi Mao](https://hanzimao.me/)<br>- [Chao-Yuan Wu](https://chaoyuan.org/)<br>- [Christoph Feichtenhofer](https://feichtenhofer.github.io/)<br>others[Trevor Darrell](https://people.eecs.berkeley.edu/~trevor/)<br>[Saining Xie](https://www.sainingxie.com/) | [![](https://camo.githubusercontent.com/4b031437079f37f50d20aeb205d5424ce9579163f952d00f6a4f23b7923b2637/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031313637)](https://doi.org/10.1109/CVPR52688.2022.01167)[![](https://camo.githubusercontent.com/7ddc4e87117148cb2a276b7b061f23bacc4a9799a2ac905b1b0fd8ea4f849df9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f436f6e764e6558743f7374796c653d736f6369616c)](https://github.com/facebookresearch/ConvNeXt)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.03545)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rwightman/pytorch-image-models), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/deit), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/unilm/tree/master/beit)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/convnext)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/QzCjXqFnWPE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/idiIllIQOfU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/QqejV0LNDHA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1CBYTIZ4tBMsVL5cqu9N_-Q3TBprqsfEO) | 19.01.2022 |
| diffsort | Differentiable Sorting Networks | - [Felix Petersen](http://petersen.ai/)<br>- [Christian Borgelt](https://borgelt.net/)<br>- [Hilde Kuehne](https://hildekuehne.github.io/)<br>- [Oliver Deussen](https://www.cgmi.uni-konstanz.de/personen/prof-dr-oliver-deussen/) | [![](https://camo.githubusercontent.com/62703eb939c8c172df71c15c373ebf78ebc03dfaf10b352c8e094bcf8f6c5bbf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f46656c69782d506574657273656e2f64696666736f72743f7374796c653d736f6369616c)](https://github.com/Felix-Petersen/diffsort)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.04019), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.09630)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Rl-sFaE1z4M) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1q0TZFFYB9FlOJYWKt0_7ZaXQT190anhm) | 17.01.2022 |
| Taming Transformers for High-Resolution Image Synthesis | We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer | - [Patrick Esser](https://github.com/pesser)<br>- [Robin Rombach](https://github.com/rromb)<br>- [Björn Ommer](https://ommer-lab.com/people/ommer/) | [![](https://camo.githubusercontent.com/683877407900aacb05b3ae2150350eaf744787350a8926714d632133595cc78e/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031323638)](https://doi.org/10.1109/CVPR46437.2021.01268) [![](https://camo.githubusercontent.com/68ac1528b862c8f6d57f5f93d54bec105425db19162b5416af93c35aca25391a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6d705669732f74616d696e672d7472616e73666f726d6572733f7374796c653d736f6369616c)](https://github.com/CompVis/taming-transformers) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.09841)<br>- [project](https://compvis.github.io/taming-transformers/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb) | 13.01.2022 |
| RealBasicVSR | Investigating Tradeoffs in Real-World Video Super-Resolution | - [Kelvin Chan](https://ckkelvinchan.github.io/)<br>- [Shangchen Zhou](https://shangchenzhou.com/)<br>- [Xiangyu Xu](https://xuxy09.github.io/)<br>- [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/) | [![](https://camo.githubusercontent.com/b96f7ecaf07b5f613e94b78f295ec247147c8ddc7bd777511623262630dc3b62/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030353837)](https://doi.org/10.1109/CVPR52688.2022.00587)[![](https://camo.githubusercontent.com/c1dac845df8e87ded36638c141bea69fea50f6c27b71c5af00a944a7a46e1d0b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636b6b656c76696e6368616e2f5265616c42617369635653523f7374796c653d736f6369616c)](https://github.com/ckkelvinchan/RealBasicVSR)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.12704)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/RealBasicVSR)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/tc8p70/rp_investigating_tradeoffs_in_realworld_video/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1JzWRUR34hpKvtCHm84IGx6nv35LCv20J) | 25.12.2021 |
| GLIDE | Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models | - [Alex Nichol](https://aqnichol.com/)<br>- [Prafulla Dhariwal](https://github.com/prafullasd)<br>- [Aditya Ramesh](http://adityaramesh.com/)<br>- [Pranav Shyam](https://github.com/pranv)<br>others[Pamela Mishkin](https://manlikemishap.github.io/)<br>[Bob McGrew](https://github.com/bmcgrew)<br>[Ilya Sutskever](http://www.cs.utoronto.ca/~ilya/)<br>[Mark Chen](https://scholar.google.com/citations?user=5fU-QMwAAAAJ) | [![](https://camo.githubusercontent.com/16c55bc6cd5961f05155be056a87b042faccbfde7bd571ca446b325e0693cb15/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e61692f676c6964652d7465787432696d3f7374796c653d736f6369616c)](https://github.com/openai/glide-text2im)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10741)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ItKi3h7IY2o) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/openai/glide-text2im/blob/master/notebooks/inpaint.ipynb) | 22.12.2021 |
| Nerfies | First method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones | - [Keunhong Park](https://keunhong.com/)<br>- [Utkarsh Sinha](https://utkarshsinha.com/)<br>- [Jon Barron](https://jonbarron.info/)<br>- [Sofien Bouaziz](http://sofienbouaziz.com/)<br>others[Dan Goldman](https://www.danbgoldman.com/home/)<br>[Steve Seitz](https://www.smseitz.com/)<br>[Ricardo Martin-Brualla](https://ricardomartinbrualla.com/) | [![](https://camo.githubusercontent.com/1af91e683d84c5205960703e4f99fac3d834b33c557eb9467f0e9129dbddc494/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435634383932322e323032312e3030353831)](https://doi.org/10.1109/ICCV48922.2021.00581) [![](https://camo.githubusercontent.com/29401bbe7e675448976a13add371129c6949e7edbf83cafd1ec98f1e9e44734a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6e6572666965733f7374796c653d736f6369616c)](https://github.com/google/nerfies) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2011.12948)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-research/google-research/tree/master/jaxnerf)<br>- [project](https://nerfies.github.io/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/photogrammetry/comments/k1i0ct/deformable_neural_radiance_fields_nerfies/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MrKrnHhk8IA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/IDMiMKWucaI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/nerfies/blob/main/notebooks/Nerfies_Capture_Processing.ipynb) | 06.12.2021 |
| HyperStyle | A hypernetwork that learns to modulate StyleGAN's weights to faithfully express a given image in editable regions of the latent space | - [Yuval Alaluf](https://yuval-alaluf.github.io/)<br>- [Omer Tov](https://github.com/omertov)<br>- [Ron Mokady](https://rmokady.github.io/)<br>- [Rinon Gal](https://rinongal.github.io/)<br>- [Amit Bermano](https://www.cs.tau.ac.il/~amberman/) | [![](https://camo.githubusercontent.com/dad062635a7357005076bcf346189f40f3f758a25983e4f0881e189e16e8c57f/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031373936)](https://doi.org/10.1109/CVPR52688.2022.01796)[![](https://camo.githubusercontent.com/fc5ff48298476d26ee0dacb45988cefaccc1851edc0e6ab5ef4d8d0fac868180/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f797576616c2d616c616c75662f68797065727374796c653f7374796c653d736f6369616c)](https://github.com/yuval-alaluf/hyperstyle)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.15666), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1904.03189), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.09036), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.07727)<br>- [data](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/clovaai/stargan-v2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TreB1eN/InsightFace_Pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/HuangYG123/CurricularFace), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/dvschultz/stylegan2-ada-pytorch)<br>- [project](https://yuval-alaluf.github.io/hyperstyle/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_sbXmLY2jMw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/yuval-alaluf/hyperstyle/blob/master/notebooks/inference_playground.ipynb) | 03.12.2021 |
| encoder4editing | Designing an Encoder for StyleGAN Image Manipulation | - [Omer Tov](https://github.com/omertov)<br>- [Yuval Alaluf](https://yuval-alaluf.github.io/)<br>- [Yotam Nitzan](https://yotamnitzan.github.io/)<br>- [Or Patashnik](https://orpatashnik.github.io/)<br>- [Daniel Cohen-Or](https://danielcohenor.com/) | [![](https://camo.githubusercontent.com/794c2bd422d653575e2f54d73477fd64f4adec710607dad733eeb5a3f2792ce7/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333435303632362e33343539383338)](https://doi.org/10.1145/3450626.3459838)[![](https://camo.githubusercontent.com/73789eabc452869884aeb457173b256424b1fd79ccb78b6b1d5aac0135542f14/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f6d6572746f762f656e636f6465723465646974696e673f7374796c653d736f6369616c)](https://github.com/omertov/encoder4editing)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.02766)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/eladrich/pixel2style2pixel) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/omertov/encoder4editing/blob/master/notebooks/inference_playground.ipynb) | 02.12.2021 |
| StyleCariGAN | Caricature Generation via StyleGAN Feature Map Modulation | - [Wonjong Jang](https://wonjongg.github.io/)<br>- [Gwangjin Ju](https://github.com/jugwangjin)<br>- [Yucheol Jung](https://ycjung.info/)<br>- [Jiaolong Yang](https://jlyang.org/)<br>others[Xin Tong](https://www.microsoft.com/en-us/research/people/xtong/)<br>[Seungyong Lee](https://scholar.google.com/citations?user=yGPH-nAAAAAJ) | [![](https://camo.githubusercontent.com/f0506fd9cf2ee9c040c85f74823e260b0442a0dfdc8b5fc2a4bd236054baee53/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333435303632362e33343539383630)](https://doi.org/10.1145/3450626.3459860)[![](https://camo.githubusercontent.com/91f97afd1b0883e685e5c199c7b0ac9af012f658982ec40b15fd9b5dbb968fa1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f776f6e6a6f6e67672f5374796c654361726947414e3f7374796c653d736f6369616c)](https://github.com/wonjongg/StyleCariGAN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2107.04331)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch)<br>- [project](https://wonjongg.github.io/StyleCariGAN/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=kpHbGOlI-BU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1HDRQGm7pvC9mAb6Lktoft_SmY9sCq_Qg) | 30.11.2021 |
| CartoonGAN | The implementation of the cartoon GAN model with PyTorch | [Tobias Sunderdiek](https://github.com/TobiasSunderdiek) | [![](https://camo.githubusercontent.com/0656e606d83c501031494ec2812d98ac18a14851a88b1729f683d1e12b090f6f/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f435650522e323031382e3030393836)](https://doi.org/10.1109/CVPR.2018.00986) <br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/alamson/safebooru)<br>- [project](https://tobiassunderdiek.github.io/cartoon-gan/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb) | 24.11.2021 |
| SimSwap | An efficient framework, called Simple Swap, aiming for generalized and high fidelity face swapping | - [Xuanhong Chen](https://github.com/neuralchen)<br>- [Bingbing Ni](https://scholar.google.com/citations?user=eUbmKwYAAAAJ)<br>- [Yanhao Ge](https://scholar.google.com/citations?user=h6tuBAcAAAAJ) | [![](https://camo.githubusercontent.com/ed8cb47743c236bd2ec66f0987c23ec262b205c45c02373b7a6cbd9bb2b32a8d/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333339343137312e33343133363330)](https://doi.org/10.1145/3394171.3413630)[![](https://camo.githubusercontent.com/7fac4b814389b7fe371d712a2be033c3a0b8ea47b3d9651183ecb8a56076d0c4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e657572616c6368656e2f53696d537761703f7374796c653d736f6369616c)](https://github.com/neuralchen/SimSwap)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.06340)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepinsight/insightface) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/neuralchen/SimSwap/blob/master/SimSwap%20colab.ipynb) | 24.11.2021 |
| RVM | Robust High-Resolution Video Matting with Temporal Guidance | - [Shanchuan Lin](https://github.com/PeterL1n)<br>- [Linjie Yang](https://sites.google.com/site/linjieyang89/)<br>- [Imran Saleemi](http://www.cs.ucf.edu/~imran/)<br>- [Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/) | [![](https://camo.githubusercontent.com/af7af2fff5348aa857e0715a98bab8da2fd97ad5af03c03d7a86642500b3eb17/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5741435635313435382e323032322e3030333139)](https://doi.org/10.1109/WACV51458.2022.00319)[![](https://camo.githubusercontent.com/6f861aacb6d534686756a37f6f5886588406404091842bb10248c212e52d5b17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f50657465724c316e2f526f62757374566964656f4d617474696e673f7374796c653d736f6369616c)](https://github.com/PeterL1n/RobustVideoMatting)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/2108.11515)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/VideoProcessingFramework), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/FeiGeChuanShu/ncnn_Android_RobustVideoMatting)<br>- [project](https://peterl1n.github.io/RobustVideoMatting)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Jvzltozpbpk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Ay-mGCEYEzM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm) | 24.11.2021 |
| RVM | Robust, real-time, high-resolution human video matting method that achieves new state-of-the-art performance | - [Shanchuan Lin](https://github.com/PeterL1n)<br>- [Linjie Yang](https://sites.google.com/site/linjieyang89)<br>- [Imran Saleemi](https://github.com/imran-saleemi)<br>- [Soumyadip Sengupta](https://github.com/senguptaumd) | [![](https://camo.githubusercontent.com/af7af2fff5348aa857e0715a98bab8da2fd97ad5af03c03d7a86642500b3eb17/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5741435635313435382e323032322e3030333139)](https://doi.org/10.1109/WACV51458.2022.00319) [![](https://camo.githubusercontent.com/6f861aacb6d534686756a37f6f5886588406404091842bb10248c212e52d5b17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f50657465724c316e2f526f62757374566964656f4d617474696e673f7374796c653d736f6369616c)](https://github.com/PeterL1n/RobustVideoMatting) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.11515)<br>- [project](https://peterl1n.github.io/RobustVideoMatting)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/pdbpmg/r_robust_highresolution_video_matting_with/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Jvzltozpbpk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Ay-mGCEYEzM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/VL-0K6HjhvQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Jhuf6M_VrBI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_oN9yyRi3HY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm) | 24.11.2021 |
| AnimeGANv2 | An improved version of AnimeGAN - it prevents the generation of high-frequency artifacts by simply changing the normalization of features in the network | - [Xin Chen](https://github.com/TachibanaYoshino)<br>- [Gang Liu](https://github.com/lg0061408)<br>- [bryandlee](https://github.com/bryandlee) | [![](https://camo.githubusercontent.com/e71d0621d2d76e749595f00c7443c96850ed33d3cecf65e7227e1b1382e741c4/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d3938312d31352d353537372d305f3138)](https://doi.org/10.1007/978-981-15-5577-0_18)[![](https://camo.githubusercontent.com/135268261f588ea2d5ccaa66671a1457b7b9393865d4f317765a082ff4f2a182/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f627279616e646c65652f616e696d6567616e322d7079746f7263683f7374796c653d736f6369616c)](https://github.com/bryandlee/animegan2-pytorch)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TachibanaYoshino/AnimeGANv2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TachibanaYoshino/AnimeGAN)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/AnimeGANv2)<br>- [project](https://tachibanayoshino.github.io/AnimeGANv2/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/bryandlee/animegan2-pytorch/blob/master/colab_demo.ipynb) | 17.11.2021 |
| SOAT | StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN | - [Min Jin Chong](https://mchong6.github.io/)<br>- [Hsin-Ying Lee](http://hsinyinglee.com/)<br>- [David Forsyth](http://luthuli.cs.uiuc.edu/~daf/) | [![](https://camo.githubusercontent.com/a4c281bf24ff8c69fbb5ac394dcfc21d33adcb29bd44ddc25e1536fa3be6520d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d63686f6e67362f534f41543f7374796c653d736f6369616c)](https://github.com/mchong6/SOAT)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.01619)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/justinpinkney/toonify), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/SOAT) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mchong6/SOAT/blob/master/infinity.ipynb) | 13.11.2021 |
| Arnheim | Generative Art Using Neural Visual Grammars and Dual Encoders | - [Chrisantha Fernando](https://www.chrisantha.co.uk/)<br>- [Ali Eslami](http://arkitus.com/)<br>- [Jean-Baptiste Alayrac](https://www.jbalayrac.com/)<br>- [Piotr Mirowski](https://piotrmirowski.com/)<br>others[Dylan Banarse](https://www.2ne1.com/)<br>[Simon Osindero](https://scholar.google.com/citations?user=Jq8ZS5kAAAAJ) | [![](https://camo.githubusercontent.com/0edba78a7f44c68eb2a1096d8aaa0c7f15682c743127b7b98e987b7f38fd0014/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f61726e6865696d3f7374796c653d736f6369616c)](https://github.com/deepmind/arnheim)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.00162), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.14843), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1801.07729), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1606.02580), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1609.09106)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/dall-e)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Compositional_pattern-producing_network)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=U7guaMdeF4g), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=zh0goLbS-l0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=SYJGNt7yu6M), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=MxkYKa0x5AU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/arnheim/blob/master/arnheim_2.ipynb) | 11.11.2021 |
| StyleGAN 2 | Generation of faces, cars, etc. | [Mikael Christensen](https://github.com/Syntopia) | [![](https://camo.githubusercontent.com/27f5849a7ec26f869cf77e380d67947f076a456bc781dba7cf6d489dc6f6e9d1/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030383133)](https://doi.org/10.1109/CVPR42600.2020.00813)[![](https://camo.githubusercontent.com/7b295c49944a7e9ffc5126375683f88b9f0ff638b3c4df64e074d18b6f42aaa8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e566c6162732f7374796c6567616e323f7374796c653d736f6369616c)](https://github.com/NVlabs/stylegan2)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/1912.04958)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/c-NJtV9Jvp0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH) | 05.11.2021 |
| ByteTrack | Multi-Object Tracking by Associating Every Detection Box | - [Yifu Zhang](https://github.com/ifzhang)<br>- [Peize Sun](https://peizesun.github.io/)<br>- [Yi Jiang](https://github.com/iFighting)<br>- [Dongdong Yu](https://miracle-fmh.github.io/)<br>others[Ping Luo](http://luoping.me/)<br>[Xinggang Wang](https://xinggangw.info/) | [![](https://camo.githubusercontent.com/3ef9dd33928453ecee81a3a3932f920fd957c23b710af3038bc41559c5312851/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d32303034372d325f31)](https://doi.org/10.1007/978-3-031-20047-2_1) [![](https://camo.githubusercontent.com/2471bb9f72e9239420b9b82c85321e075fa2579368c0954ddac1037677931226/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f69667a68616e672f42797465547261636b3f7374796c653d736f6369616c)](https://github.com/ifzhang/ByteTrack) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2110.06864)<br>- [data](https://motchallenge.net/), [data](https://www.crowdhuman.org/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Megvii-BaseDetection/YOLOX), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ifzhang/FairMOT), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PeizeSun/TransTrack), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/samylee/Towards-Realtime-MOT-Cpp)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/multi-object-tracking) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1bDilg4cmXFa8HCKHbsZ_p16p0vrhLyu0) | 30.10.2021 |
| GPT-2 | Retrain an advanced text generating neural network on any text dataset using gpt-2-simple! | [Max Woolf](https://minimaxir.com/) | [![](https://camo.githubusercontent.com/846a673b11ced56ec964a53e1539a16442dd5d6d138fd1874190e2f7fd82ca1f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e61692f6770742d323f7374796c653d736f6369616c)](https://github.com/openai/gpt-2) <br>- [blog post](https://minimaxir.com/2019/09/howto-gpt2/), [blog post](https://openai.com/research/better-language-models)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/minimaxir/gpt-2-simple)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/aqlzde/r_openai_better_language_models_and_their/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) | 18.10.2021 |
| ConvMixer | An extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network | - [Asher Trockman](http://ashertrockman.com/)<br>- [Zico Kolter](http://zicokolter.com/) | [![](https://camo.githubusercontent.com/4f129e54d88f1ef5be8e7165f5acc983da88d01904d889d760a61bfbbf11f84a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6f6375736c61622f636f6e766d697865723f7374796c653d736f6369616c)](https://github.com/locuslab/convmixer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.09792)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/locuslab/convmixer-cifar10), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rwightman/pytorch-image-models)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/codex/an-overview-on-convmixer-patches-are-all-you-need-8502a8d87011)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Gl0s0GDqN3c?t=990) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/locuslab/convmixer/blob/main/pytorch-image-models/notebooks/EffResNetComparison.ipynb) | 06.10.2021 |
| IC-GAN | Instance-Conditioned GAN | - [Arantxa Casanova](https://github.com/ArantxaCasanova)<br>- [Marlène Careil](https://www.linkedin.com/in/marl%C3%A8ne-careil-901804155)<br>- [Jakob Verbeek](http://thoth.inrialpes.fr/~verbeek/)<br>- [Michał Drożdżal](https://scholar.google.com/citations?user=XK_ktwQAAAAJ)<br>- [Adriana Romero-Soriano](https://sites.google.com/site/adriromsor) | [![](https://camo.githubusercontent.com/a7286a24927011fad4f852e8ab7e2fbfb4f00b07b88b33e4eb311b6a336f8a9c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f69635f67616e3f7374796c653d736f6369616c)](https://github.com/facebookresearch/ic_gan) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.05070)<br>- [blog post](https://ai.facebook.com/blog/instance-conditioned-gans/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/faiss), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ajbrock/BigGAN-PyTorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2-ada-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/bioinf-jku/TTUR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mit-han-lab/data-efficient-gans)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2021/hash/e7ac288b0f2d41445904d071ba37aaff-Abstract.html) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/ic_gan/blob/master/inference/icgan_colab.ipynb) | 01.10.2021 |
| Skillful Precipitation Nowcasting Using Deep Generative Models of Radar | Open-sourced dataset and model snapshot for precipitation nowcasting | - [Suman Ravuri](https://www.linkedin.com/in/suman-ravuri-81928082)<br>- [Karel Lenc](https://www.robots.ox.ac.uk/~karel/)<br>- [Matthew Willson](https://www.linkedin.com/in/matthew-willson-6a1b422)<br>- [Dmitry Kangin](https://scholar.google.com/citations?user=vv-leaMAAAAJ)<br>others[Rémi Lam](https://github.com/remilam)<br>[Piotr Mirowski](https://piotrmirowski.com/)<br>[Maria Athanassiadou](https://scholar.google.com/citations?user=VtkgHP0AAAAJ)<br>[Sheleem Kashem](https://www.linkedin.com/in/sheleemkashem/)<br>[Rachel Prudden](https://computerscience.exeter.ac.uk/staff/rep218)<br>[Amol Mandhane](https://github.com/amol-mandhane)<br>[Aidan Clark](https://scholar.google.com/citations?user=_19DrfIAAAAJ)<br>[Andrew Brock](https://github.com/ajbrock)<br>[Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ)<br>[Raia Hadsell](https://github.com/raiah)<br>[Niall Robinson](https://github.com/niallrobinson)<br>[Ellen Clancy](https://www.linkedin.com/in/ellen-clancy-815967124)<br>[Shakir Mohamed](https://www.shakirm.com/) | [![](https://camo.githubusercontent.com/567ac2d9abc0d9ecefd71673c051bcf1f664af61cfea4d088cff344cd41b27d2/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313033382f7334313538362d3032312d30333835342d7a)](https://doi.org/10.1038/s41586-021-03854-z) [![](https://camo.githubusercontent.com/84e2ed71d8b746b7df87e90d2aebee940aec2e3dbec7ac93fba00b0f1d46f326/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f646565706d696e642d72657365617263683f7374796c653d736f6369616c)](https://github.com/deepmind/deepmind-research/tree/master/nowcasting) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.00954)<br>- [blog post](https://deepmind.com/blog/article/nowcasting)<br>- [local kernel](https://research.google.com/colaboratory/local-runtimes.html)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/hub) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nowcasting/Open_sourced_dataset_and_model_snapshot_for_precipitation_nowcasting.ipynb) | 29.09.2021 |
| Live Speech Portraits | Real-Time Photorealistic Talking-Head Animation | - [Yuanxun Lu](https://github.com/YuanxunLu)<br>- [Jinxiang Chai](https://scholar.google.com/citations?user=OcN1_gwAAAAJ)<br>- [Xun Cao](https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html) | [![](https://camo.githubusercontent.com/b245d49901446ffc892ed1328dd0997a366efb25ba26a7870bbb95db5c3fca83/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333437383531332e33343830343834)](https://doi.org/10.1145/3478513.3480484)[![](https://camo.githubusercontent.com/2e73920bb0bfa690afb291ea3558b7ffe5331ffb59d184a65538826881f466bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5975616e78756e4c752f4c697665537065656368506f727472616974733f7374796c653d736f6369616c)](https://github.com/YuanxunLu/LiveSpeechPortraits)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.10595)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lelechen63/ATVGnet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DinoMan/speech-driven-animation), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)<br>- [project](https://yuanxunlu.github.io/projects/LiveSpeechPortraits/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1tKvi-9kY3GkEK8lgtfTSM70rMFo_TY50) | 26.09.2021 |
| StylEx | Training a GAN to explain a classifier in StyleSpace | - [Oran Lang](https://research.google/people/105975/)<br>- [Yossi Gandelsman](https://yossigandelsman.github.io/)<br>- [Michal Yarom](https://scholar.google.com/citations?user=GMVxiYgAAAAJ)<br>- [Yoav Wald](https://scholar.google.com/citations?user=hh5nOn4AAAAJ)<br>others[Gal Elidan](https://research.google/people/105719/)<br>[Avinatan Hassidim](https://research.google/people/105831/)<br>[William Freeman](https://billf.mit.edu/)<br>[Phillip Isola](http://web.mit.edu/phillipi/)<br>[Amir Globerso](https://cs3801.wixsite.com/amirgloberson)<br>[Michal Irani](http://www.weizmann.ac.il/math/irani/)<br>[Inbar Mosseri](https://research.google/people/InbarMosseri/) | [![](https://camo.githubusercontent.com/1c847362ec4ce7ff3f12c35094f3f9bfe0824c5c0374675b185b95392bc21e39/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435634383932322e323032312e3030303733)](https://doi.org/10.1109/ICCV48922.2021.00073)[![](https://camo.githubusercontent.com/b322c80c90bd8c42bb40705595befcbf2db6a1ed4e01e3606ed5af149bc12ac2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6578706c61696e696e672d696e2d7374796c653f7374796c653d736f6369616c)](https://github.com/google/explaining-in-style)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.13369), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1906.10112), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2011.12799), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1912.04958), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1710.01711)<br>- [blog post](https://ai.googleblog.com/2022/01/introducing-stylex-new-approach-for.html)<br>- [project](https://explaining-in-style.github.io/)<br>- [supplementary](https://explaining-in-style.github.io/supmat.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/wLk2eBdXH4M) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/explaining-in-style/blob/main/Explaining_in_Style_AttFind.ipynb) | 25.08.2021 |
| VITS | Parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models | - [Jaehyeon Kim](https://jaywalnut310.github.io/)<br>- [Jungil Kong](https://github.com/jik876)<br>- [Juhee Son](https://juheeuu.github.io/) | [![](https://camo.githubusercontent.com/f80d67c6200607f10bbf55ff6bd0396fc34227be9bfeafffef37006234bb78f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a617977616c6e75743331302f766974733f7374796c653d736f6369616c)](https://github.com/jaywalnut310/vits) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.06103)<br>- [demo](https://jaywalnut310.github.io/vits-demo/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1CO61pZizDj7en71NQG_aqqKdGaA_SaBf) | 23.08.2021 |
| Bringing Old Photo Back to Life | Restoring old photos that suffer from severe degradation through a deep learning approach | - [Ziyu Wan](http://raywzy.com/)<br>- [Bo Zhang](https://bo-zhang.me/)<br>- [Dongdong Chen](http://www.dongdongchen.bid/)<br>- [Pan Zhang](https://panzhang0212.github.io/)<br>others[Dong Chen](http://www.dongchen.pro/)<br>[Jing Liao](https://liaojing.github.io/html/)<br>[Fang Wen](https://www.microsoft.com/en-us/research/people/fangwen/) | [![](https://camo.githubusercontent.com/2e301feeadf905aabb921cb5e095bb29e56e5cfc045fa82b2b5da9ecaeb77957/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030323832)](https://doi.org/10.1109/CVPR42600.2020.00282) [![](https://camo.githubusercontent.com/40acb2f4bb00052fed3b40dbfc72cfed932f2c92ab9bae795d7b3135164f9d24/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f4272696e67696e672d4f6c642d50686f746f732d4261636b2d746f2d4c6966653f7374796c653d736f6369616c)](https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.09484)<br>- [demo](https://replicate.com/microsoft/bringing-old-photos-back-to-life)<br>- [project](http://raywzy.com/Old_Photo/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Q5bhszQq9eA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA) | 13.07.2021 |
| PTI | Pivotal Tuning Inversion enables employing off-the-shelf latent based semantic editing techniques on real images using StyleGAN | - [Daniel Roich](https://github.com/danielroich)<br>- [Ron Mokady](https://rmokady.github.io/)<br>- [Amit Bermano](https://www.cs.tau.ac.il/~amberman/)<br>- [Daniel Cohen-Or](https://danielcohenor.com/) | [![](https://camo.githubusercontent.com/18b1927939124c4a42435e25689a8c439f0e1867692419dda596ee59a742cddf/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f33353434373737)](https://doi.org/10.1145/3544777)[![](https://camo.githubusercontent.com/ad65493c131234e66673cfdd998f3a55d74d6ff9f97ab495a5b6fb06f792c167/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f64616e69656c726f6963682f5054493f7374796c653d736f6369616c)](https://github.com/danielroich/PTI)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.05744)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2-ada-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/richzhang/PerceptualSimilarity) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/danielroich/PTI/blob/main/notebooks/inference_playground.ipynb) | 01.07.2021 |
| TediGAN | Framework for multi-modal image generation and manipulation with textual descriptions | - [Weihao Xia](https://github.com/weihaox)<br>- [Yujiu Yang](http://www.fiesta.tsinghua.edu.cn/pi/3/24)<br>- [Jing-Hao Xue](http://www.homepages.ucl.ac.uk/~ucakjxu/)<br>- [Baoyuan Wu](https://sites.google.com/site/baoyuanwu2015/home) | [![](https://camo.githubusercontent.com/ce6cc531315e4b1ad3229b4b981b5d6f355d7269b872d9cd69754126ef4df455/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3030323239)](https://doi.org/10.1109/CVPR46437.2021.00229)[![](https://camo.githubusercontent.com/9b260c5c423ec96b6d7150212a04cd7d4dfd4baf3c50fa48660d777a7388bb3f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f494947524f55502f5465646947414e3f7374796c653d736f6369616c)](https://github.com/IIGROUP/TediGAN)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.03308), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.08910)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/weihaox/Multi-Modal-CelebA-HQ), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/ffhq-dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch/), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fyu/lsun)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/L8Na2f5viAM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](http://colab.research.google.com/github/weihaox/TediGAN/blob/master/playground.ipynb) | 30.06.2021 |
| SCALE | Modeling Clothed Humans with a Surface Codec of Articulated Local Elements | - [Qianli Ma](https://qianlim.github.io/)<br>- [Shunsuke Saito](https://shunsukesaito.github.io/)<br>- [Jinlong Yang](https://is.mpg.de/~jyang)<br>- [Siyu Tang](https://scholar.google.com/citations?user=BUDh_4wAAAAJ)<br>- [Michael Black](https://ps.is.mpg.de/~black) | [![](https://camo.githubusercontent.com/a54bb55079ba490c09616960923e3162403d2d2095e2aaa8ebf2695d035e7c6d/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031353832)](https://doi.org/10.1109/CVPR46437.2021.01582) [![](https://camo.githubusercontent.com/4cbdbf3eca3c519de0236e88d4fd651dab6bf92fed282b2258cfedfa7c69bb8f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7169616e6c696d2f5343414c453f7374796c653d736f6369616c)](https://github.com/qianlim/SCALE) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.07660)<br>- [data](https://cape.is.tue.mpg.de/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/krrish94/chamferdist), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/shunsukesaito/SCANimate)<br>- [poster](https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/650/SCALE_poster_CVPR_final_compressed.pdf)<br>- [project](https://qianlim.github.io/SCALE.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-EvWqFCUb7U), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/v4rWCxJJzhc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1lp6r-A-s1kBorIvg6rLD4Ja3o6JOvu3G) | 26.06.2021 |
| CogView | Mastering Text-to-Image Generation via Transformers | - [Ming Ding](https://scholar.google.com/citations?user=Va50YzkAAAAJ)<br>- [Zhuoyi Yang](https://scholar.google.com/citations?user=tgAt-gEAAAAJ)<br>- [Wenyi Hong](https://github.com/wenyihong)<br>- [Wendi Zheng](https://github.com/minkowski0125)<br>others[Chang Zhou](https://scholar.google.com/citations?user=QeSoG3sAAAAJ)<br>[Junyang Lin](https://justinlin610.github.io/)<br>[Xu Zou](http://xuzou.cn/)<br>[Zhou Shao](https://www.researchgate.net/profile/Shao_Zhou4)<br>[Hongxia Yang](https://sites.google.com/site/hystatistics/home)<br>[Jie Tang](https://keg.cs.tsinghua.edu.cn/jietang/) | [![](https://camo.githubusercontent.com/cb415c9e5e6efbaefa6c1308fcaf0010b5c1c299b242c4c90bd1bb8d0f2a97e0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f544855444d2f436f67566965773f7374796c653d736f6369616c)](https://github.com/THUDM/CogView) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.13290)<br>- [demo](https://thudm.github.io/CogView/index.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/apex), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Sleepychord/cogdata)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/cogview-image-generation-and-language-modelling-at-scale-8d358a0686d2)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2021/hash/a4d92e2cd541fca87e4620aba658316d-Abstract.html)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/nmxsd8/r_cogview_mastering_texttoimage_generation_via/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Cw1r8ACIj8U) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Bi2TnSUp2vNiSUhamsNuC4HqkZ2J4WwZ) | 21.06.2021 |
| GANs N' Roses | Stable, Controllable, Diverse Image to Image Translation | - [Min Jin Chong](https://mchong6.github.io/)<br>- [David Forsyth](http://luthuli.cs.uiuc.edu/~daf/) | [![](https://camo.githubusercontent.com/c32316340ad66ae0d16c1a57829ac1242ad5a4cb097379ab7ba1233a90381a5a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d63686f6e67362f47414e734e526f7365733f7374796c653d736f6369616c)](https://github.com/mchong6/GANsNRoses)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.06561), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2007.06600)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/znxlwm/UGATIT-pytorch)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/VNg0NyCGl_4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mchong6/GANsNRoses/blob/master/inference_colab.ipynb) | 19.06.2021 |
| Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes | A method to stylize images by optimizing parameterized brushstrokes instead of pixels | - [Dmytro Kotovenko](https://scholar.google.de/citations?user=T_U8yxwAAAAJ)<br>- [Matthias Wright](https://matthias-wright.github.io/)<br>- [Arthur Heimbrecht](https://github.com/arwehei)<br>- [Björn Ommer](https://ommer-lab.com/people/ommer/) | [![](https://camo.githubusercontent.com/dd21c1579d26d074f5b9515be56f0a796e91a640b8df1d3a1c5b6c9f2a59a7d9/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031323032)](https://doi.org/10.1109/CVPR46437.2021.01202) [![](https://camo.githubusercontent.com/7d73827d9ec64cba1653993187a0c07bcea536815fe6b3eede02efca44df89d4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6d705669732f62727573687374726f6b652d706172616d65746572697a65642d7374796c652d7472616e736665723f7374796c653d736f6369616c)](https://github.com/CompVis/brushstroke-parameterized-style-transfer) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.17185)<br>- [project](https://compvis.github.io/brushstroke-parameterized-style-transfer/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/CompVis/brushstroke-parameterized-style-transfer/blob/tensorflow_v2/notebooks/BrushstrokeStyleTransfer_TF2.ipynb) | 02.06.2021 |
| Pixel2Style2Pixel | Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation | - [Elad Richardson](https://github.com/eladrich)<br>- [Yuval Alaluf](https://yuval-alaluf.github.io/)<br>- [Yotam Nitzan](https://yotamnitzan.github.io/)<br>- [Daniel Cohen-Or](https://danielcohenor.com/) | [![](https://camo.githubusercontent.com/31ce56354dfde8c696840185e7f90cff7803f468a4f83bd31841963eb0efbc1b/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3030323332)](https://doi.org/10.1109/CVPR46437.2021.00232)[![](https://camo.githubusercontent.com/cae0f54780681099a733f06624b2b35619038695ead3400d7855b54159706235/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f656c6164726963682f706978656c327374796c6532706978656c3f7374796c653d736f6369616c)](https://github.com/eladrich/pixel2style2pixel)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2008.00951)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/HuangYG123/CurricularFace)<br>- [project](https://eladrich.github.io/pixel2style2pixel/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bfvSwhqsTgM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb) | 01.06.2021 |
| Fine-tuning a BERT | We will work through fine-tuning a BERT model using the tensorflow-models PIP package | - [Chen Chen](https://github.com/chenGitHuber)<br>- [Claire Yao](https://github.com/claireyao-fen) | [![](https://camo.githubusercontent.com/6ca17dd7aec85150f2a72bdf5a7aa9823a7687123bc8e0b0c2c0c9f6a00a3152/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f4e31392d31343233)](https://doi.org/10.18653/v1/N19-1423)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.04805)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://tensorflow.org/hub) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb) | 25.05.2021 |
| ReStyle | A Residual-Based StyleGAN Encoder via Iterative Refinement | - [Yuval Alaluf](https://yuval-alaluf.github.io/)<br>- [Or Patashnik](https://orpatashnik.github.io/)<br>- [Daniel Cohen-Or](https://danielcohenor.com/) | [![](https://camo.githubusercontent.com/0f6e8a884fde37e443a0900e928f0ff12ac25dbb94f27a9e11b8fd9d6933e0a0/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435634383932322e323032312e3030363634)](https://doi.org/10.1109/ICCV48922.2021.00664)[![](https://camo.githubusercontent.com/6e66f14e93a59e8a39bd2d629c5c0b4f93bfd4e114ed73c45c749dd25b0ed101/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f797576616c2d616c616c75662f72657374796c652d656e636f6465723f7374796c653d736f6369616c)](https://github.com/yuval-alaluf/restyle-encoder)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.02699), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2008.00951), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.02766)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TreB1eN/InsightFace_Pytorch)<br>- [project](https://yuval-alaluf.github.io/restyle-encoder/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/yuval-alaluf/restyle-encoder/blob/master/notebooks/inference_playground.ipynb) | 21.05.2021 |
| Motion Representations for Articulated Animation | Novel motion representations for animating articulated objects consisting of distinct parts | - [Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/)<br>- [Oliver Woodford](https://ojwoodford.github.io/)<br>- [Jian Ren](https://alanspike.github.io/)<br>- [Menglei Chai](https://mlchai.com/)<br>- [Sergey Tulyakov](http://www.stulyakov.com/) | [![](https://camo.githubusercontent.com/39c2e307efff238311dc631c368424426a6d93cbdaa088e5f472ec0f161ac7a7/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031333434)](https://doi.org/10.1109/CVPR46437.2021.01344) [![](https://camo.githubusercontent.com/32cea9518479afa425e00d48f20dcc441ca702115a7d028a9aa08994ef131ffb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f736e61702d72657365617263682f6172746963756c617465642d616e696d6174696f6e3f7374796c653d736f6369616c)](https://github.com/snap-research/articulated-animation) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.11280)<br>- [project](https://snap-research.github.io/articulated-animation/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=gpBYN8t8_yY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/articulated-animation/blob/master/demo.ipynb) | 29.04.2021 |
| SAM | Age Transformation Using a Style-Based Regression Model | - [Yuval Alaluf](https://yuval-alaluf.github.io/)<br>- [Or Patashnik](https://orpatashnik.github.io/)<br>- [Daniel Cohen-Or](https://danielcohenor.com/) | [![](https://camo.githubusercontent.com/2dd5ae015106354b9c799dd5707a1e1777520247399b7aed1d6465a49c7df381/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333435303632362e33343539383035)](https://doi.org/10.1145/3450626.3459805)[![](https://camo.githubusercontent.com/dc3df54c8a1270d2fe0e55befb698316d1953aa8280796ac35f3ac64fb573c76/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f797576616c2d616c616c75662f53414d3f7374796c653d736f6369616c)](https://github.com/yuval-alaluf/SAM)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.02754)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/eladrich/pixel2style2pixel), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch)<br>- [project](https://yuval-alaluf.github.io/SAM/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/X_pYC_LtBFw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](http://colab.research.google.com/github/yuval-alaluf/SAM/blob/master/notebooks/animation_inference_playground.ipynb) | 26.04.2021 |
| Geometry-Free View Synthesis | Is a geometric model required to synthesize novel views from a single image? | - [Robin Rombach](https://github.com/rromb)<br>- [Patrick Esser](https://github.com/pesser)<br>- [Björn Ommer](https://ommer-lab.com/people/ommer/) | [![](https://camo.githubusercontent.com/4fc3a8f7daae6f7c91cb352affde3ccf5d1fbb0e6f9f7b0b254a1aaf6b01f0c0/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435634383932322e323032312e3031343039)](https://doi.org/10.1109/ICCV48922.2021.01409) [![](https://camo.githubusercontent.com/bfd3f778486557c3524e455522c00ce79407c102e92ab1ebb512f393b17ff5e0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6d705669732f67656f6d657472792d667265652d766965772d73796e7468657369733f7374796c653d736f6369616c)](https://github.com/CompVis/geometry-free-view-synthesis) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.07652)<br>- [data](https://google.github.io/realestate10k/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/colmap/colmap) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/CompVis/geometry-free-view-synthesis/blob/master/scripts/braindance.ipynb) | 22.04.2021 |
| NeRViS | An algorithm for full-frame video stabilization by first estimating dense warp fields | - [Yu-Lun Liu](http://www.cmlab.csie.ntu.edu.tw/~yulunliu/)<br>- [Wei-Sheng Lai](https://www.wslai.net/)<br>- [Ming-Hsuan Yang](https://faculty.ucmerced.edu/mhyang/)<br>- [Yung-Yu Chuang](https://www.csie.ntu.edu.tw/~cyy/)<br>- [Jia-Bin Huang](https://jbhuang0604.github.io/) | [![](https://camo.githubusercontent.com/7b97012f37dd9d9e7de011482f2dfef646bd06e53ff8eef419fe5cb95a806c6d/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943435634383932322e323032312e3030323330)](https://doi.org/10.1109/ICCV48922.2021.00230) [![](https://camo.githubusercontent.com/c5301bdfadec62fc803269934e6b1c54cbdaabdb485a28adccb25fb12f0da920/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616c657830343037323030302f4e65525669533f7374796c653d736f6369616c)](https://github.com/alex04072000/NeRViS) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.06205)<br>- [data](http://liushuaicheng.org/SIGGRAPH2013/database.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cxjyxxme/deep-online-video-stabilization), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jinsc37/DIFRINT)<br>- [project](https://alex04072000.github.io/NeRViS/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/KO3sULs4hso) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH) | 11.04.2021 |
| NeX | View synthesis based on enhancements of multiplane image that can reproduce NeXt-level view-dependent effects in real time | - [Suttisak Wizadwongsa](https://www.linkedin.com/in/suttisak-wizadwongsa-763a931a5/)<br>- [Pakkapon Phongthawee](http://pureexe.github.io/)<br>- [Jiraphon Yenphraphai](https://www.linkedin.com/in/jiraphon-yenphraphai-990ba6175/)<br>- [Supasorn Suwajanakorn](https://www.supasorn.com/) | [![](https://camo.githubusercontent.com/3457167f5a0ae42736264daf58e5f61a43ed91f4a8303826b0cd0befc1d2e686/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3030383433)](https://doi.org/10.1109/CVPR46437.2021.00843) [![](https://camo.githubusercontent.com/c1bc2a9c3a595b51eeceb1bac850c19a034cb8e79d84cc6b3f274cb5a2ce79de/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e65782d6d70692f6e65782d636f64653f7374796c653d736f6369616c)](https://github.com/nex-mpi/nex-code) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.05606)<br>- [data](https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9WXRVQTQtQTcyVWc), [data](https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VyalBSUkw5Sm5GSXA4TU42ZDFqRXVvQjNYVm94SmtmZlBqZm9QeWhIa2owZGc%5FcnRpbWU9bC0yYWctRTcyVWc&id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fmodified%5Fdataset)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Fyusion/LLFF)<br>- [project](https://nex-mpi.github.io/)<br>- [vistec](https://vistec.ist/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=HyfkF7Z-ddA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1hXVvYdAwLA0EFg2zrafJUE0bFgB_F7PU) | 25.03.2021 |
| Score SDE | Score-Based Generative Modeling through Stochastic Differential Equations | - [Yang Song](https://yang-song.net/)<br>- [Jascha Sohl-Dickstein](http://www.sohldickstein.com/)<br>- [Diederik Kingma](http://dpkingma.com/)<br>- [Abhishek Kumar](https://abhishek.umiacs.io/)<br>others[Stefano Ermon](https://cs.stanford.edu/~ermon/)<br>[Ben Poole](https://cs.stanford.edu/~poole/) | [![](https://camo.githubusercontent.com/8e69d768e48e68f33af633dcd7424d326959da08aaec49c561cd9ebf95c10237/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f79616e672d736f6e672f73636f72655f7364653f7374796c653d736f6369616c)](https://github.com/yang-song/score_sde)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2011.13456), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.05600), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.09011), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.11239)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yang-song/score_sde_pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google/ml_collections)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/L9ZegT87QK8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/yang-song/score_sde/blob/main/Score_SDE_demo.ipynb) | 18.03.2021 |
| Talking Head Anime from a Single Image | The network takes as input an image of an anime character's face and a desired pose, and it outputs another image of the same character in the given pose | [Pramook Khungurn](https://pkhungurn.github.io/) | [![](https://camo.githubusercontent.com/00faf5272bf8fea0e7ed04818aa4f73699a3e42dcd3cfb2a3dd8b8bc766d3d53/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f706b68756e6775726e2f74616c6b696e672d686561642d616e696d652d64656d6f3f7374796c653d736f6369616c)](https://github.com/pkhungurn/talking-head-anime-demo) <br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lincolnhard/head-pose-estimation)<br>- [project](https://pkhungurn.github.io/talking-head-anime/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Virtual_YouTuber), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/MikuMikuDance)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/kMQCERkTdO0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/T1Gp-RxFZwU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/FioRJ6x_RbI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/pkhungurn/talking-head-anime-demo/blob/master/tha_colab.ipynb) | 23.02.2021 |
| NFNet | An adaptive gradient clipping technique, a significantly improved class of Normalizer-Free ResNets | - [Andrew Brock](https://github.com/ajbrock)<br>- [Soham De](https://sohamde.github.io/)<br>- [Samuel L. Smith](https://scholar.google.co.uk/citations?user=fyEqU5oAAAAJ)<br>- [Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ) | [![](https://camo.githubusercontent.com/84e2ed71d8b746b7df87e90d2aebee940aec2e3dbec7ac93fba00b0f1d46f326/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f646565706d696e642d72657365617263683f7374796c653d736f6369616c)](https://github.com/deepmind/deepmind-research/tree/master/nfnets)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.06171), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2101.08692)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/jaxline)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rNkHjZtH0RQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/qyy2WhRRSI4?feature=share) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nfnets/nfnet_demo_colab.ipynb) | 17.02.2021 |
| RITM | Simple feedforward model for click-based interactive segmentation that employs the segmentation masks from previous steps | - [Konstantin Sofiiuk](https://github.com/ksofiyuk)<br>- [Ilia Petrov](https://virtualhumans.mpi-inf.mpg.de/people/Petrov.html)<br>- [Anton Konushin](https://scholar.google.com/citations?user=ZT_k-wMAAAAJ) | [![](https://camo.githubusercontent.com/ecb7d3b0c44674b86f0222a5a22ffefcff9b581513fa644dcd14ae000449096c/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943495034363537362e323032322e39383937333635)](https://doi.org/10.1109/ICIP46576.2022.9897365)[![](https://camo.githubusercontent.com/15a42b0d3d05e287a504f8729bada5a3b97eb4684a7afa459c987fe010075125/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53616d73756e674c6162732f7269746d5f696e7465726163746976655f7365676d656e746174696f6e3f7374796c653d736f6369616c)](https://github.com/SamsungLabs/ritm_interactive_segmentation)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.06583)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/HRNet/HRNet-Image-Classification)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/interactive-segmentation-on-grabcut?p=reviving-iterative-training-with-mask), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/interactive-segmentation-on-berkeley?p=reviving-iterative-training-with-mask) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/SamsungLabs/ritm_interactive_segmentation/blob/master/notebooks/colab_test_any_model.ipynb) | 13.02.2021 |
| CLIP | A neural network which efficiently learns visual concepts from natural language supervision | - [Jong Wook Kim](https://jongwook.kim/)<br>- [Alec Radford](http://newmu.github.io/)<br>- [Ilya Sutskever](http://www.cs.utoronto.ca/~ilya/) | [![](https://camo.githubusercontent.com/6849bf039ac3e17c52bf0db3c32b3a5ebb98c355faf9e9053818ec328a027484/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e61692f434c49503f7374796c653d736f6369616c)](https://github.com/openai/CLIP) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.00020)<br>- [data](https://www.cs.toronto.edu/~kriz/cifar.html)<br>- [paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)<br>- [project](https://openai.com/blog/clip/)<br>- [slides](https://icml.cc/media/icml-2021/Slides/9193.pdf) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb) | 29.01.2021 |
| Adversarial Patch | A method to create universal, robust, targeted adversarial image patches in the real world | [Tom Brown](https://github.com/nottombrown) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1712.09665) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/cleverhans-lab/cleverhans/blob/master/examples/adversarial_patch/AdversarialPatch.ipynb) | 27.01.2021 |
| MSG-Net | Multi-style Generative Network with a novel Inspiration Layer, which retains the functionality of optimization-based approaches and has the fast speed of feed-forward networks | - [Hang Zhang](https://hangzhang.org/)<br>- [Kristin Dana](https://www.ece.rutgers.edu/~kdana/dana.html) | [![](https://camo.githubusercontent.com/a4eff278881693bb496a85b9e9bdd0ed3338570dc6f23b24e7a3a13249f5230b/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033302d31313031382d355f3332)](https://doi.org/10.1007/978-3-030-11018-5_32) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1703.06953)<br>- [project](http://computervisionrutgers.github.io/MSG-Net/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=oy6pWNWBt4Y) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/zhanghang1989/PyTorch-Multi-Style-Transfer/blob/master/msgnet.ipynb) | 25.01.2021 |
| f-BRS | Feature backpropagating refinement scheme that solves an optimization problem with respect to auxiliary variables instead of the network inputs, and requires running forward and backward pass just for a small part of a network | - [Konstantin Sofiiuk](https://github.com/ksofiyuk)<br>- [Ilia Petrov](https://virtualhumans.mpi-inf.mpg.de/people/Petrov.html)<br>- [Olga Barinova](https://github.com/OlgaBarinova)<br>- [Anton Konushin](https://scholar.google.com/citations?user=ZT_k-wMAAAAJ) | [![](https://camo.githubusercontent.com/4680c1aa0929aef79d054cc3c587b37deb79fd33632e9b339cadf1e7929592b6/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030383635)](https://doi.org/10.1109/CVPR42600.2020.00865)[![](https://camo.githubusercontent.com/53168c6e2ffcf0c3c86adbb3095a10f753c422f5b1f024296b948cd3ef1688e5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53616d73756e674c6162732f666272735f696e7465726163746976655f7365676d656e746174696f6e3f7374796c653d736f6369616c)](https://github.com/SamsungLabs/fbrs_interactive_segmentation)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2001.10331)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/HRNet/HRNet-Image-Classification)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ArcZ5xtyMCk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/xg-5J9gLuXA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/SamsungLabs/fbrs_interactive_segmentation/blob/master/notebooks/colab_test_any_model.ipynb) | 25.01.2021 |
| Neural Style Transfer | Implementation of Neural Style Transfer in Keras 2.0+ | [Somshubra Majumdar](http://titu1994.github.io/) | [![](https://camo.githubusercontent.com/7f32c38a96d6f993ad368ece4c0b3e0771e246470641df300ff9f52dd8fd19c5/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313136372f31362e31322e333236)](https://doi.org/10.1167/16.12.326)[![](https://camo.githubusercontent.com/f52b09e051892c1a4c9da946abe4c2e01a1dc32c933b40d2a08ca2dd6235270b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74697475313939342f4e657572616c2d5374796c652d5472616e736665723f7374796c653d736f6369616c)](https://github.com/titu1994/Neural-Style-Transfer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/1508.06576), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/1605.04603), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1606.05897) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/titu1994/Neural-Style-Transfer/blob/master/NeuralStyleTransfer.ipynb) | 22.01.2021 |
| SkyAR | A vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles | [Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/) | [![](https://camo.githubusercontent.com/c73bd7b2646f4d09b5d29de355dce3e8fc10fc69f4274235fc96540b0435b42a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5449502e323032322e33313932373137)](https://doi.org/10.1109/TIP.2022.3192717) [![](https://camo.githubusercontent.com/5476088a653f8bc03243955450d3d19f15e9ff2ddd832f29edfa366ab4c27ef2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a697570696e6a69612f536b7941523f7374796c653d736f6369616c)](https://github.com/jiupinjia/SkyAR) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.11800)<br>- [project](https://jiupinjia.github.io/skyar/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=zal9Ues0aOQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jiupinjia/SkyAR/blob/master/colab_demo.ipynb) | 18.01.2021 |
| MusicXML Documentation | The goal of this notebook is to explore one of the magenta libraries for music | - [Prakruti Joshi](https://github.com/prakruti-joshi)<br>- [Falak Shah](https://falaktheoptimist.github.io/)<br>- [Twisha Naik](https://github.com/twisha96) | - [magenta](https://magenta.tensorflow.org/)<br>- [music theory](http://musictheoryblog.blogspot.com/2008/02/learn-music-theory.html)<br>- [musicXML](https://www.musicxml.com/for-developers/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicXML_Document_Structure_Documentation.ipynb) | 08.01.2021 |
| SVG VAE | A colab demo for the SVG VAE model | [Raphael Gontijo Lopes](https://raphagl.com/) | [![](https://camo.githubusercontent.com/7b29441e34f1959a1a556189bc94c01e4eb3bd5ee248d73feb448c3d30286ffe/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343562e323031392e3030383032)](https://doi.org/10.1109/ICCV.2019.00802) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1904.02632)<br>- [blog post](https://magenta.tensorflow.org/svg-vae) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/vae_svg_decoding.ipynb) | 08.01.2021 |
| Neural Magic Eye | Learning to See and Understand the Scene Behind an Autostereogram | - [Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/)<br>- [Tianyang Shi](https://www.shitianyang.tech/)<br>- [Yi Yuan](https://yiyuan1991.github.io/)<br>- [Zhenwei Shi](http://levir.buaa.edu.cn/) | [![](https://camo.githubusercontent.com/97f9bf1aa9c0b0e9114e91b94fcc617744ac9c280efa7bea9862175052cc4967/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a697570696e6a69612f6e657572616c2d6d616769632d6579653f7374796c653d736f6369616c)](https://github.com/jiupinjia/neural-magic-eye) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.15692)<br>- [project](https://jiupinjia.github.io/neuralmagiceye/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=Fkh7DEblqJ8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1f59dFLJ748i2TleE54RkbUZSMo9Hyx7l) | 01.01.2021 |
| FGVC | Method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges | - [Chen Gao](http://chengao.vision/)<br>- [Ayush Saraf](https://github.com/ayush29feb)<br>- [Johannes Kopf](https://johanneskopf.de/)<br>- [Jia-Bin Huang](https://jbhuang0604.github.io/) | [![](https://camo.githubusercontent.com/1709edffc5cb7d9c71d19794e42ec960ff70c5b7178d457ddc9ab899c651fff4/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033302d35383631302d325f3432)](https://doi.org/10.1007/978-3-030-58610-2_42) [![](https://camo.githubusercontent.com/2a149a75e45567348a773ad552d439df287d613179ddf623b7f67e8ac4eaec22/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76742d766c2d6c61622f464756433f7374796c653d736f6369616c)](https://github.com/vt-vl-lab/FGVC) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2009.01835)<br>- [project](http://chengao.vision/FGVC/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=CHHVPxHT7rc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1pb6FjWdwq_q445rG2NP0dubw7LKNUkqc) | 30.12.2020 |
| VIBE | Video Inference for Body Pose and Shape Estimation, which makes use of an existing large-scale motion capture dataset together with unpaired, in-the-wild, 2D keypoint annotations | - [Muhammed Kocabas](https://ps.is.mpg.de/person/mkocabas)<br>- [Nikos Athanasiou](https://github.com/athn-nik)<br>- [Michael Black](https://ps.is.mpg.de/person/black) | [![](https://camo.githubusercontent.com/a8a38ad2b10e1b1180fbe01350fad1f986d12ab1949af9c78ee7162c3b58cfc1/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030353330)](https://doi.org/10.1109/CVPR42600.2020.00530)[![](https://camo.githubusercontent.com/e82279470f6f14a5d1e73ffd866621aec45a5ad4a7c8a0a9b8d3f9b90dc0448d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6b6f63616261732f564942453f7374796c653d736f6369616c)](https://github.com/mkocabas/VIBE)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1912.05656)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/carlosedubarreto/vibe_win_install), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vchoutas/smplx), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/akanazawa/human_dynamics), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/MandyMo/pytorch_HMR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/soulslicer/STAF/tree/staf)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/3d-human-pose-estimation-on-3dpw?p=vibe-video-inference-for-human-body-pose-and)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3qhs5IRJ1LI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/w1biKeiQThY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rIr-nX63dUA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/fW0sIZfQcIs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8Qt0wA16kTo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/xyo5gl5GLEI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/XNzgUhxKC38), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hErK0MamTY4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Gfmm8uMfMq0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1dFfwxZ52MN86FA6uFNypMEdFShd2euQA) | 23.12.2020 |
| SeFa | A closed-form approach for unsupervised latent semantic factorization in GANs | - [Yujun Shen](https://shenyujun.github.io/)<br>- [Bolei Zhou](https://boleizhou.github.io/) | [![](https://camo.githubusercontent.com/a53593294d7bc9d7c444f4c7dcd54f322712b7c81f7c38e71fe479227532e274/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3030313538)](https://doi.org/10.1109/CVPR46437.2021.00158) [![](https://camo.githubusercontent.com/27ee1fb00530888f83850a84fa47bd072483e9561f26ee7d0dd720a17b170576/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67656e666f7263652f736566613f7374796c653d736f6369616c)](https://github.com/genforce/sefa) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2007.06600)<br>- [project](https://genforce.github.io/sefa/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=OFHW2WbXXIQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/genforce/sefa/blob/master/docs/SeFa.ipynb) | 06.12.2020 |
| Stylized Neural Painting | An image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles | - [Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/)<br>- [Tianyang Shi](https://www.shitianyang.tech/)<br>- [Yi Yuan](https://yiyuan1991.github.io/)<br>- [Zhenwei Shi](http://levir.buaa.edu.cn/) | [![](https://camo.githubusercontent.com/621516d12ce8d5cfae8979882de2ef0a693742eb92722580e43195c8e1dca33b/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031353433)](https://doi.org/10.1109/CVPR46437.2021.01543) [![](https://camo.githubusercontent.com/9a71c469b8109c3507038469f7621887d3732f3e705a38de7fef08bdce56538a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a697570696e6a69612f7374796c697a65642d6e657572616c2d7061696e74696e673f7374796c653d736f6369616c)](https://github.com/jiupinjia/stylized-neural-painting) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2011.08114)<br>- [project](https://jiupinjia.github.io/neuralpainter/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=oerb-nwrXhk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1ch_41GtcQNQT1NLOA21vQJ_rQOjjv9D8) | 01.12.2020 |
| BiT | Big Transfer: General Visual Representation Learning | - [Alexander Kolesnikov](https://github.com/akolesnikoff)<br>- [Lucas Beyer](http://lucasb.eyer.be/)<br>- [Xiaohua Zhai](https://github.com/xiaohuazhai)<br>- [Joan Puigcerver](https://www.jpuigcerver.net/)<br>others[Jessica Yung](https://github.com/jessicayung)<br>[Sylvain Gelly](https://scholar.google.com/citations?user=m7LvuTkAAAAJ)<br>[Neil Houlsby](https://neilhoulsby.github.io/) | [![](https://camo.githubusercontent.com/565954f92abea968f8f1751a1e1aa05a0e769a7452425c66f0ec2430a561d5af/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033302d35383535382d375f3239)](https://doi.org/10.1007/978-3-030-58558-7_29)[![](https://camo.githubusercontent.com/8192d6198e873c2829c15df219ffd73e3849fc10fc9d2ac26c4d74315182cc15/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f6269675f7472616e736665723f7374796c653d736f6369616c)](https://github.com/google-research/big_transfer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1912.11370), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.05237)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/google/bit-50)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://sh-tsang.medium.com/review-big-transfer-bit-general-visual-representation-learning-cb4bf8ed9732)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/k1GOF2jmX7c), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0iTgt5-SOsU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/X5Rhm__OxvA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-research/big_transfer/blob/master/colabs/big_transfer_tf2.ipynb) | 12.11.2020 |
| LaSAFT | Latent Source Attentive Frequency Transformation for Conditioned Source Separation | [Woosung Choi](https://ws-choi.github.io/) | [![](https://camo.githubusercontent.com/c6c1c832f3c3b39302214833b0bf7237ce7f7cbb9712d1cfc821b5528dd08f97/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f49434153535033393732382e323032312e39343133383936)](https://doi.org/10.1109/ICASSP39728.2021.9413896) [![](https://camo.githubusercontent.com/3107693e64a495abe3418934f1b5d3f6b05905160762009e6f12e68911c87a4a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77732d63686f692f436f6e646974696f6e65642d536f757263652d53657061726174696f6e2d4c61534146543f7374796c653d736f6369616c)](https://github.com/ws-choi/Conditioned-Source-Separation-LaSAFT) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.11631)<br>- [data](https://sigsep.github.io/datasets/musdb.html)<br>- [project](https://lasaft.github.io/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ws-choi/Conditioned-Source-Separation-LaSAFT/blob/master/colab_demo/LaSAFT_with_GPoCM_Stella_Jang_Example.ipynb) | 01.11.2020 |
| Lifespan Age Transformation Synthesis | Multi-domain image-to-image generative adversarial network architecture, whose learned latent space models a continuous bi-directional aging process | - [Roy Or-El](https://homes.cs.washington.edu/~royorel/)<br>- [Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/)<br>- [Ohad Fried](https://www.ohadf.com/)<br>- [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)<br>- [Ira Kemelmacher-Shlizerman](https://www.irakemelmacher.com/) | [![](https://camo.githubusercontent.com/6b564e76944783a4b0d4674266edcee7c0a2f5413dd026cea56e04128a4468de/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033302d35383533392d365f3434)](https://doi.org/10.1007/978-3-030-58539-6_44)[![](https://camo.githubusercontent.com/0df74f01f0071dba40fca4d01e67ac161264cc9c4574f1fc4e81d30bacb66543/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726f796f72656c2f4c6966657370616e5f4167655f5472616e73666f726d6174696f6e5f53796e7468657369733f7374796c653d736f6369616c)](https://github.com/royorel/Lifespan_Age_Transformation_Synthesis)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.09764)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/royorel/FFHQ-Aging-Dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/pix2pixHD), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/style-based-gan-pytorch)<br>- [project](https://grail.cs.washington.edu/projects/lifespan_age_transformation_synthesis/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_jTFcjN2hBk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/9fulnt2_q_Y) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/royorel/Lifespan_Age_Transformation_Synthesis/blob/master/LATS_demo.ipynb) | 31.10.2020 |
| HiGAN | Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis | - [Ceyuan Yang](https://ceyuan.me/)<br>- [Yujun Shen](https://shenyujun.github.io/)<br>- [Bolei Zhou](https://boleizhou.github.io/) | [![](https://camo.githubusercontent.com/19d256907229e010bcc04bd4af3173277d906bc12264a39720224875ebb76fda/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f7331313236332d3032302d30313432392d35)](https://doi.org/10.1007/s11263-020-01429-5)[![](https://camo.githubusercontent.com/db381745faf2f53268b8cc4edafe3fef6ba963411099f80fc4720c04b3a23495/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67656e666f7263652f686967616e3f7374796c653d736f6369616c)](https://github.com/genforce/higan)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1911.09267), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1412.6856), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1906.10112)<br>- [project](https://genforce.github.io/higan/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=X5yWu2Jwjpg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/genforce/higan/blob/master/docs/HiGAN_Bedroom.ipynb) | 14.10.2020 |
| InterFaceGAN | Interpreting the Latent Space of GANs for Semantic Face Editing | - [Yujun Shen](https://shenyujun.github.io/)<br>- [Jinjin Gu](https://www.jasongt.com/)<br>- [Xiaoou Tang](https://www.ie.cuhk.edu.hk/people/xotang.shtml)<br>- [Bolei Zhou](https://boleizhou.github.io/) | [![](https://camo.githubusercontent.com/6cd1c6b8ffbcc387ee8c08cbbd40853d5b30d581c9d9d97b228f2b58f90d9f1a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030393236)](https://doi.org/10.1109/CVPR42600.2020.00926)[![](https://camo.githubusercontent.com/9648d51d3a0f01d2df6d05a3d0dcf4fd52866119e98232f4d6211082597f7b7a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67656e666f7263652f696e7465726661636567616e3f7374796c653d736f6369616c)](https://github.com/genforce/interfacegan)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.10786), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.09635), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1710.10196)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tkarras/progressive_growing_of_gans), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan)<br>- [project](https://genforce.github.io/interfacegan/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=uoftpl3Bj6w) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/genforce/interfacegan/blob/master/docs/InterFaceGAN.ipynb) | 13.10.2020 |
| Instance-aware Image Colorization | Novel deep learning framework to achieve instance-aware colorization | [Jheng-Wei Su](https://github.com/ericsujw) | [![](https://camo.githubusercontent.com/e74521fef3860ccdc65e1d74274bc67beb3d944ad97f299d10b411793ab54795/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030373939)](https://doi.org/10.1109/CVPR42600.2020.00799) [![](https://camo.githubusercontent.com/9decbdc6e7aa058e4dfee4f7af619a70e9131ab70159b8a806102b6e13615e10/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6572696373756a772f496e7374436f6c6f72697a6174696f6e3f7374796c653d736f6369616c)](https://github.com/ericsujw/InstColorization) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.10825)<br>- [project](https://ericsujw.github.io/InstColorization/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=Zj1N4uE1ehk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ericsujw/InstColorization/blob/master/InstColorization.ipynb) | 30.08.2020 |
| MoCo | Momentum Contrast for unsupervised visual representation learning | - [Kaiming He](https://kaiminghe.github.io/)<br>- [Haoqi Fan](https://haoqifan.github.io/)<br>- [Yuxin Wu](https://ppwwyyxx.com/)<br>- [Saining Xie](http://sainingxie.com/)<br>- [Ross Girshick](https://www.rossgirshick.info/) | [![](https://camo.githubusercontent.com/f95b02deedc30b9c6ce3b74d605ee4c7ddf8585bf979673915741d37b33e8484/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030393735)](https://doi.org/10.1109/CVPR42600.2020.00975)[![](https://camo.githubusercontent.com/8fbd491dcf4877685e6f4abc7c7911ad20b9f2461e1c315895e74d48b20ff2c9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f6d6f636f3f7374796c653d736f6369616c)](https://github.com/facebookresearch/moco)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1911.05722), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.04297), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.02677)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ppwwyyxx/moco.tensorflow)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/LvHwBQF14zs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/4VVGtYPM8JE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/o5Qh61dLDf0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb) | 20.08.2020 |
| CAPE | Learning to Dress 3D People in Generative Clothing | - [Qianli Ma](https://qianlim.github.io/)<br>- [Jinlong Yang](https://scholar.google.com/citations?user=HGt39SUAAAAJ)<br>- [Anurag Ranjan](https://anuragranj.github.io/)<br>- [Sergi Pujades](https://github.com/pujades)<br>others[Gerard Pons-Moll](https://virtualhumans.mpi-inf.mpg.de/)<br>[Siyu Tang](https://scholar.google.com/citations?user=BUDh_4wAAAAJ)<br>[Michael Black](https://ps.is.mpg.de/~black) | [![](https://camo.githubusercontent.com/c7e00e20f6506b2040303620e2211a9607fc131dca9dbefc1d279e10e28ba9fb/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030363530)](https://doi.org/10.1109/CVPR42600.2020.00650)[![](https://camo.githubusercontent.com/0eabc54a6f93fe5193be59ad0092c3bcddb1dddeebab8d77df225bc6617407ed/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7169616e6c696d2f434150453f7374796c653d736f6369616c)](https://github.com/qianlim/CAPE)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.13615), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1807.10267), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.02658)<br>- [data](https://cape.is.tue.mpg.de/dataset)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/MPI-IS/mesh), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vchoutas/smplx), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/anuragranj/coma)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@mahyarfardinfar/learning-to-dress-3d-people-in-generative-clothing-486eb90136ff)<br>- [project](https://cape.is.tue.mpg.de/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/e4W-hPFNwDE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/NOEA-Rtq6vM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1DCNo2OyyTNi1xDG-7j32FZQ9sBA6i9Ys) | 05.08.2020 |
| Rewriting a Deep Generative Model | We ask if a deep network can be reprogrammed to follow different rules, by enabling a user to directly change the weights, instead of training with a data set | - [David Bau](https://people.csail.mit.edu/davidbau/home/)<br>- [Steven Liu](http://people.csail.mit.edu/stevenliu/)<br>- [Tongzhou Wang](https://ssnl.github.io/)<br>- [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)<br>- [Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/) | [![](https://camo.githubusercontent.com/7ebc8f0ed54c6d8bdf9b057cdde3e865679f44f9f6e9284284d08cf2a84c8ff8/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033302d35383435322d385f3231)](https://doi.org/10.1007/978-3-030-58452-8_21)[![](https://camo.githubusercontent.com/227b624ebc4cd71bc5b6ca03bc31c5fa5683b01f0008dd63cf429ecb16686dff/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f64617669646261752f726577726974696e673f7374796c653d736f6369616c)](https://github.com/davidbau/rewriting)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2007.15646), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1912.04958)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch)<br>- [project](https://rewriting.csail.mit.edu/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=i2_-zNqtEPk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://rewriting.csail.mit.edu/video/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/davidbau/rewriting/blob/master/notebooks/rewriting-interface.ipynb) | 01.08.2020 |
| SIREN | Implicit Neural Representations with Periodic Activation Functions | - [Vincent Sitzmann](https://vsitzmann.github.io/)<br>- [Julien Martel](http://web.stanford.edu/~jnmartel/) | [![](https://camo.githubusercontent.com/608912a086414a35c5aaf62ad0153812da67b8beeb75eef2241001d03cf4cdb8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f767369747a6d616e6e2f736972656e3f7374796c653d736f6369616c)](https://github.com/vsitzmann/siren) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.09661)<br>- [data](https://drive.google.com/drive/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html)<br>- [project](https://vsitzmann.github.io/siren/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=Q2fLWGBeaiI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb) | 25.06.2020 |
| 3D Photo Inpainting | Method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view | - [Meng-Li Shih](https://shihmengli.github.io/)<br>- [Shih-Yang Su](https://lemonatsu.github.io/)<br>- [Johannes Kopf](https://johanneskopf.de/)<br>- [Jia-Bin Huang](https://jbhuang0604.github.io/) | [![](https://camo.githubusercontent.com/22b983b68805b7b7669fa751f31dd3141aecda71f8bd3c48570a24db98ca2f9e/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030383035)](https://doi.org/10.1109/CVPR42600.2020.00805) [![](https://camo.githubusercontent.com/f3ac903c1156bea2091d82c8c6ae42f83cfd526dd2616dbd6677a9ac48ac1c23/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76742d766c2d6c61622f33642d70686f746f2d696e7061696e74696e673f7374796c653d736f6369616c)](https://github.com/vt-vl-lab/3d-photo-inpainting) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.04727)<br>- [project](https://shihmengli.github.io/3D-Photo-Inpainting/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz) | 04.05.2020 |
| Motion Supervised co-part Segmentation | A self-supervised deep learning method for co-part segmentation | - [Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/)<br>- [Subhankar Roy](https://github.com/roysubhankar) | [![](https://camo.githubusercontent.com/800e3a050b334ea69f93bf250f9c06df28f9923f4cce09da4d51282543b9b332/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943505234383830362e323032312e39343132353230)](https://doi.org/10.1109/ICPR48806.2021.9412520)[![](https://camo.githubusercontent.com/bfbb47c0d5d1d1f8a0a2caf8fef2215ea323ed90b4ea5e76b3b50d228d1aaaab/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f416c69616b73616e6472536961726f68696e2f6d6f74696f6e2d636f7365676d656e746174696f6e3f7374796c653d736f6369616c)](https://github.com/AliaksandrSiarohin/motion-cosegmentation)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/2004.03234)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AliaksandrSiarohin/video-preprocessing)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=RJ4Nj1wV5iA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/motion-cosegmentation/blob/master/part_swap.ipynb) | 07.04.2020 |
| Onsets and Frames | Onsets and Frames is an automatic music transcription framework with piano and drums models | - [Curtis Hawthorne](https://github.com/cghawthorne)<br>- [Erich Elsen](https://github.com/ekelsen) | [![](https://camo.githubusercontent.com/caa390a03471ee7a1f74b1153d656a7135c49848d73123b6c300b22c610c84f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6167656e74612f6d6167656e74613f7374796c653d736f6369616c)](https://github.com/magenta/magenta/tree/main/magenta/models/onsets_frames_transcription)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1710.11153), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.12247), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.00188)<br>- [blog post](http://g.co/magenta/onsets-frames)<br>- [data](https://g.co/magenta/maestro-wave2midi2wave), [data](https://magenta.tensorflow.org/datasets/e-gmd) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/notebooks/magenta/onsets_frames_transcription/onsets_frames_transcription.ipynb) | 02.04.2020 |
| FBA Matting | Low-cost modification to alpha matting networks to also predict the foreground and background colours | - [Marco Forte](https://github.com/MarcoForte)<br>- [François Pitié](https://francois.pitie.net/) | [![](https://camo.githubusercontent.com/0795c41074fd36d96a6be540b89d62bcadcb44bd14b183cc09af8ef18180f5c0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d6172636f466f7274652f4642415f4d617474696e673f7374796c653d736f6369616c)](https://github.com/MarcoForte/FBA_Matting)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.07711)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/MarcoForte/closed-form-matting)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/leonelhs/FBA-Matting)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/image-matting-on-composition-1k?p=f-b-alpha-matting) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Ut2szLBTxPejGHt_GYUkua21yUVWseOE) | 19.03.2020 |
| BERT score | An automatic evaluation metric for text generation | [Tianyi Zhang](https://tiiiger.github.io/) | [![](https://camo.githubusercontent.com/517a5132c35c3946b7387c6d62084da92ed09d5ae8045789341af343ece787fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f546969696765722f626572745f73636f72653f7374796c653d736f6369616c)](https://github.com/Tiiiger/bert_score)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1904.09675)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/bert-score/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Tiiiger/bert_score/blob/master/example/Demo.ipynb) | 05.03.2020 |
| ProxylessNAS | Directly learn the architectures for large-scale target tasks and target hardware platforms | - [Han Cai](https://han-cai.github.io/)<br>- [Ligeng Zhu](https://han-cai.github.io/)<br>- [Song Han](https://hanlab.mit.edu/songhan) | [![](https://camo.githubusercontent.com/bbcfbc1155d34bdf55fbaa0ded03cc779a90580ba9f34d8a4de245daa8857400/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d69742d68616e2d6c61622f70726f78796c6573736e61733f7374796c653d736f6369616c)](https://github.com/mit-han-lab/proxylessnas)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1812.00332), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1802.03494), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1811.08886)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://sh-tsang.medium.com/paper-proxylessnas-direct-neural-architecture-search-on-target-task-image-classification-73c35ebd8aed)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/hub/pytorch_vision_proxylessnas/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/a3a1xy/r_proxylessnas_direct_neural_architecture_search/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/X0aZmppnO1s), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/6AeCIJH9eGI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_vision_proxylessnas.ipynb) | 29.10.2019 |
| Generating Piano Music with Transformer | This Colab notebook lets you play with pretrained Transformer models for piano music generation, based on the Music Transformer | - [Ian Simon](https://github.com/iansimon)<br>- [Anna Huang](https://github.com/czhuang)<br>- [Jesse Engel](https://github.com/jesseengel)<br>- [Curtis Hawthorne](https://github.com/cghawthorne) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.03762), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1809.04281)<br>- [blog post](http://g.co/magenta/music-transformer) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb) | 16.09.2019 |
| HMR | End-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image | - [Angjoo Kanazawa](https://people.eecs.berkeley.edu/~kanazawa/)<br>- [Michael Black](https://ps.is.mpg.de/person/black)<br>- [David Jacobs](https://www.cs.umd.edu/~djacobs/)<br>- [Jitendra Malik](https://people.eecs.berkeley.edu/~malik/) | [![](https://camo.githubusercontent.com/2e733b204d7179fa514e5d7176d5b00cc56c5cedf44f50fcf9bb4eb02f0d5413/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f435650522e323031382e3030373434)](https://doi.org/10.1109/CVPR.2018.00744)[![](https://camo.githubusercontent.com/255607895a632643386846e7de9dddc6c94e24f440df539d6c7e10aa53d6cbeb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616b616e617a6177612f686d723f7374796c653d736f6369616c)](https://github.com/akanazawa/hmr)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1712.06584)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/dawars/hmr/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mattloper/chumpy), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CMU-Perceptual-Computing-Lab/openpose), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/MandyMo/pytorch_HMR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/layumi/hmr), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/russoale/hmr2.0)<br>- [project](https://akanazawa.github.io/hmr/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bmMV9aJKa-c) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Dene33/video_to_bvh/blob/master/video_to_bvh.ipynb) | 15.03.2019 |
| GANSynth | This notebook is a demo GANSynth, which generates audio with Generative Adversarial Networks | [Jesse Engel](https://github.com/jesseengel) | [![](https://camo.githubusercontent.com/caa390a03471ee7a1f74b1153d656a7135c49848d73123b6c300b22c610c84f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6167656e74612f6d6167656e74613f7374796c653d736f6369616c)](https://github.com/magenta/magenta/tree/main/magenta/models/gansynth)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1902.08710), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1809.11096)<br>- [project](https://storage.googleapis.com/magentadata/papers/gansynth/index.html) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/notebooks/magenta/gansynth/gansynth_demo.ipynb) | 25.02.2019 |
| Latent Constraints | Conditional Generation from Unconditional Generative Models | - [Jesse Engel](https://github.com/jesseengel)<br>- [Matthew Hoffman](http://matthewdhoffman.com/)<br>- [Adam Roberts](https://github.com/adarob) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1711.05772)<br>- [data](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/notebooks/latent_constraints/latentconstraints.ipynb) | 27.11.2017 |
| Performance RNN | This notebook shows you how to generate new performed compositions from a trained model | - [Ian Simon](https://github.com/iansimon)<br>- [Sageev Oore](https://github.com/osageev)<br>- [Curtis Hawthorne](https://github.com/cghawthorne) | [![](https://camo.githubusercontent.com/caa390a03471ee7a1f74b1153d656a7135c49848d73123b6c300b22c610c84f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6167656e74612f6d6167656e74613f7374796c653d736f6369616c)](https://github.com/magenta/magenta/tree/master/magenta/models/performance_rnn) <br>- [blog post](https://magenta.tensorflow.org/performance-rnn)<br>- [data](http://www.piano-e-competition.com/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/notebooks/magenta/performance_rnn/performance_rnn.ipynb) | 11.07.2017 |
| NSynth | This colab notebook has everything you need to upload your own sounds and use NSynth models to reconstruct and interpolate between them | - [Jesse Engel](https://github.com/jesseengel)<br>- [Cinjon Resnick](https://github.com/cinjon)<br>- [Adam Roberts](https://github.com/adarob)<br>- [Sander Dieleman](https://benanne.github.io/)<br>others[Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ)<br>[Mohammad Norouzi](https://norouzi.github.io/)<br>[Douglas Eck](https://github.com/douglaseck) | [![](https://camo.githubusercontent.com/0cb0b4036aa827e8ba7978e08091341d37b0d0787c35d6682bb8f6cdbf3512f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f6d6167656e74613f7374796c653d736f6369616c)](https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1704.01279)<br>- [blog post](https://magenta.tensorflow.org/nsynth)<br>- [data](https://magenta.tensorflow.org/datasets/nsynth)<br>- [tutorial](https://magenta.tensorflow.org/nsynth-fastgen)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=AaALLWQmCdI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=BOoSy-Pg8is) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb) | 06.04.2017 |

## Tutorials

[Permalink: Tutorials](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md#tutorials)

TUTORIALS

| name | description | authors | links | colaboratory | update |
| --- | --- | :-- | :-- | :-: | :-: |
| LangGraph | Library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows | [LangChain](https://www.langchain.com/) | [![](https://camo.githubusercontent.com/88b532f8e44248b0ff46c78ca47929bdc10d9f7493b7ce4cf03548860ffbf0fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e67636861696e2d61692f6c616e6767726170683f7374796c653d736f6369616c)](https://github.com/langchain-ai/langgraph) <br>- [blog post](https://www.langchain.com/langgraph)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://langchain-ai.github.io/langgraph/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/langchain-ai/langgraphjs)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787?gi=eb24d42206bf), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/cyberark-engineering/building-production-ready-ai-agents-with-langgraph-a-real-life-use-case-7bda34c7f4e4)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/langgraph/)<br>- [website](https://www.langchain.com/langgraph)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLfaIDFEXuae16n2TWUkKq5PgJ0w6Pkwtg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1bUy-1hGZpI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PqS1kib7RTw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PNr3f7QyQU4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qaWOwbFw3cs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/langchain-ai/langgraph/blob/main/docs/docs/tutorials/introduction.ipynb) | 03.02.2025 |
| VC | Client software for performing real-time voice conversion using various Voice Conversion AI | [w-okada](https://github.com/w-okada) | [![](https://camo.githubusercontent.com/bed1e0cd42146bb2ce63b7d688503cdcaeeffe5a0d790c1a9d7b9df64b111df1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f772d6f6b6164612f766f6963652d6368616e6765723f7374796c653d736f6369616c)](https://github.com/w-okada/voice-changer)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yxlllc/DDSP-SVC)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/wok000/vcclient000)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/POo_Cg0eFMU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/fba9Zhsukqw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/s_GirFEGvaA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Q7bbEC4aeKM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_JXbvSTGPoo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/pHhjg2JwdPI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/We5oYpCR3WQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/aVfoC1EHlVs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YF1lBaqeyt8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/hinabl/voice-changer-colab/blob/master/Hina_Modified_Realtime_Voice_Changer_on_Colab.ipynb) | 30.01.2025 |
| YOLOv8 | State-of-the-art model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility | [Glenn Jocher](https://github.com/glenn-jocher) | [![](https://camo.githubusercontent.com/6bb27e4c4557560b490abaee8cae2be867468ad0fe3c46ad3d106cbb0ef8766e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756c7472616c79746963732f756c7472616c79746963733f7374796c653d736f6369616c)](https://github.com/ultralytics/ultralytics) <br>- [COCO](http://cocodataset.org/)<br>- [ImageNet](https://www.image-net.org/)<br>- [blog post](https://habr.com/ru/articles/710016/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://ultralytics.com/discord)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/ultralytics/ultralytics)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.ultralytics.com/)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/ultralytics/yolov8)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/ultralytics)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtube.com/ultralytics), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/m9fH9OWn8YM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/wuZtUMEiKWY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gRAyOPjQ9_s), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/fhzCwJkDONE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/IHbJcOex6dk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb) | 30.01.2025 |
| Building Your Own Federated Learning Algorithm | We discuss how to implement federated learning algorithms without deferring to the tff.learning API | [Zachary Charles](https://zachcharles.com/) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.08610)<br>- [blog post](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/federated-learning)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-federated/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb) | 29.01.2025 |
| Federated Learning for Image Classification | We use the classic MNIST training example to introduce the Federated Learning API layer of TFF, tff.learning - a set of higher-level interfaces that can be used to perform common types of federated learning tasks, such as federated training, against user-supplied models implemented in TensorFlow | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1602.05629)<br>- [data](https://www.nist.gov/srd/nist-special-database-19)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/federated-learning), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/image-classification)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-federated/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb) | 29.01.2025 |
| Federated Learning for Text Generation | We start with a RNN that generates ASCII characters, and refine it via federated learning | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1812.01097), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1602.05629)<br>- [data](http://www.ibiblio.org/pub/docs/books/gutenberg/9/98/98.txt), [data](http://www.ibiblio.org/pub/docs/books/gutenberg/4/46/46.txt)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-federated/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/hub) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb) | 29.01.2025 |
| Custom Federated Algorithms, Part 1: Introduction to the Federated Core | This tutorial is the first part of a two-part series that demonstrates how to implement custom types of federated algorithms in TensorFlow Federated using the Federated Core - a set of lower-level interfaces that serve as a foundation upon which we have implemented the Federated Learning layer | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1602.05629)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/federated-learning)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-federated/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/federated_core), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/federated_learning) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_1.ipynb) | 29.01.2025 |
| Custom Federated Algorithms, Part 2: Implementing Federated Averaging | This tutorial is the second part of a two-part series that demonstrates how to implement custom types of federated algorithms in TFF using the Federated Core, which serves as a foundation for the Federated Learning layer | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | [![](https://camo.githubusercontent.com/d3e242474ab1ac12696b9fd7f22db763d034cdf045adef448c2ec9eb631e6937/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f6665646572617465643f7374796c653d736f6369616c)](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/learning/federated_averaging.py)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/federated-learning)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-federated/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/federated_core), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/federated_learning) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_2.ipynb) | 29.01.2025 |
| Kornia | Library is composed by a subset of packages containing operators that can be inserted within neural networks to train models to perform image transformations, epipolar geometry, depth estimation, and low-level image processing such as filtering and edge detection that operate directly on tensors | - [Edgar Riba](https://github.com/edgarriba)<br>- [Dmytro Mishkin](https://dmytro.ai/)<br>- [Daniel Ponsa](https://github.com/DanielPonsa)<br>- [Ethan Rublee](https://github.com/ethanrublee)<br>- [Gary Bradski](https://github.com/garybradski) | [![](https://camo.githubusercontent.com/68956f1a79eaa644d60f24ebf93d63a9b4893a35189a493fa1440f271edf3e3c/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5741435634353537322e323032302e39303933333633)](https://doi.org/10.1109/WACV45572.2020.9093363) [![](https://camo.githubusercontent.com/71f7e10792d7dfb416a6aa44970ce2f2ae7b521851f62d93c497d4bf1cc809ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b6f726e69612f6b6f726e69613f7374796c653d736f6369616c)](https://github.com/kornia/kornia) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.02190)<br>- [blog post](https://opencv.org/kornia-an-open-source-differentiable-computer-vision-library-for-pytorch/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://kornia.readthedocs.io/en/latest/)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://join.slack.com/t/kornia/shared_invite/zt-csobk21g-2AQRi~X9Uu6PLMuUZdvfjA)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/kornia_foss)<br>- [website](https://kornia.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/channel/UCI1SE1Ij2Fast5BSKxoa7Ag), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3RmCYFhwclE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/AAZa-mXjYF0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/kornia/kornia/blob/master/examples/augmentation/kornia_augmentation.ipynb) | 27.01.2025 |
| Contextualized Topic Models | Family of topic models that use pre-trained representations of language to support topic modeling | - [Federico Bianchi](https://federicobianchi.io/)<br>- [Silvia Terragni](https://silviatti.github.io/)<br>- [Dirk Hovy](http://dirkhovy.com/)<br>- [Debora Nozza](https://www.deboranozza.com/)<br>- [Elisabetta Fersini](https://www.unimib.it/elisabetta-fersini) | [![](https://camo.githubusercontent.com/4f07f511f6f917583e55859b3a733776a24c832459b08cbfb7d79613a03c1294/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f323032312e6561636c2d6d61696e2e313433)](https://doi.org/10.18653/v1/2021.eacl-main.143)[![](https://camo.githubusercontent.com/f7bfb7468267a9e37d752e46e53b35e8096fcc352caf62594e45be163c9d9bd9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d696c614e4c50726f632f636f6e7465787475616c697a65642d746f7069632d6d6f64656c733f7374796c653d736f6369616c)](https://github.com/MilaNLProc/contextualized-topic-models)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.03974)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://contextualized-topic-models.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/estebandito22/PyTorchAVITM), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/dlukes/rbo)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/towards-data-science/contextualized-topic-modeling-with-python-eacl2021-eacf6dfa576)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.python.org/pypi/contextualized_topic_models)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/n1_G8K07KoM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1fXJjr_rwqvpp1IdNQ4dxqN4Dp88cxO97) | 16.01.2025 |
| dm\_control | DeepMind Infrastructure for Physics-Based Simulation | - [Saran Tunyasuvunakool](https://github.com/saran-t)<br>- [Alistair Muldal](https://github.com/alimuldal)<br>- [Yotam Doron](http://www.yotamdoron.com/)<br>- [Siqi Liu](http://siqi.fr/)<br>others[Steven Bohez](https://github.com/sbohez)<br>[Josh Merel](https://sites.google.com/site/jsmerel/)<br>[Tom Erez](https://github.com/erez-tom)<br>[Timothy Lillicrap](https://contrastiveconvergence.net/~timothylillicrap/index.php)<br>[Nicolas Heess](https://scholar.google.com/citations?user=79k7bGEAAAAJ)<br>[Yuval Tassa](https://github.com/yuvaltassa) | [![](https://camo.githubusercontent.com/65b45eab027582c3e341c9600720f4e05e3114e67541cc4fc4b0174eb8cd717b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f646d5f636f6e74726f6c3f7374796c653d736f6369616c)](https://github.com/deepmind/dm_control)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.12983), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1801.00690), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1902.07151), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1707.02286), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1802.09564), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1802.10567)<br>- [blog post](https://www.deepmind.com/publications/dm-control-software-and-tasks-for-continuous-control)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Tippe_top)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/CMjoiU482Jk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rAai4QzcYbs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/WhaRsrlaXLk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb) | 15.01.2025 |
| MuJoCo | A general purpose physics engine that aims to facilitate research and development in robotics, biomechanics, graphics and animation, machine learning, and other areas which demand fast and accurate simulation of articulated structures interacting with their environment | - [Emo Todorov](https://homes.cs.washington.edu/~todorov/)<br>- [Tom Erez](https://github.com/erez-tom)<br>- [Yuval Tassa](https://github.com/yuvaltassa) | [![](https://camo.githubusercontent.com/4379fbeed4e27df4a1ff45cc3e49b2fbe8c1efa8825c7a1f2fcd02b93b7de27a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f6d756a6f636f3f7374796c653d736f6369616c)](https://github.com/deepmind/mujoco)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.12983)<br>- [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://www.deepmind.com/blog/opening-up-a-physics-simulator-for-robotics), [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://www.deepmind.com/blog/open-sourcing-mujoco)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mujoco.readthedocs.io/en/latest/overview.html)<br>- [website](https://mujoco.org/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Tippe_top), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Chaos_theory), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/3D_projection#Mathematical_formula)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0ORsj_E17B0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/yHZVVfsJ8mc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/eyzzsGJ1iic) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/dm_control/blob/master/dm_control/mujoco/tutorial.ipynb) | 15.01.2025 |
| Ollama | Get up and running with large language models | [Michael Yang](https://github.com/mxyng) | [![](https://camo.githubusercontent.com/57812a95a319d8a004bd11776a82cf59fb1d76c0648972874d7fbd3d53a34a01/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f6c6c616d612f6f6c6c616d613f7374796c653d736f6369616c)](https://github.com/ollama/ollama)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/ollama/ollama)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ollama/ollama-python), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ollama/ollama-js), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ggerganov/llama.cpp)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/ollama/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://x.com/ollama)<br>- [website](https://ollama.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rIRkxZSn-A8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1xdneyn6zjw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/cTxENLLX1ho), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ztBJqzBU5kc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Ox8hhpgrUi0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/lhQ8ixnYO2Y), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/pxhkDaKzBaY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ollama/ollama/blob/master/examples/jupyter-notebook/ollama.ipynb) | 13.01.2025 |
| LangChain | Framework for developing applications powered by large language models | [LangChain](https://www.langchain.com/) | [![](https://camo.githubusercontent.com/6910d0f894fb43e876ff0b5d40d135a18084d314f1bcda5c43f91501eef67aa1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e67636861696e2d61692f6c616e67636861696e3f7374796c653d736f6369616c)](https://github.com/langchain-ai/langchain)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://python.langchain.com/docs/introduction/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/langchain-ai/langchainjs), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/langchain-ai/langchain-extract), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/langchain-ai/chat-langchain), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/langchain-ai/weblangchain)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@neelmakvana168/what-is-lang-chain-in-llm-e55e021da2b3), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@bijit211987/llm-powered-applications-building-with-langchain-cad4032d733c), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/munchy-bytes/exploring-langchain-ff13fff63340)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/langchain/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/langchainai)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/LangChain)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1bUy-1hGZpI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/9AXP7tCI9PI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/aywZrzNaKjs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dXxQ0LR-3Hg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/sVcwVQRHIc8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MlK6SIjcjE8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TLf90ipMzfE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLZoTAELRMXVORE4VF7WQ_fAl0L1Gljtar) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/qa_chat_history.ipynb) | 11.01.2025 |
| NotebookLlama | Open Source version of NotebookLM | [Meta](https://www.llama.com/) | [![](https://camo.githubusercontent.com/61559f10b1967acb391895b1e8f6a4b2762e20df63b83b45df691c82a30598dc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6574612d6c6c616d612f6c6c616d612d726563697065733f7374796c653d736f6369616c)](https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama) <br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/ai-disruption/meta-launches-open-source-version-notebookllama-rivals-googles-popular-notebooklm-9a41edd99c24)<br>- [meidum](https://medium.com/ai-artistry/notebook-llama-an-open-source-guide-to-building-a-pdf-to-podcast-workflow-e8fceec888a9)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/OpenSourceeAI/comments/1gdsmax/meta_ai_silently_releases_notebookllama_an_open/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/quickstart/NotebookLlama/Step-1%20PDF-Pre-Processing-Logic.ipynb) | 09.01.2025 |
| Anomalib | Deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets | - [Samet Akcay](https://github.com/samet-akcay)<br>- [Dick Ameln](https://github.com/djdameln)<br>- [Ashwin Vaidya](https://ashwinvaidya.com/)<br>- [Barath Lakshmanan](https://github.com/blakshma)<br>others[Nilesh Ahuja](https://github.com/nahuja-intel)<br>[Utku Genc](https://github.com/ugenc-intel) | [![](https://camo.githubusercontent.com/7d7dc8be7e87912f47bedad556acdb6baea49b7c7f8ee56960debe0cd81bb00f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e76696e6f746f6f6c6b69742f616e6f6d616c69623f7374796c653d736f6369616c)](https://github.com/openvinotoolkit/anomalib) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2011.08785)<br>- [data](https://www.mvtec.com/company/research/datasets/mvtec-ad)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://openvinotoolkit.github.io/anomalib/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rwightman/pytorch-image-models), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vnk8071/anomaly-detection-in-industry-manufacturing/tree/master/anomalib_contribute)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/lib/timm) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/openvinotoolkit/anomalib/blob/main/notebooks/000_getting_started/001_getting_started.ipynb) | 08.01.2025 |
| Hello, many worlds | This tutorial shows how a classical neural network can learn to correct qubit calibration errors | [Michael Broughton](https://github.com/MichaelBroughton) | - [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/quantum/api_docs/python/tfq/layers), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/quantum/api_docs/python/tfq/get_expectation_op), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/keras/functional)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Pauli_matrices)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-o9AhIz1uvo) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/quantum/blob/master/docs/tutorials/hello_many_worlds.ipynb) | 04.01.2025 |
| Opik | From RAG chatbots to code assistants to complex agentic pipelines and beyond, build LLM systems that run better, faster, and cheaper with tracing, evaluations, and dashboards | [Jacques Verré](https://github.com/jverre) | [![](https://camo.githubusercontent.com/53d496d377ea9ce24214b26c2afd09523040749e1b383052d9d74d87231d016d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f6d65742d6d6c2f6f70696b3f7374796c653d736f6369616c)](https://github.com/comet-ml/opik)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://www.comet.com/docs/opik/self-host/local_deployment/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://www.comet.com/docs/opik/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/opik/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/Python/comments/1fq33rw/opik_open_source_llm_evaluation_framework/), [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/programming/comments/1fj13qx/opik_opensource_llm_evaluation_framework/), [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/machinelearningnews/comments/1fj99eu/comet_launches_opik_a_comprehensive_opensource/)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://chat.comet.com/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://x.com/Cometml)<br>- [website](https://www.comet.com/site/products/opik/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/B4oboG62lyA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/9JN-JaaOpa8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/quickstart_notebook.ipynb) | 30.12.2024 |
| LightAutoML | Allows you create machine learning models using just a few lines of code, or build your own custom pipeline using ready blocks | - [Alexander Ryzhkov](https://github.com/alexmryzhkov)<br>- [Anton Vakhrushev](https://www.kaggle.com/btbpanda)<br>- [Dmitry Simakov](https://github.com/DESimakov) | [![](https://camo.githubusercontent.com/2b6becd6d207af7859ded333129f40761c640707ec42bd717a9a2c70109ab6e6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73622d61692d6c61622f4c696768744175746f4d4c3f7374796c653d736f6369616c)](https://github.com/sb-ai-lab/LightAutoML)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.01528)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://lightautoml.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Rishat-skoltech/LightAutoML_GPU), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sb-ai-lab/SLAMA)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/alexryzhkov/n3-tps-april-21-lightautoml-starter), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/alexryzhkov/lightautoml-titanic-love), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/alexryzhkov/lightautoml-extreme-short-titanic-solution), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/alexryzhkov/lightautoml-houseprices-love), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/simakov/lama-whitebox-preset-example), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/simakov/lama-custom-automl-pipeline-example), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/mikhailkuz/lightautoml-nn-happiness)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://alexmryzhkov.medium.com/lightautoml-preset-usage-tutorial-2cce7da6f936)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/lightautoml)<br>- [website](https://developers.sber.ru/portal/products/lightautoml)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/4pbO673B9Oo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ci8uqgWFJGg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TYu1UG-E9e8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLJU_M19giWaEXcQtWWhpOKJf_luMc12B2), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hr8GbPOHaEE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/AILab-MLTools/LightAutoML/blob/master/examples/tutorials/Tutorial_1_basics.ipynb) | 22.12.2024 |
| TorchGeo | PyTorch domain library that provides datasets, transforms, samplers, and pre-trained models specific to geospatial data | - [Adam Stewart](https://github.com/adamjstewart)<br>- [Caleb Robinson](https://calebrob.com/)<br>- [Isaac Corley](https://github.com/isaaccorley)<br>- [Anthony Ortiz](https://github.com/anthonymlortiz)<br>others[Juan Lavista Ferres](https://www.microsoft.com/en-us/research/people/jlavista/)<br>[Arindam Banerjee](https://arindam.cs.illinois.edu/) | [![](https://camo.githubusercontent.com/92aa56cbf1245435d03ce92ac4d779fafbd067427f017eab012cbfe32414184b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f746f72636867656f3f7374796c653d736f6369616c)](https://github.com/microsoft/torchgeo) <br>- [NDBI](https://www.linkedin.com/pulse/ndvi-ndbi-ndwi-calculation-using-landsat-7-8-tek-bahadur-kshetri/)<br>- [NDVI](https://gisgeography.com/ndvi-normalized-difference-vegetation-index/)<br>- [NDWI](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndwi/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.08872)<br>- [data](https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/), [data](https://www.cogeo.org/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/davemlz/awesome-spectral-indices) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/microsoft/torchgeo/blob/main/docs/tutorials/indices.ipynb) | 19.12.2024 |
| AutoGen | Framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks | [microsoft](https://github.com/microsoft) | [![](https://camo.githubusercontent.com/bbb6473b9c51bb308dd56301a0b8326babe2f298f927cbebf276c59781266e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f6175746f67656e3f7374796c653d736f6369616c)](https://github.com/microsoft/autogen) <br>- [blog post](https://www.microsoft.com/en-us/research/blog/autogen-enabling-next-generation-large-language-model-applications/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/pAbnFJrkgZ)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@multiplatform.ai/microsoft-autogen-transforming-ai-frameworks-for-enhanced-problem-solving-video-ac2655e7cdf)<br>- [project](https://microsoft.github.io/autogen/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/zdcCD--IieY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dCCr52uT0W8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/JMpgsx74XDI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/quickstart.ipynb) | 19.12.2024 |
| Langfun | PyGlove powered library that aims to make language models fun to work with | [Daiyi Peng](https://github.com/daiyip) | [![](https://camo.githubusercontent.com/e2464132cc99dedd579545dbeab3bd66919c09fe4fc7a86fc4f1b1abaaabc437/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6c616e6766756e3f7374796c653d736f6369616c)](https://github.com/google/langfun)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/U6wPN9R68k)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/pallets/jinja)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/langfun/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/langfun/blob/main/docs/notebooks/langfun101.ipynb) | 16.12.2024 |
| ComfyUI | Powerful and modular stable diffusion GUI and backend | [comfyanonymous](https://github.com/comfyanonymous) | [![](https://camo.githubusercontent.com/57efebdb98eae1b1a9ccda191d64c0937aa4a3ac775c6aacd529671e7d7bc566/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f6d6679616e6f6e796d6f75732f436f6d667955493f7374796c653d736f6369616c)](https://github.com/comfyanonymous/ComfyUI) <br>- [examples](https://comfyanonymous.github.io/ComfyUI_examples/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/madebyollin/taesd)<br>- [pytorch](https://developer.apple.com/metal/pytorch/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/10lzgze/i_figured_out_a_way_to_apply_different_prompts_to/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/vUTV85D51yk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gySLXbe7WZQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ovjeVGmy6ZM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/comfyanonymous/ComfyUI/blob/master/notebooks/comfyui_colab.ipynb) | 13.12.2024 |
| TransformerLens | Library for doing mechanistic interpretability of GPT-2 Style language models | - [Neel Nanda](https://www.neelnanda.io/about)<br>- [Joseph Bloom](https://github.com/jbloomAus) | [![](https://camo.githubusercontent.com/006f073f0ecf6c0e8b6e23a01c466d0c0db0ce83a2e475a0f5d96a979a856195/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5472616e73666f726d65724c656e734f72672f5472616e73666f726d65724c656e733f7374796c653d736f6369616c)](https://github.com/TransformerLensOrg/TransformerLens)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2302.03025), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.08112)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://transformerlensorg.github.io/TransformerLens/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jbloomAus/DecisionTransformerInterpretability)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@fgkffbvkhg/transformerlens-understanding-the-model-e339be551299)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/transformer-lens/)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-1qosyh8g3-9bF3gamhLNJiqCL_QqLFrA)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/channel/UCBMJ0D-omcRay8dh4QT0doQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/oL67e-uEgWI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Main_Demo.ipynb) | 09.12.2024 |
| Supervision | Reusable computer vision tools | [Roboflow](https://roboflow.com/about) | [![](https://camo.githubusercontent.com/1b39f8841d1d11b310e911a38b018c9079106a311e56bd8419e0df2defde48f0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726f626f666c6f772f7375706572766973696f6e3f7374796c653d736f6369616c)](https://github.com/roboflow/supervision)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/GbfgXGJ8Bk)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://github.com/roboflow/inference), [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.roboflow.com/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/roboflow/notebooks)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/Roboflow/Annotators)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/leoroboflow/inferring-on-a-dataset-with-a-roboflow-model)<br>- [website](https://supervision.roboflow.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uWP6UjDeZvY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/4Q3ut7vqD5o), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtube.com/roboflow) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb) | 05.12.2024 |
| XManager | Framework for managing machine learning experiment | [Andrew Chen](https://github.com/andrewluchen) | [![](https://camo.githubusercontent.com/45ae4d006532193e71a05a1d2b84fd503aac29983529118e51b0208cf1e981c4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d646565706d696e642f786d616e616765723f7374796c653d736f6369616c)](https://github.com/google-deepmind/xmanager) <br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/xmanager/)<br>- [slides](https://storage.googleapis.com/gresearch/xmanager/deepmind_xmanager_slides.pdf) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-deepmind/xmanager/blob/main/colab_codelab.ipynb) | 05.12.2024 |
| Flax | Neural network library and ecosystem for JAX designed for flexibility | - [Jonathan Heek](https://github.com/jheek)<br>- [Anselm Levskaya](https://anselmlevskaya.com/)<br>- [Avital Oliver](https://github.com/avital)<br>- [Marvin Ritter](https://github.com/Marvin182)<br>others[Bertrand Rondepierre](https://github.com/BertrandRdp)<br>[Andreas Steiner](https://github.com/andsteing)<br>[Marc van Zee](https://research.google/people/marc-van-zee/) | [![](https://camo.githubusercontent.com/ac616e049f8a071b14833e99c8134467a386cb4a3fa4bf6a6fb0f71f73b8abc9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f666c61783f7374796c653d736f6369616c)](https://github.com/google/flax)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://flax.readthedocs.io/)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://github.com/huggingface/transformers/tree/main/examples/flax)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/syncedreview/google-introduces-flax-a-neural-network-library-for-jax-84bdc6f8f160)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/erpdf7/p_flax_a_neural_network_library_for_jax_designed/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/e8StU6WQCqw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/HOlQzrn84A4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5eUSmJvK8WA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/quick_start.ipynb) | 05.12.2024 |
| Haiku | A library built on top of JAX designed to provide simple, composable abstractions for machine learning research | - [Tom Hennigan](https://github.com/tomhennigan)<br>- [Trevor Cai](https://github.com/trevorcai)<br>- [Tamara Norman](https://github.com/tamaranorman)<br>- [Igor Babuschkin](https://www.babushk.in/) | [![](https://camo.githubusercontent.com/40937edb8df4ebe86f8e408ad6607182c762f726b4ae88de5997725e9ec5b331/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f646d2d6861696b753f7374796c653d736f6369616c)](https://github.com/deepmind/dm-haiku) <br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://dm-haiku.readthedocs.io/en/latest/)<br>- [website](https://www.haiku-os.org/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/dm-haiku/blob/main/examples/haiku_lstms.ipynb) | 05.12.2024 |
| SAE Lens | Training Sparse Autoencoders on Language Models | - [Joseph Bloom](https://github.com/jbloomAus)<br>- [Curt Tigges](https://curttigges.com/)<br>- [David Chanin](https://chanind.github.io/) | [![](https://camo.githubusercontent.com/663e9d11ee3d9d4f473a03455c5a3fac9cadbc5f10e5bf54d47085ed80b9f13f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a626c6f6f6d4175732f5341454c656e733f7374796c653d736f6369616c)](https://github.com/jbloomAus/SAELens)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://jbloomaus.github.io/SAELens/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/sae-lens/)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-2k0id7mv8-CsIgPLmmHd03RPJmLUcapw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb) | 03.12.2024 |
| moondream | Tiny vision language model that kicks ass and runs anywhere | [Vik Korrapati](https://github.com/vikhyat) | [![](https://camo.githubusercontent.com/58a411121543468c9cef0fcf3a55a536064d46eafe37753a4f23bcae857bf0d3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76696b687961742f6d6f6f6e647265616d3f7374796c653d736f6369616c)](https://github.com/vikhyat/moondream)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.com/invite/tRUdpjDQfH)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kijai/ComfyUI-moondream)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/vikhyatk/moondream2), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/google/docci), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/vikhyatk/moondream1)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@indradumnabanerjee/getting-started-with-vision-language-model-moondream-783c264a02b9)<br>- [website](https://moondream.ai/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/vikhyat/moondream/blob/main/notebooks/Finetuning.ipynb) | 30.11.2024 |
| Feast | An open source feature store for machine learning | - [Willem Pienaar](https://github.com/woop)<br>- [Danny Chiao](https://github.com/adchia)<br>- [Achal Shah](http://achals.com/)<br>- [Terence Lim](https://terryyylim.github.io/portfolio/)<br>others[Ches Martin](https://github.com/ches)<br>[Judah Rand](https://github.com/judahrand)<br>[Matt Delacour](https://github.com/MattDelac)<br>[Miguel Trejo Marrufo](https://github.com/TremaMiguel)<br>[Francisco Javier Arceo](https://franciscojavierarceo.github.io/) | [![](https://camo.githubusercontent.com/a867fb66d0abc1cede061f8fa3723eaa02d91eea6e6a6cd4313199294fe64990/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66656173742d6465762f66656173743f7374796c653d736f6369616c)](https://github.com/feast-dev/feast)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.feast.dev/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/baineng/feast-hive), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Shopify/feast-trino), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Azure/feast-azure), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/amundsen-io/amundsen/blob/main/databuilder/databuilder/extractor/feast_extractor.py)<br>- [website](https://feast.dev/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/DaNv-Wf1MBA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/p2cuq4eJ2BY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/feast-dev/feast/blob/master/examples/quickstart/quickstart.ipynb) | 22.11.2024 |
| FiftyOne | Open-source tool for building high-quality datasets and computer vision models | - [Brian Moore](https://github.com/brimoor)<br>- [Jason Corso](https://web.eecs.umich.edu/~jjcorso/) | [![](https://camo.githubusercontent.com/23fe44634799ad671345c9ef0503558e2201781ac67e175d818c0b028b49dcc8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f766f78656c35312f66696674796f6e653f7374796c653d736f6369616c)](https://github.com/voxel51/fiftyone) <br>- [blog post](https://voxel51.com/blog/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.voxel51.com/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/voxel51/fiftyone-examples)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/voxel51), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/open-source-tools-for-fast-computer-vision-model-building-b39755aab490)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://slack.voxel51.com/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/voxel51)<br>- [website](https://voxel51.com/fiftyone/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLuREAXoPgT0SJLKsgFzKxffMApbXp90Gi) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/voxel51/fiftyone-examples/blob/master/examples/quickstart.ipynb) | 21.11.2024 |
| CatBoost | High-performance open source library for gradient boosting on decision trees | - [Anna Veronika Dorogush](https://github.com/annaveronika)<br>- [Vasily Ershov](https://linkedin.com/in/vasily-ershov-04768199)<br>- [Andrey Gulin](https://www.linkedin.com/in/andreygulin)<br>- [Liudmila Prokhorenkova](https://github.com/ostroumova-la)<br>others[Gleb Gusev](https://scholar.google.com/citations?user=RWX4sYcAAAAJ)<br>[Aleksandr Vorobev](https://scholar.google.com/citations?user=WiCXGGIAAAAJ) | [![](https://camo.githubusercontent.com/b71756c7dc3d052c12754cf7a648963b86f3d419e952e51778d77782fae73a2a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636174626f6f73742f636174626f6f73743f7374796c653d736f6369616c)](https://github.com/catboost/catboost)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.11363), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.09516)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://catboost.ai/en/docs/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@mohan-gupta/catboost-algorithm-2156129d740d)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper_files/paper/2018/hash/14491b756b3a51daac41c24863285549-Abstract.html)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/catboost/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/CatBoostML)<br>- [website](https://catboost.ai/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/CatBoost)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8o0e-r0B5xQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/usdEWSDisS0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/KXOTSkPL2X4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/UYDwhuyWYSo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/xl1fwCza9C8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Q_xa4RvnDcY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ySla2kczbeM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/47-mAVms-b8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nrGt5VKZpzc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/catboost/tutorials/blob/master/python_tutorial.ipynb) | 18.11.2024 |
| Gemma 2 | New addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters | [unsloth](https://unsloth.ai/) | [![](https://camo.githubusercontent.com/1b27822e8ac242c69b6392cda35db29df4aed3ea4a700371600b85b053448e8d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756e736c6f746861692f756e736c6f74683f7374796c653d736f6369616c)](https://github.com/unslothai/unsloth) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2408.00118)<br>- [blog post](https://blog.google/technology/developers/google-gemma-2/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/unsloth)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/google/gemma-2-2b)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/danielhanchen/kaggle-gemma-7b-unsloth-notebook/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/unsloth/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/t3js5iy1pcE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/xxCkuxQuT_g), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/4N38V4h9S0A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qFULISWcjQc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MARG5S1uNbc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4) | 17.11.2024 |
| Llama 3.1 | First openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation | [unsloth](https://unsloth.ai/) | [![](https://camo.githubusercontent.com/1b27822e8ac242c69b6392cda35db29df4aed3ea4a700371600b85b053448e8d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756e736c6f746861692f756e736c6f74683f7374796c653d736f6369616c)](https://github.com/unslothai/unsloth) <br>- [blog post](https://unsloth.ai/blog/llama3-1)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/unsloth)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/meta-llama)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook)<br>- [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://ai.meta.com/blog/meta-llama-3-1/), [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://llama.meta.com/), [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/unsloth/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/unslothai)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/QyRWqJehK7I), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1xdneyn6zjw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/p5O-_AiKD_Q), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/4rk9fHIOGTU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp) | 17.11.2024 |
| Mistral Small | Enterprise-grade small model | [unsloth](https://unsloth.ai/) | [![](https://camo.githubusercontent.com/1b27822e8ac242c69b6392cda35db29df4aed3ea4a700371600b85b053448e8d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756e736c6f746861692f756e736c6f74683f7374796c653d736f6369616c)](https://github.com/unslothai/unsloth) <br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/unsloth)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/unsloth/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/LocalLLaMA/comments/1fj4unz/mistralaimistralsmallinstruct2409_new_22b_from/)<br>- [website](https://mistral.ai/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/damcEQdlpqY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1oCEHcED15DzL8xXGU1VTx5ZfOJM8WY01) | 17.11.2024 |
| DPO Zephyr | Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization to learn a chat model with significantly improved intent alignment | - [Lewis Tunstall](https://lewtun.github.io/blog/)<br>- [Edward Beeching](https://edbeeching.github.io/)<br>- [Nathan Lambert](https://www.natolambert.com/)<br>- [Nazneen Rajani](https://www.nazneenrajani.com/)<br>others[Kashif Rasul](https://github.com/kashif)<br>[Younes Belkada](https://github.com/younesbelkada)<br>[Shengyi Huang](https://costa.sh/)<br>[Leandro Werra](https://github.com/lvwerra)<br>[Clémentine Fourrier](https://github.com/clefourrier)<br>[Nathan Habib](https://github.com/NathanHB)<br>[Nathan Sarrazin](https://nsarrazin.com/)<br>[Omar Sanseviero](https://osanseviero.github.io/hackerllama/)<br>[Alexander Rush](https://scholar.google.com/citations?user=LIjnUGgAAAAJ)<br>[Thomas Wolf](https://thomwolf.io/) | [![](https://camo.githubusercontent.com/ac1d0d0cb26c222a1017790616194abf97fa949cc2e6e852b05127e1c1e50ccf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f78666163746c61622f6f72706f3f7374796c653d736f6369616c)](https://github.com/xfactlab/orpo)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2310.16944)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/unsloth)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/unslothai/unsloth)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/blog/alvarobartt/notus-7b-v1), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/alignment-handbook/zephyr-7b-dpo-full)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://artgor.medium.com/paper-review-zephyr-direct-distillation-of-lm-alignment-035191df982e), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://isamu-website.medium.com/understanding-zephyr-12c5b9d3b822)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/unsloth/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/17lmxk7/r_zephyr_direct_distillation_of_lm_alignment/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://x.com/Thom_Wolf/status/1717821614467739796)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/wr2bHf41VE4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Js59UF_HUAs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/cuObPxCOBCw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/r2xpSzwetp0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP) | 17.11.2024 |
| ORPO | Get up and running with large language models | - [Jiwoo Hong](https://jiwooya1000.github.io/)<br>- [Noah Lee](https://nlee-208.github.io/)<br>- [James Thorne](https://jamesthorne.com/) | [![](https://camo.githubusercontent.com/ac1d0d0cb26c222a1017790616194abf97fa949cc2e6e852b05127e1c1e50ccf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f78666163746c61622f6f72706f3f7374796c653d736f6369616c)](https://github.com/xfactlab/orpo)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2403.07691)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/unsloth)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/unslothai/unsloth)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/reciperesearch/dolphin-sft-v0.1-preference), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/trl/main/en/orpo_trainer)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@AriaLeeNotAriel/numbynum-orpo-monolithic-optimization-without-reference-model-hong-et-al-2024-reviewed-262d0778e08c), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@zergtant/optimizing-language-model-preferences-without-a-reference-model-introducing-the-orpo-method-1144b3e7aec3)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/unsloth/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/LLMResearch/comments/1bh8iq5/orpo_monolithic_preference_optimization_without/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/52kMBrAI_IM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/6kkJGkPZP88), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8MEPCPdKUH8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/11t4njE3c4Lxl-07OD8lJSMKkfyJml3Tn) | 17.11.2024 |
| Phi-3.5 | 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5, despite being small enough to be deployed on a phone | [unsloth](https://unsloth.ai/) | [![](https://camo.githubusercontent.com/1b27822e8ac242c69b6392cda35db29df4aed3ea4a700371600b85b053448e8d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756e736c6f746861692f756e736c6f74683f7374796c653d736f6369616c)](https://github.com/unslothai/unsloth) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2404.14219)<br>- [blog post](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/unsloth)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@mysocial81/phi-3-5-microsofts-efficient-multilingual-and-secure-open-source-slms-5ed7d36738aa)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/unsloth/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/mlscaling/comments/1cberec/phi3_technical_report_a_highly_capable_language/), [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/LocalLLaMA/comments/1ey5i22/phi35_is_very_safe_microsoft_really_outdid/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/unslothai)<br>- [website](https://azure.microsoft.com/en-us/products/phi-3)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Enp70Kkjb8k) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4) | 17.11.2024 |
| Simple audio recognition | This tutorial will show you how to build a basic speech recognition network that recognizes ten different words | [Google](https://www.tensorflow.org/) | - [coursera](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/speech-recognition)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/speech_commands), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/audio/simple_audio)<br>- [tf.js](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb) | 15.11.2024 |
| xFormers | Toolbox to Accelerate Research on Transformers | - [Benjamin Lefaudeux](https://github.com/blefaudeux)<br>- [Francisco Massa](https://github.com/fmassa)<br>- [Diana Liskovich](https://www.linkedin.com/in/dianaliskovich)<br>- [Wenhan Xiong](https://xwhan.github.io/)<br>others[Vittorio Caggiano](https://vittorio-caggiano.github.io/)<br>[Sean Naren](https://github.com/SeanNaren)<br>[Min Xu](https://github.com/min-xu-ai)<br>[Jieru Hu](https://github.com/jieru-hu)<br>[Marta Tintore](https://github.com/MartaTintore)<br>[Susan Zhang](https://suchenzang.github.io/)<br>[Patrick Labatut](https://github.com/patricklabatut)<br>[Daniel Haziza](https://scholar.google.com/citations?user=2eSKdFMAAAAJ) | [![](https://camo.githubusercontent.com/62d94c836ff6525ecd9b0b9859480ce40efc5ba3689fb95be28d8a41c7437374/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f78666f726d6572733f7374796c653d736f6369616c)](https://github.com/facebookresearch/xformers)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://facebookresearch.github.io/xformers/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-research/sputnik), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hgyhungry/ge-spmm), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/triton), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/RobinBruegger/RevTorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mlpen/Nystromformer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/fairscale), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/huggingface/pytorch-image-models), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Dao-AILab/flash-attention)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/NJyZCdxnGe4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/xformers/blob/main/docs/source/xformers_mingpt.ipynb) | 13.11.2024 |
| High-performance simulations with TFF | This tutorial will describe how to setup high-performance simulations with TFF in a variety of common scenarios | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | - [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/federated-learning)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-federated/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/tutorials/simulations) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/simulations.ipynb) | 01.11.2024 |
| Autodistill | Uses big, slower foundation models to train small, faster supervised models | [autodistill](https://github.com/autodistill) | [![](https://camo.githubusercontent.com/c62d847952be21af33a3611f569579505b739360aa218da771156aec4b408990/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6175746f64697374696c6c2f6175746f64697374696c6c3f7374796c653d736f6369616c)](https://github.com/autodistill/autodistill) <br>- [blog post](https://blog.roboflow.com/autodistill/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.autodistill.com/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-grounded-sam), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-yolov8), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-yolonas), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-yolov5), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-detr), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-detic), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-grounding-dino), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-owl-vit), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-sam-clip), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-llava), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-kosmos-2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-owlv2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-roboflow-universe), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-azure-vision), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-rekognition), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/autodistill/autodistill-gcp-vision), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/roboflow/inference)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gKTYMfwPo4M), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/M_QZ_Q0zT0k), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtube.com/roboflow) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb) | 01.11.2024 |
| Crawl4AI | LLM Friendly Web Crawler & Scrapper | [UncleCode](https://github.com/unclecode) | [![](https://camo.githubusercontent.com/72f9e500bc258c28125926ccfcf0af0ef85a76b85cfcbed8fcab6a39f6ebf34b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756e636c65636f64652f637261776c3461693f7374796c653d736f6369616c)](https://github.com/unclecode/crawl4ai)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://crawl4ai.com/mkdocs/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@pankaj_pandey/crawl4ai-your-ultimate-asynchronous-web-crawling-companion-%EF%B8%8F-66a21cf57c0a)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/Crawl4AI/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/unclecode)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Ex3EpKxlMO0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/KAvuVUh0XU8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/lpOb1bQO7aM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/81KIBvg0bsQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb) | 30.10.2024 |
| XGBoost | Optimized distributed gradient boosting library designed to be highly efficient, flexible and portable | - [Tianqi Chen](https://tqchen.com/)<br>- [Carlos Guestrin](https://guestrin.su.domains/) | [![](https://camo.githubusercontent.com/cbdd5105a30ed3ebbc581fe471bcb6b4d3cf178b5e89613e7c7139d829b9dc20/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f323933393637322e32393339373835)](https://doi.org/10.1145/2939672.2939785)[![](https://camo.githubusercontent.com/f87d11f2584f82abc9b6a251c09e99080ba7d3a9bc2ec0ededa17e7ec293321c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646d6c632f7867626f6f73743f7374796c653d736f6369616c)](https://github.com/dmlc/xgboost)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://xgboost.readthedocs.org/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.python.org/pypi/xgboost/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/XGBoostProject)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Gradient_boosting), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/XGBoost)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLblh5JKOoLULU0irPgs1SnKO6wqVjKUsQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/vV12dGe_Fho), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gPciUPwWJQQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TyvYZ26alZs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/kho6oANGu_A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0Xc9LIb_HTw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/OQKQHNCVf5k) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-training/xgboost/notebooks/how_to_use_comet_with_xgboost_tutorial.ipynb) | 22.10.2024 |
| YOLOv5 | You Only Look Once | [Glenn Jocher](https://github.com/glenn-jocher) | [![](https://camo.githubusercontent.com/c4b9baacd1c41802bd1cab446fd9e0918cdead966293fc2713e04ecd067b431d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756c7472616c79746963732f796f6c6f76353f7374796c653d736f6369616c)](https://github.com/ultralytics/yolov5) <br>- [data](http://cocodataset.org/#upload)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/ultralytics/yolov5), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/ultralytics/coco128) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb) | 19.10.2024 |
| YOLOv3 | You Only Look Once | [Glenn Jocher](https://github.com/glenn-jocher) | [![](https://camo.githubusercontent.com/44655dae6444f06099d49af479b703057e872b1788dbf203320973ccfc3bd8c5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756c7472616c79746963732f796f6c6f76333f7374796c653d736f6369616c)](https://github.com/ultralytics/yolov3) <br>- [data](http://cocodataset.org/#upload)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/ultralytics/yolov3), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/ultralytics/coco128) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb) | 19.10.2024 |
| Swarm | Educational framework exploring ergonomic, lightweight multi-agent orchestration | - [Ilan Bigio](https://ilanbigio.com/)<br>- [James Hills](https://github.com/jhills20)<br>- [Shyamal Anadkat](https://shyamal.me/)<br>- [Charu Jaiswal](https://github.com/charuj)<br>others[Colin Jarvis](https://github.com/colin-openai)<br>[Katia Guzman](https://github.com/katia-openai) | [![](https://camo.githubusercontent.com/c144a0e783e92062296c1af8e0d2eafb305215f954643a5033fd1d5d8ceaa795/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e61692f737761726d3f7374796c653d736f6369616c)](https://github.com/openai/swarm)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@michael_79773/exploring-openais-swarm-an-experimental-framework-for-multi-agent-systems-5ba09964ca18), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://ai.plainenglish.io/openai-releases-swarm-what-is-it-b61ecb88d67e)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/LocalLLaMA/comments/1g56itb/openai_swarm_the_agentic_framework_should_you_care/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Cw0ME8OZ0xI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/q7_5eCmu0MY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/LBih635lzps), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/npAljHBeKPc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/NirDiamant/GenAI_Agents/blob/main/all_agents_tutorials/blog_writer_swarm.ipynb) | 15.10.2024 |
| LM Evaluation Harness | Framework for few-shot evaluation of language models. | [EleutherAI](https://www.eleuther.ai/) | [![](https://camo.githubusercontent.com/8524788891e8bc5377c656f85fa0c020b65de46ab68317f4725e25928ab9d7df/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f456c65757468657241492f6c6d2d6576616c756174696f6e2d6861726e6573733f7374796c653d736f6369616c)](https://github.com/EleutherAI/lm-evaluation-harness)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.14165)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/eleutherai)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AutoGPTQ/AutoGPTQ), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/EleutherAI/gpt-neox), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/Megatron-DeepSpeed), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vllm-project/vllm)<br>- [project](https://www.eleuther.ai/projects/large-language-model-evaluation) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/EleutherAI/lm-evaluation-harness/blob/main/examples/lm-eval-overview.ipynb) | 04.10.2024 |
| CodeGemma | How to load, fine-tune and deploy CodeGemma model on SQL by utilising Hugging Face | [Carlo Fisicaro](https://github.com/carlofisicaro) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2406.11409)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://ai.google.dev/gemma/docs/codegemma)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/google/codegemma-7b)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/models/google/codegemma)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/LocalLLaMA/comments/1bzr24e/codegemma_an_official_google_release_for_code/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/diqk-LscvBs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/CodeGemma/CodeGemma_finetuned_on_SQL_with_HF.ipynb) | 27.09.2024 |
| Multimodal Maestro | Gives you more control over large multimodal models to get the outputs you want | [Roboflow](https://roboflow.com/about) | [![](https://camo.githubusercontent.com/62d4444e5830025e14de23d1d543026515e80bb4a80b23a951ca77095f7e854d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726f626f666c6f772f6d756c74696d6f64616c2d6d61657374726f3f7374796c653d736f6369616c)](https://github.com/roboflow/multimodal-maestro)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2310.11441), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.17421)<br>- [blog post](https://blog.roboflow.com/multimodal-maestro-advanced-lmm-prompting/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/computervision/comments/186o2b2/multimodal_maestro_prompt_tools_for_use_with_lmms/)<br>- [website](https://maestro.roboflow.com/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/roboflow/multimodal-maestro/blob/develop/cookbooks/multimodal_maestro_gpt_4_vision.ipynb) | 26.09.2024 |
| TRL | Set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step, Reward Modeling step to the Proximal Policy Optimization step | - [Leandro Werra](https://github.com/lvwerra)<br>- [Younes Belkada](https://github.com/younesbelkada)<br>- [Lewis Tunstall](https://lewtun.github.io/blog/)<br>- [Edward Beeching](https://edbeeching.github.io/)<br>others[Tristan Thrush](http://www.tristanthrush.com/)<br>[Nathan Lambert](https://www.natolambert.com/) | [![](https://camo.githubusercontent.com/39edb3493dd498fa6daf101de06e1b9f5f2325629a505583dbc1d3f44f2abbf7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f74726c3f7374796c653d736f6369616c)](https://github.com/huggingface/trl)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.08593)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](http://hf.co/docs/trl)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/lm-human-preferences)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/xQ5nc1CF7iQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/67SO20dszNA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/trl/blob/master/examples/notebooks/best_of_n.ipynb) | 24.09.2024 |
| PEFT | Parameter-Efficient Fine-Tuning methods enable efficient adaptation of pre-trained language models to various downstream applications without fine-tuning all the model's parameters | - [Sourab Mangrulkar](https://github.com/pacman100)<br>- [Sylvain Gugger](https://github.com/sgugger)<br>- [Lysandre Debut](http://lysand.re/)<br>- [Younes Belkada](https://github.com/younesbelkada)<br>- [Sayak Paul](https://sayak.dev/) | [![](https://camo.githubusercontent.com/ddd2294aaafd345b7629f176dad26d2fca990409c5ab6e7270846706f40c6992/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f706566743f7374796c653d736f6369616c)](https://github.com/huggingface/peft) <br>- [blog post](https://www.philschmid.de/fine-tune-flan-t5-peft)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://huggingface.co/docs/peft)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/DeepSpeed/issues/3002)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/ought/raft/viewer/twitter_complaints), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/bigscience/T0_3B), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/bigscience/mt0-xxl), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/opt-6.7b), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/roberta-large), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/glue/viewer/mrpc)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YVU5wAA6Txo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Us5ZFp16PaU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YKCtbIJC3kQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/peft/blob/master/examples/int8_training/Finetune_flan_t5_large_bnb_peft.ipynb) | 13.09.2024 |
| SAA+ | Framework, Segment Any Anomaly +, for zero-shot anomaly segmentation with hybrid prompt regularization to improve the adaptability of modern foundation models | - [Yunkang Cao](https://caoyunkang.github.io/)<br>- [Xiaohao Xu](https://scholar.google.com/citations?user=3Ifn2DoAAAAJ)<br>- [Chen Sun](https://www.researchgate.net/profile/Chen-Sun-58)<br>- [Yuqi Cheng](https://scholar.google.com/citations?user=02BC-WgAAAAJ)<br>others[Zongwei Du](https://github.com/duzongwei)<br>[Liang Gao](https://scholar.google.com/citations?user=NqIi8_8AAAAJ)<br>[Weiming Shen](https://scholar.google.com/citations?user=FuSHsx4AAAAJ) | [![](https://camo.githubusercontent.com/23020854e2de9614557db39fe8e33d1639db86f29b3b513517e346889c06da27/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f63616f79756e6b616e672f5365676d656e742d416e792d416e6f6d616c793f7374796c653d736f6369616c)](https://github.com/caoyunkang/Segment-Any-Anomaly)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.10724)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/abin24/Magnetic-tile-defect-datasets.), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/caoyunkang/WinClip)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/Caoyunkang/Segment-Any-Anomaly) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/12Sh0j92YYmTa0oIuSEWWpPBCpIwCSVhz) | 13.09.2024 |
| TensorRT | SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications | [nvidia](https://developer.nvidia.com/) | [![](https://camo.githubusercontent.com/0375d1cf222c092085220f7c6c3c9969ea4549482355526cf93ed5d689fcd87b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e56494449412f54656e736f7252543f7374796c653d736f6369616c)](https://github.com/NVIDIA/TensorRT) <br>- [blog post](https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorrt-updated/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.nvidia.com/deeplearning/tensorrt/)<br>- [forum](https://forums.developer.nvidia.com/c/ai-data-science/deep-learning/tensorrt)<br>- [website](https://developer.nvidia.com/tensorrt)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TU5BMU6iYZ0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/6rZNLaS775w), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/G_KhUFCUSsY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7kJ-jph9gCw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/NVIDIA/TensorRT/blob/main/quickstart/IntroNotebooks/0.%20Running%20This%20Guide.ipynb) | 12.09.2024 |
| DataChain | AI-dataframe to enrich, transform and analyze data from cloud storages for ML training and LLM apps | [Iterative](https://iterative.ai/) | [![](https://camo.githubusercontent.com/93b34d0029ee244dfde7b397d0021bb762fc53aaa84b94b36f50b36ea0d7610b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6974657261746976652f64617461636861696e3f7374796c653d736f6369616c)](https://github.com/iterative/datachain)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://dvc.org/chat)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://datachain.dvc.ai/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/datachain/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/DVCorg)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qoqhllB3gN8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/JT5AwGz5QMI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/iterative/datachain-examples/blob/main/multimodal/clip_fine_tuning.ipynb) | 09.09.2024 |
| TFF for Federated Learning Research: Model and Update Compression | We use the EMNIST dataset to demonstrate how to enable lossy compression algorithms to reduce communication cost in the Federated Averaging algorithm | [Weikang Song](https://github.com/swkpku) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1602.05629)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/federated-learning)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-federated/)<br>- [tensor encoding](http://jakubkonecny.com/files/tensor_encoding.pdf)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/emnist), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/api_docs/python/tff/learning/build_federated_averaging_process), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/federated/tutorials/tff_for_federated_learning_research_compression) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/tff_for_federated_learning_research_compression.ipynb) | 05.09.2024 |
| LlamaIndex | Data framework for your LLM application | [Jerry Liu](https://github.com/jerryjliu) | [![](https://camo.githubusercontent.com/42916e39576dd21ceb99b5b15260429304ae53f5659833c19a08eda99247e0fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f72756e2d6c6c616d612f6c6c616d615f696e6465783f7374796c653d736f6369616c)](https://github.com/run-llama/llama_index)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/dGcwcsnxhU)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.llamaindex.ai/en/stable/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/run-llama/LlamaIndexTS), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/run-llama/llama-lab)<br>- [![meta](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/meta.svg)](https://llama.meta.com/docs/integration-guides/llamaindex/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/llama-index/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/llama_index)<br>- [website](https://www.llamaindex.ai/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/@LlamaIndex), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TRjq7t2Ms5I), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/pApPGFwbigI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/zeAyuLc_f3Q), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hH4WkgILUD4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/v6g8eo86T8A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/FQBou-YgxyE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bQw92baScME), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/cNMYeW2mpBs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/oreilly_course_cookbooks/Module-2/Components_Of_LlamaIndex.ipynb) | 05.09.2024 |
| Deforum Stable Diffusion | Open source project is designed to be free to use and easy to modify for custom needs and pipelines | - [EnzymeZoo](https://linktr.ee/enzymezoo)<br>- [Артем Храпов](https://github.com/kabachuha)<br>- [Forest Star Walz](https://github.com/reallybigname)<br>- [pharmapsychotic](https://github.com/pharmapsychotic) | [![](https://camo.githubusercontent.com/52d47d871aaac6be12bdfb0b9c6709563edc6d817c84f0489c272af336e9d696/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6465666f72756d2d6172742f6465666f72756d2d737461626c652d646966667573696f6e3f7374796c653d736f6369616c)](https://github.com/deforum-art/deforum-stable-diffusion) <br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/deforum)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.google.com/document/d/1RrQv7FntzOuLg4ohjRZPVL7iptIyBhwwbcEYEW2OfcI)<br>- [project](https://deforum.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/w_sxuDMt_V0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bicPayZDI60), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dqkQo2alZvU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deforum-art/deforum-stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb) | 30.08.2024 |
| Nerfstudio | API that allows for a simplified end-to-end process of creating, training, and testing NeRFs | - [Matthew Tancik](https://github.com/tancik)<br>- [Ethan Weber](https://ethanweber.me/)<br>- [Evonne Ng](http://people.eecs.berkeley.edu/~evonne_ng/)<br>- [Ruilong Li](http://www.liruilong.cn/)<br>others[Brent Yi](https://github.com/brentyi)<br>[Justin Kerr](https://kerrj.github.io/)<br>[Terrance Wang](https://github.com/terrancewang)<br>[Alexander Kristoffersen](https://akristoffersen.com/)<br>[Jake Austin](https://github.com/jake-austin)<br>[Kamyar Salahi](https://github.com/TheQuantumFractal)<br>[Abhik Ahuja](https://abhikahuja.com/)<br>[David McAllister](https://github.com/mcallisterdavid)<br>[Angjoo Kanazawa](https://github.com/akanazawa) | [![](https://camo.githubusercontent.com/0222dcd8ca62a7c981e3eeb99f5682d644a39de6ebf0780a2396dad9d34b0c38/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e65726673747564696f2d70726f6a6563742f6e65726673747564696f3f7374796c653d736f6369616c)](https://github.com/nerfstudio-project/nerfstudio) <br>- [Viewer](https://viewer.nerf.studio/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2302.04264)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/uMbNqcraFc)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.nerf.studio/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/tiny-cuda-nn)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/nerfstudioteam)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/XwKq7qDQCQk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nSFsugarWzk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/h5EWiRRxYEQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8cv9G7izdPY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/nerfstudio-project/nerfstudio/blob/main/colab/demo.ipynb) | 19.08.2024 |
| PyTerrier | A Python framework for performing information retrieval experiments | - [Craig Macdonald](https://www.dcs.gla.ac.uk/~craigm/)<br>- [Nicola Tonellotto](https://github.com/tonellotto) | [![](https://camo.githubusercontent.com/7995db72166894d02294ddc0e70adae1a01dca1d6308b39ae8aef9c4edd2f94e/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333435393633372e33343832303133)](https://doi.org/10.1145/3459637.3482013)[![](https://camo.githubusercontent.com/0dab4f300125a585e17f4dbc56f61f9edc0e7d1127f74b6be15cba7907debb57/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f746572726965722d6f72672f7079746572726965723f7374796c653d736f6369616c)](https://github.com/terrier-org/pyterrier)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2007.14271)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://pyterrier.readthedocs.io/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/terrier-org/ecir2021tutorial), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/terrierteam/pyterrier_ance), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/terrierteam/pyterrier_colbert), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/terrierteam/pyterrier_pisa), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/terrierteam/pyterrier_t5), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/terrierteam/pyterrier_doc2query), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/terrierteam/pyterrier_deepct) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/terrier-org/pyterrier/blob/master/examples/notebooks/non_en_retrieval.ipynb) | 16.08.2024 |
| highway-env | A collection of environments for autonomous driving and tactical decision-making tasks | [Edouard Leurent](https://edouardleurent.com/) | [![](https://camo.githubusercontent.com/ff82e96eb16397701d7db9fb4ae56f822af9be05566f2b4aef3101a8732ebcc2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f656c657572656e742f686967687761792d656e763f7374796c653d736f6369616c)](https://github.com/eleurent/highway-env)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.03483), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.05701), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2101.07140)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://highway-env.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/eleurent/rl-agents), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/eleurent/finite-mdp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/baselines/tree/master/baselines/her) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/eleurent/highway-env/blob/master/scripts/parking_model_based.ipynb) | 09.08.2024 |
| GNN | Production-tested library for building GNNs at large scale | - [Oleksandr Ferludin](https://github.com/aferludin)<br>- [Arno Eigenwillig](https://github.com/arnoegw)<br>- [Martin Blais](https://github.com/blais)<br>- [Dustin Zelle](https://github.com/dzelle)<br>others[Jan Pfeifer](https://github.com/janpfeifer)<br>[Alvaro Sanchez-Gonzalez](https://github.com/alvarosg)<br>[Wai Lok Sibon Li](https://scholar.google.com/citations?user=qX9aUx8AAAAJ)<br>[Sami Abu-El-Haija](https://samihaija.github.io/)<br>[Peter Battaglia](https://scholar.google.com/citations?user=nQ7Ij30AAAAJ)<br>[Neslihan Bulut](https://scholar.google.com/citations?user=k_cadGsAAAAJ)<br>[Jonathan Halcrow](https://scholar.google.com/citations?user=2zZucy4AAAAJ)<br>[Filipe Miguel Gonçalves de Almeida](https://github.com/fmgda)<br>[Pedro Gonnet](https://research.google/people/pedro-gonnet/)<br>[Liangze Jiang](https://liangzejiang.github.io/)<br>[Parth Kothari](https://thedebugger811.github.io/)<br>[Silvio Lattanzi](https://sites.google.com/site/silviolattanzi/)<br>[André Linhares](https://scholar.google.com/citations?user=YYRnhTkAAAAJ)<br>[Brandon Mayer](https://github.com/brandonmayer-zz)<br>[Vahab Mirrokni](https://people.csail.mit.edu/mirrokni/Welcome.html)<br>[John Palowitch](http://ml.johnpalowitch.com/)<br>[Mihir Paradkar](https://www.linkedin.com/in/mihir-paradkar-22b88579)<br>[Jennifer She](https://scholar.google.com/citations?user=Gjf_sd0AAAAJ)<br>[Anton Tsitsulin](https://tsitsul.in/)<br>[Kevin Villela](https://www.linkedin.com/in/kevin-villela-612a6443)<br>[Lisa Wang](https://scholar.google.com/citations?user=5KmYPkIAAAAJ)<br>[Bryan Perozzi](http://www.perozzi.net/) | [![](https://camo.githubusercontent.com/9d59baa8a07242af178f53335634db44af8c817e2990f66f47378ee0ad827cd4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f676e6e3f7374796c653d736f6369616c)](https://github.com/tensorflow/gnn)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.03522)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/fidels/introduction-to-tf-gnn)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@techtes.com/getting-started-with-tf-gnn-with-python-26d8e341db05)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://blog.tensorflow.org/2024/02/graph-neural-networks-in-tensorflow.html), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://blog.tensorflow.org/2021/11/introducing-tensorflow-gnn.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PL2PZTwLd0HMJC1fU_NkwwpRkcjoGqAECX), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/JqWROPYeqjA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YdGN-J322y4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/VDzrvhgyxsU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/e6WHg1l7AMs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/a75Q6dtg1_s) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/gnn/blob/master/examples/notebooks/graph_network_shortest_path.ipynb) | 09.08.2024 |
| Pix2Pix | This notebook demonstrates image to image translation using conditional GAN's | - [Phillip Isola](https://web.mit.edu/phillipi/)<br>- [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)<br>- [Tinghui Zhou](https://tinghuiz.github.io/)<br>- [Alexei Efros](https://people.eecs.berkeley.edu/~efros/) | [![](https://camo.githubusercontent.com/d1af62c11f5e671314c7b037137a684e68055193fd4e46a6a8fcbdcfda205884/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f435650522e323031372e363332)](https://doi.org/10.1109/CVPR.2017.632) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1611.07004)<br>- [data](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/the-ai-team/image-to-image-translation-using-conditional-dcgans-7edc9e78c476)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/generative/pix2pix) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb) | 24.07.2024 |
| Image classification | This tutorial shows how to classify images of flowers | [Billy Lamberta](https://github.com/lamberta) | - [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/image-classification)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/images/classification), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb) | 24.07.2024 |
| Kor | Half-baked prototype that "helps" you extract structured data from text using LLMs | [Eugene Yurtsev](https://eyurtsev.github.io/) | [![](https://camo.githubusercontent.com/36dd648f2dbe2943a67f6fabe9c118d88f3563f7847d30322d19cc2d9c6d9856/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f65797572747365762f6b6f723f7374796c653d736f6369616c)](https://github.com/eyurtsev/kor)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.com/channels/1038097195422978059/1170024642245832774)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://eyurtsev.github.io/kor/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/eyurtsev/kor/blob/main/docs/source/guidelines.ipynb) | 20.07.2024 |
| Mistral Inference | Minimal code to run Mistral models | [mistral](https://mistral.ai/) | [![](https://camo.githubusercontent.com/8be69a4a3d2460da602913440c68c912c2ab58ea1c6a326bd40a781625d4d284/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d69737472616c61692f6d69737472616c2d696e666572656e63653f7374796c653d736f6369616c)](https://github.com/mistralai/mistral-inference) <br>- [blog post](https://mistral.ai/news/announcing-mistral-7b/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.com/invite/mistralai)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.mistral.ai/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@parikshitsaikia1619/mistral-mastery-fine-tuning-fast-inference-guide-62e163198b06)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/mistral-inference/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/mYRqvB1_gRk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mistralai/mistral-inference/blob/main/tutorials/getting_started.ipynb) | 16.07.2024 |
| XLand-MiniGrid | Suite of tools and grid-world environments for meta-reinforcement learning research | - [Alexander Nikulin](https://howuhh.github.io/)<br>- [Vladislav Kurenkov](https://github.com/vkurenkov)<br>- [Ilya Zisman](https://zis.mn/)<br>- [Artem Agarkov](https://github.com/agarkovv)<br>others[Viacheslav Sinii](https://github.com/ummagumm-a)<br>[Sergey Kolesnikov](https://scitator.com/) | [![](https://camo.githubusercontent.com/9c5feeb974331558dfc25831e779f78eb582370f4efd4ebcc7abd74d5126c84a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f64756e6e6f6c61622f786c616e642d6d696e69677269643f7374796c653d736f6369616c)](https://github.com/dunnolab/xland-minigrid)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2312.12044)<br>- [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://deepmind.google/discover/blog/generally-capable-agents-emerge-from-open-ended-play/), [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://sites.google.com/view/adaptive-agent/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/dunno-lab/xland-minigrid-datasets), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Farama-Foundation/MiniGrid), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/MichaelTMatthews/Craftax)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/Howuhh/xland_minigrid)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://neurips.cc/virtual/2023/77592)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/xminigrid/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/vladkurenkov/status/1731709425524543550) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/corl-team/xland-minigrid/blob/main/examples/walkthrough.ipynb) | 12.07.2024 |
| PyTorch3D | Library for deep learning with 3D data | - [Nikhila Ravi](https://nikhilaravi.com/)<br>- [Jeremy Reizenstein](https://github.com/bottler)<br>- [David Novotny](https://d-novotny.github.io/)<br>- [Taylor Gordon](https://scholar.google.com/citations?user=CNOoeQ0AAAAJ)<br>others[Wan-Yen Lo](https://github.com/wanyenlo)<br>[Justin Johnson](https://web.eecs.umich.edu/~justincj/)<br>[Georgia Gkioxari](https://gkioxari.github.io/) | [![](https://camo.githubusercontent.com/891accc4d83d307453a6411fa6e56e5e5069d6f7194a552cbc2a7126dd7dd804/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7079746f72636833643f7374796c653d736f6369616c)](https://github.com/facebookresearch/pytorch3d)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2007.08501), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1906.02739)<br>- [blog post](https://ai.meta.com/blog/implicitron-a-new-modular-extensible-framework-for-neural-implicit-representations-in-pytorch3d/), [blog post](https://ai.meta.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://pytorch3d.readthedocs.org/)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/sohonjit/rendering-with-pytorch3d)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/glimpse-into-pytorch3d-an-open-source-3d-deep-learning-library-291a4beba30f), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@phamtdong0406/crafting-realistic-renderings-with-pytorch3d-947a38194f0a), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/how-to-render-3d-files-using-pytorch3d-ef9de72483f8)<br>- [website](https://pytorch3d.org/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0JEb7knenps), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Pph1r-x9nyY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/eCDBA_SbxCE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MOBAJb5nJRI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/g50RiDnfIfY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hgBk9WlF-XA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Sb9gCCnSAUg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ZLqJ33Ey-MU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/pytorch3d/blob/master/docs/tutorials/implicitron_config_system.ipynb) | 11.07.2024 |
| Stable Diffusion Videos | Create videos with Stable Diffusion by exploring the latent space and morphing between text prompts | [Nathan Raw](https://github.com/nateraw) | [![](https://camo.githubusercontent.com/f77eeec2a1445bbd121b663e739486a611b3041ce3cca9acb186248bcb5ccf85/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e6174657261772f737461626c652d646966667573696f6e2d766964656f733f7374796c653d736f6369616c)](https://github.com/nateraw/stable-diffusion-videos)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://gist.github.com/nateraw/c989468b74c616ebbc6474aa8cdd9e53) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb) | 11.07.2024 |
| Transfer learning and fine-tuning | You will learn how to classify images of cats and dogs by using transfer learning from a pre-trained network | [François Chollet](https://fchollet.com/) | - [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/transfer-learning)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/images/transfer_learning)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Transfer_learning) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb) | 26.06.2024 |
| MARS5 | Speech model for insane prosody | [CAMB.AI](https://www.camb.ai/) | [![](https://camo.githubusercontent.com/2fb0c16172fbfab87413182508eb9e4cf99eeb2133d7da31367310b4e898f6b0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f43616d622d61692f4d415253352d5454533f7374796c653d736f6369616c)](https://github.com/Camb-ai/MARS5-TTS) <br>- [demo](https://6b1a3a8e53ae.ngrok.app/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/FFQNCSKSXX)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/cambai/mars5ttsimage)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.camb.ai/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/RF5/transfusion-asr), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ehoogeboom/multinomial_diffusion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/karpathy/minbpe)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/CAMB-AI/MARS5-TTS)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bmJSLPYrKtE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Camb-ai/mars5-tts/blob/master/mars5_demo.ipynb) | 25.06.2024 |
| ToonCrafter | Can interpolate two cartoon images by leveraging the pre-trained image-to-video diffusion priors | - [Jinbo Xing](https://doubiiu.github.io/)<br>- [Hanyuan Liu](https://github.com/hyliu)<br>- [Menghan Xia](https://menghanxia.github.io/)<br>- [Yong Zhang](https://yzhang2016.github.io/)<br>others[Xintao Wang](https://xinntao.github.io/)<br>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)<br>[Tien-Tsin Wong](https://ttwong12.github.io/myself.html) | [![](https://camo.githubusercontent.com/a671bed61315c0011a77ebe8eba2457ede1bd8020cd32e806670e94126bea62c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f546f6f6e437261667465722f546f6f6e437261667465723f7374796c653d736f6369616c)](https://github.com/ToonCrafter/ToonCrafter) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2405.17933v1)<br>- [project](https://doubiiu.github.io/projects/ToonCrafter/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1d470rv/tooncrafter_generative_cartoon_interpolation/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/u3F35do93_8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/E89R5_hQ5bQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/kK-A9jOaO1U), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ricylysRayw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hc5nF6rGa68), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/mEn3CYU7s_A) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/gist/0smboy/baef995b8f5974f19ac114ec20ac37d5/tooncrafter.ipynb) | 20.06.2024 |
| Brax | A differentiable physics engine that simulates environments made up of rigid bodies, joints, and actuators | - [Daniel Freeman](https://github.com/cdfreeman-google)<br>- [Erik Frey](https://fawx.com/)<br>- [Anton Raichuk](https://scholar.google.com/citations?user=fquIpvgAAAAJ)<br>- [Sertan Girgin](https://sites.google.com/site/girgint/home)<br>others[Igor Mordatch](https://scholar.google.com/citations?user=Vzr1RukAAAAJ)<br>[Olivier Bachem](http://olivierbachem.ch/) | [![](https://camo.githubusercontent.com/5b76c605f1a19fde214ab99c5dd7d91d622f51d06ede70b4d00f6e889f9daccb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f627261783f7374796c653d736f6369616c)](https://github.com/google/brax)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.13281)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://neurips.cc/Conferences/2021/CallForDatasetsBenchmarks) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/brax/blob/main/notebooks/basics.ipynb) | 07.06.2024 |
| DiffSynth | Restructured architectures including Text Encoder, UNet, VAE, among others, maintaining compatibility with models from the open-source community while enhancing computational performance | [Artiprocher](https://github.com/Artiprocher) | [![](https://camo.githubusercontent.com/9f5a2bf1c8752537a24d800a41bb0d310e2e192ccbade6e90607124ef3b1ffda/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4172746970726f636865722f4469666653796e74682d53747564696f3f7374796c653d736f6369616c)](https://github.com/Artiprocher/DiffSynth-Studio)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2401.16224)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/Helsinki-NLP/opus-mt-en-zh), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/alibaba-pai/pai-bloom-1b1-text2prompt-sd) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Artiprocher/DiffSynth-Studio/blob/main/examples/Diffutoon.ipynb) | 06.06.2024 |
| Transformer | This tutorial trains a Transformer model to translate Portuguese to English | - [Ashish Vaswani](https://en.wikipedia.org/wiki/Ashish_Vaswani)<br>- [Noam Shazeer](https://en.wikipedia.org/wiki/Noam_Shazeer)<br>- [Niki Parmar](https://scholar.google.com/citations?user=q2YXPSgAAAAJ)<br>- [Jakob Uszkoreit](http://jakob.uszkoreit.net/)<br>others[Llion Jones](https://scholar.google.com/citations?user=_3_P5VwAAAAJ)<br>[Aidan Gomez](https://aidangomez.ca/)<br>[Łukasz Kaiser](https://scholar.google.com/citations?user=JWmiQR0AAAAJ)<br>[Illia Polosukhin](https://scholar.google.com/citations?user=3SyxFIAAAAAJ) | [![](https://camo.githubusercontent.com/4c557421669276766cbf950260905a8ad6cbde8626a1ca58c88e9732a4d8e4ef/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e65756c61622f776f72642d656d62656464696e67732d666f722d6e6d743f7374796c653d736f6369616c)](https://github.com/neulab/word-embeddings-for-nmt)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.03762), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1903.03878)<br>- [link](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/7181-attention-is-all-you-need)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/text/tutorials/transformer) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb) | 31.05.2024 |
| NeMo | A conversational AI toolkit built for researchers working on automatic speech recognition, natural language processing, and text-to-speech synthesis | - [Oleksii Kuchaiev](http://kuchaev.com/)<br>- [Jason Li](https://scholar.google.com/citations?user=V28bxDwAAAAJ)<br>- [Chip Huyen](https://huyenchip.com/)<br>- [Oleksii Hrinchuk](https://github.com/AlexGrinch)<br>others[Ryan Leary](https://github.com/ryanleary)<br>[Boris Ginsburg](https://github.com/borisgin)<br>[Samuel Kriman](https://github.com/sam1373)<br>[Stanislav Beliaev](https://github.com/stasbel)<br>[Vitaly Lavrukhin](https://github.com/vsl9)<br>[Jack Cook](https://jackcook.com/) | [![](https://camo.githubusercontent.com/3552ebde2ba5cb141114b714cdcb4ee2b5d4ca3d66987e6d3d6daf678d60ac23/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4e56494449412f4e654d6f3f7374796c653d736f6369616c)](https://github.com/NVIDIA/NeMo) <br>- [project](https://docs.nvidia.com/deeplearning/nemo/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/wBgpMf_KQVw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/NVIDIA/NeMo/blob/master/tutorials/00_NeMo_Primer.ipynb) | 25.05.2024 |
| SentencePiece | An unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training | - [Taku Kudo](http://chasen.org/~taku/)<br>- [John Richardson](https://scholar.google.com/citations?user=PEvmYfgAAAAJ) | [![](https://camo.githubusercontent.com/ac153c8f312b5cb42a7b2a777047923ce6c8d9c596a7363e49d0bb51f3a454af/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f73656e74656e636570696563653f7374796c653d736f6369616c)](https://github.com/google/sentencepiece)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1808.06226), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1508.07909), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1804.10959), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.13267), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1609.08144)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rsennrich/subword-nmt), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/gperftools/gperftools), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Microsoft/vcpkg)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://jacky2wong.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/U51ranzJBpY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb) | 21.05.2024 |
| Llama3 from scratch | Llama3 from scratch, one tensor and matrix multiplication at a time | [Nishant Aklecha](https://www.naklecha.com/) | [![](https://camo.githubusercontent.com/a19d31728f4b7cb61c2c9ef52d506c9cf918cffd8be3bb82692b34683bc0489e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e616b6c656368612f6c6c616d61332d66726f6d2d736372617463683f7374796c653d736f6369616c)](https://github.com/naklecha/llama3-from-scratch)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/karpathy/minbpe)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/naklecha), [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/aaaaaaaaaaorg)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/o29P0Kpobz0?t=530) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/naklecha/llama3-from-scratch/blob/main/llama3-from-scratch.ipynb) | 19.05.2024 |
| IC-Light | Manipulate the illumination of images | - [Lvmin Zhang](https://github.com/lllyasviel)<br>- [Anyi Rao](https://anyirao.com/)<br>- [Maneesh Agrawala](https://graphics.stanford.edu/~maneesh/) | [![](https://camo.githubusercontent.com/129796e4cf606ef90704a778d2d6251f05c4ff5b36a7f48a2734503e9e007a6f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6c6c7961737669656c2f49432d4c696768743f7374796c653d736f6369616c)](https://github.com/lllyasviel/IC-Light)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2312.06886), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2402.18848)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/U_ZIkFb9P8w), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3EsJrdXGnpo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BuSsw8Nv1N4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/IC-Light-jupyter/blob/main/IC_Light_jupyter.ipynb) | 09.05.2024 |
| Neural style transfer | This tutorial uses deep learning to compose one image in the style of another image | - [Leon Gatys](https://scholar.google.com/citations?user=ADMVEmsAAAAJ)<br>- [Alexander Ecker](https://eckerlab.org/)<br>- [Matthias Bethge](https://bethgelab.org/) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1508.06576) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb) | 06.05.2024 |
| Autoencoders | This tutorial introduces autoencoders with three examples: the basics, image denoising, and anomaly detection | [Billy Lamberta](https://github.com/lamberta) | - [blog post](https://blog.keras.io/building-autoencoders-in-keras.html)<br>- [book](https://www.deeplearningbook.org/contents/autoencoders.html)<br>- [data](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000)<br>- [examples](https://anomagram.fastforwardlabs.com/#/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/method/autoencoder)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/generative/autoencoder) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb) | 15.04.2024 |
| MagicTime | Metamorphic time-lapse video generation model, which learns real-world physics knowledge from time-lapse videos and implements metamorphic generation | - [Shenghai Yuan](https://shyuanbest.github.io/)<br>- [Jinfa Huang](https://infaaa.github.io/)<br>- [Yujun Shi](https://yujun-shi.github.io/)<br>- [Yongqi Xu](https://cheliosoops.github.io/YongqiXu.io/)<br>others[Ruijie Zhu](https://ruijie-zhu.github.io/)<br>[Bin Lin](https://github.com/LinB203)<br>[Xinhua Cheng](https://cxh0519.github.io/)<br>[Li Yuan](https://yuanli2333.github.io/)<br>[Jiebo Luo](https://www.cs.rochester.edu/u/jluo/) | [![](https://camo.githubusercontent.com/251a080a29d3d015a8146374b16c2d272d24804d3ecdedde2a71e10fa3c6c2ab/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f504b552d5975616e47726f75702f4d6167696354696d653f7374796c653d736f6369616c)](https://github.com/PKU-YuanGroup/MagicTime)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2404.05014), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2406.18522)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PKU-YuanGroup/ChronoMagic-Bench), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kijai/ComfyUI-MagicTimeWrapper), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xuduo35/MakeLongVideo), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Vchitect/LaVie), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Vchitect/Latte)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/BestWishYsh/MagicTime?logs=build), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/BestWishYsh/ChronoMagic), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/cerspense/zeroscope_v2_576w)<br>- [project](https://pku-yuangroup.github.io/MagicTime/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1c1rv7q/magictime_demo_timelapse_video_generation_models/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://x.com/_akhaliq/status/1777538468043792473), [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/vhjf36495872/status/1777525817087553827?s=61&t=r2HzCsU2AnJKbR8yKSprKw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/MagicTime-jupyter/blob/main/MagicTime_jupyter.ipynb) | 14.04.2024 |
| SAGE | Methodology for generative spelling correction, which was tested on English and Russian languages and potentially can be extended to any language with minor changes | - [Nikita Martynov](https://github.com/meduzick)<br>- [Mark Baushenko](https://github.com/e0xextazy)<br>- [Anastasia Kozlova](https://github.com/anastasia-kozlova)<br>- [Katerina Kolomeytseva](https://www.linkedin.com/in/katerina-kolomeytseva-394a7a21a)<br>others[Aleksandr Abramov](https://github.com/Ab1992ao)<br>[Alena Fenogenova](https://github.com/Alenush) | [![](https://camo.githubusercontent.com/0fe2b88a26e1c5149168419963574b186b2e3fbe7f4f5fe75460c94fbefdd01d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61692d666f72657665722f736167653f7374796c653d736f6369616c)](https://github.com/ai-forever/sage)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2308.09435)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ai-forever/augmentex)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/ai-forever/RuM2M100-1.2B), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/ai-forever/FRED-T5-large-spell), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/ai-forever/RuM2M100-418M), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/ai-forever/T5-large-spell), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/ai-forever/spellcheck_benchmark)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Levenshtein_distance)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/yFfkV0Qjuu0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ai-forever/sage/blob/main/notebooks/text_correction_demo.ipynb) | 11.04.2024 |
| Image segmentation | This tutorial focuses on the task of image segmentation, using a modified U-Net | - [Olaf Ronneberger](https://lmb.informatik.uni-freiburg.de/people/ronneber/)<br>- [Philipp Fischer](https://scholar.google.com/citations?user=M2j8KYMAAAAJ)<br>- [Thomas Brox](https://lmb.informatik.uni-freiburg.de/people/brox/index.en.html) | [![](https://camo.githubusercontent.com/d5b16ded3f2751f2e2320d381be7cad1b73e6fdd1982987e4612f4b0bc79d76a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3331392d32343537342d345f3238)](http://doi.org/10.1007/978-3-319-24574-4_28) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1505.04597)<br>- [data](https://www.robots.ox.ac.uk/~vgg/data/pets/)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/c/carvana-image-masking-challenge/overview)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/images/segmentation) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb) | 09.04.2024 |
| Open-Sora Plan | Simple and efficient design along with remarkable performance in text-to-video generation | [YUAN Lab at PKU](https://github.com/PKU-YuanGroup) | [![](https://camo.githubusercontent.com/ac93e2385945f6fba23f17cb16bb74b516b2aa6f079ee8491006c0677b33eb6e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f504b552d5975616e47726f75702f4f70656e2d536f72612d506c616e3f7374796c653d736f6369616c)](https://github.com/PKU-YuanGroup/Open-Sora-Plan)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.15595)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/YtsBNg7n)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PKU-YuanGroup/Open-Sora-Dataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Vchitect/Latte), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/whlzy/FiT)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/LanguageBind/Open-Sora-Plan-v1.1.0), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/LanguageBind/Open-Sora-Plan-v1.0.0)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/cRUz3c7hRs4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/mYnRwR0RyvE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/Open-Sora-Plan-jupyter/blob/main/Open_Sora_Plan_jupyter.ipynb) | 07.04.2024 |
| Gorilla | Finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls | - [Shishir Patil](https://shishirpatil.github.io/)<br>- [Tianjun Zhang](https://github.com/tianjunz)<br>- [Xin Wang](https://xinw.ai/)<br>- [Joseph Gonzalez](https://people.eecs.berkeley.edu/~jegonzal/) | [![](https://camo.githubusercontent.com/ba68b044acc5bbb30ec37cecb30fef586999c396973a4a9cfef213609783dd8c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53686973686972506174696c2f676f72696c6c613f7374796c653d736f6369616c)](https://github.com/ShishirPatil/gorilla) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.15334)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/SwTyuTAxX3)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/gorilla-llm/gorilla-cli)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/latinxinai/try-gorilla-a-large-language-model-connected-with-massive-apis-442f3b554ffb)<br>- [project](http://gorilla.cs.berkeley.edu/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/4EdyWkcddPc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/RMgM3tPTpXI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/CX1Kzijq2TI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8AqQBPI4CFI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/iQwYoii4YiI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/alDArqcxSvw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/EypdTAlmoo4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/LkV5DTRNxAg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP) | 06.04.2024 |
| Cleanlab | Helps you clean data and labels by automatically detecting issues in a ML dataset | - [Curtis Northcutt](https://www.curtisnorthcutt.com/)<br>- [Lu Jiang](http://www.lujiang.info/)<br>- [Isaac Chuang](http://feynman.mit.edu/ike/homepage/index.html) | [![](https://camo.githubusercontent.com/7ae4cf02a42c648ebe42173a87aec73f8f7770e9d67957eda251b577c6936702/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313631332f6a6169722e312e3132313235)](https://doi.org/10.1613/jair.1.12125) [![](https://camo.githubusercontent.com/ee4d029b59a5a015c0022ac44b9cd0b1218dfa5a8199369c5ae4929770c2e641/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636c65616e6c61622f636c65616e6c61623f7374796c653d736f6369616c)](https://github.com/cleanlab/cleanlab) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1911.00068)<br>- [blog post](https://l7.curtisnorthcutt.com/confident-learning)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.cleanlab.ai/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@sujathamudadla1213/cleanlab-python-library-34e0a37720ef)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://cleanlab.ai/slack)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/CleanlabAI)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BnOTv0f9Msk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nGye-lrsLRc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/QHaT_AiUljw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/cleanlab/cleanlab/blob/master/docs/source/tutorials/image.ipynb) | 30.03.2024 |
| AniPortrait | Framework for generating high-quality animation driven by audio and a reference portrait image | - [Zejun Yang](https://github.com/Zejun-Yang)<br>- [Zhisheng Wang](https://scholar.google.com/citations?user=XrK2HNcAAAAJ) | [![](https://camo.githubusercontent.com/f6221390daf8fd0d8c80b2d3b05482b2e827ebf2a9e7542c903b122476a2b6ee/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5a656a756e2d59616e672f416e69506f7274726169743f7374796c653d736f6369616c)](https://github.com/Zejun-Yang/AniPortrait)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2403.17694)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CelebV-HQ/CelebV-HQ), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/HumanAIGC/EMO), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/MooreThreads/Moore-AnimateAnyone), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/magic-research/magic-animate), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/guoqincode/Open-AnimateAnyone)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/ZJYang/AniPortrait), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/runwayml/stable-diffusion-v1-5), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/sd-vae-ft-mse), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/wav2vec2-base-960h)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1bp7rnj/aniportrait_audiodriven_synthesis_of/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/wdRhYLQFQH8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/T-B6xJRG6fQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/AniPortrait-jupyter/blob/main/AniPortrait_vid2vid_jupyter.ipynb) | 27.03.2024 |
| OpenVINO | Open-source toolkit for optimizing and deploying AI inference | [intel](https://www.intel.com/content/www/us/en/developer/topic-technology/open/overview.html) | [![](https://camo.githubusercontent.com/739efdd36838aef02d043f818bec13c5446a9bf61dbaeb47092365a93930072e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e76696e6f746f6f6c6b69742f6f70656e76696e6f3f7374796c653d736f6369616c)](https://github.com/openvinotoolkit/openvino) <br>- [blog post](https://blog.openvino.ai/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/7pVRxUwdWG)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.openvino.ai/)<br>- [forum](https://software.intel.com/en-us/forums/computer-vision)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvinotoolkit/open_model_zoo), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Tencent/TNN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvinotoolkit/openvino_contrib), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvinotoolkit/training_extensions), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvinotoolkit/model_server), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/opencv/cvat), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvinotoolkit/datumaro)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/OpenVINO)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@openvino), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/openvino-toolkit)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/OpenVINO)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLg-UKERBljNxdIQir1wrirZJ50yTp4eHv), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Je8n8M0OwxQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Ru51DELfc-Q), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5X0RmlH6JI4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hhVRSLbpI5Q), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/JH8fsEAIaXo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLWw98q-Xe7iH06qxEW5a22SBsSNsGnYjZ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/openvinotoolkit/openvino_notebooks/blob/main/notebooks/001-hello-world/001-hello-world.ipynb) | 25.03.2024 |
| Gazelle | Joint Speech Language Model | [Tincans](https://tincans.ai/) | [![](https://camo.githubusercontent.com/9ed2b4f8f24427c76d7a9a6562ecee8aaecf043e8f78ebf829eae961dc5aeebf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74696e63616e732d61692f67617a656c6c653f7374796c653d736f6369616c)](https://github.com/tincans-ai/gazelle) <br>- [blog post](https://tincans.ai/slm)<br>- [demo](https://demo.tincans.ai/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/qyC5h3FSzU)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://www.reddit.com/r/LocalLLaMA/comments/1cr84gb/joint_speechlanguage_model_respond_directly_to/)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/tincans-ai/gazelle-v0.1), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/tincans-ai/gazelle-v0.2), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/tincans-ai/gazelle-v0.2-dpo), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/wav2vec2-base-960h), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/meta-llama/Llama-2-7b-chat)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Spike_/(software_development)) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tincans-ai/gazelle/blob/master/examples/infer_quantized.ipynb) | 20.03.2024 |
| Intel® Extension for Transformers | Transformer-based Toolkit to Accelerate GenAI/LLM Everywhere | [intel](https://www.intel.com/content/www/us/en/developer/topic-technology/open/overview.html) | [![](https://camo.githubusercontent.com/f4d4307a94909a9b3903a488d0093587ef669a6591ba6e055a1ea02a1624c28f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e74656c2f696e74656c2d657874656e73696f6e2d666f722d7472616e73666f726d6572733f7374796c653d736f6369616c)](https://github.com/intel/intel-extension-for-transformers)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.17453), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2311.00502), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.07715), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2210.17114), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.05754)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/Wxk3J3ZJkU)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://intel.github.io/intel-extension-for-transformers/latest/docs/Welcome.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ggerganov/ggml), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ggerganov/llama.cpp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TimDettmers/bitsandbytes), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lm-sys/FastChat), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IntelLabs/fastRAG), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IST-DASLab/gptq), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mit-han-lab/streaming-llm)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/blog/assisted-generation), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/Intel/neural-chat-7b-v3-1), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/blog/Andyrasika/neural-chat-intel)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@NeuralCompressor/creating-your-own-llms-on-your-laptop-a08cc4f7c91b), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@NeuralCompressor/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@NeuralCompressor/llm-performance-of-intel-extension-for-transformers-f7d061556176), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@NeuralCompressor/high-performance-low-bit-layer-wise-weight-only-quantization-on-a-laptop-712580899396), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/intel-analytics-software/reduce-large-language-model-carbon-footprint-with-intel-neural-compressor-and-intel-extension-for-dfadec3af76a)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bWhZ1u_1rlc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=RbKRELWP9y8&t=2954s), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7_urstS-noU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bWhZ1u_1rlc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/KWT6yKfu4n0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/intel/intel-extension-for-transformers/blob/main/docs/tutorials/pytorch/text-classification/SetFit_model_compression_AGNews.ipynb) | 19.03.2024 |
| Datasets | A Community Library for Natural Language Processing | - [Quentin Lhoest](https://github.com/lhoestq)<br>- [Albert Villanova](https://albertvillanova.github.io/)<br>- [Yacine Jernite](https://yjernite.github.io/)<br>- [Abhishek Thakur](https://github.com/abhishekkrthakur)<br>others[Patrick von Platen](https://github.com/patrickvonplaten)<br>[Suraj Patil](https://github.com/patil-suraj)<br>[Julien Chaumond](https://github.com/julien-c)<br>[Mariama Dramé](https://scholar.google.com/citations?user=0pwfXH0AAAAJ)<br>[Julien Plu](https://jplu.github.io/)<br>[Lewis Tunstall](https://lewtun.github.io/blog/)<br>[Joe Davison](https://joeddav.github.io/)<br>[Mario Šaško](https://github.com/mariosasko)<br>[Gunjan Chhablani](https://gchhablani.github.io/)<br>[Bhavitvya Malik](https://github.com/bhavitvyamalik)<br>[Simon Brandeis](https://github.com/SBrandeis)<br>[Teven Le Scao](https://github.com/TevenLeScao)<br>[Victor Sanh](https://github.com/VictorSanh)<br>[Canwen Xu](https://www.canwenxu.net/)<br>[Nicolas Patry](https://github.com/Narsil)<br>[Angelina McMillan-Major](https://github.com/mcmillanmajora)<br>[Philipp Schmid](https://www.philschmid.de/)<br>[Sylvain Gugger](https://github.com/sgugger)<br>[Clément Delangue](https://scholar.google.com/citations?user=bRMboT8AAAAJ)<br>[Théo Matussière](https://theo.matussie.re/)<br>[Lysandre Debut](http://lysand.re/)<br>[Stas Bekman](https://stasosphere.com/machine-learning/)<br>[Pierric Cistac](https://github.com/Pierrci)<br>[Thibault Goehringer](https://github.com/beurkinger)<br>[Victor Mustar](https://github.com/gary149)<br>[François Lagunas](https://github.com/madlag)<br>[Alexander Rush](https://rush-nlp.com/)<br>[Thomas Wolf](https://thomwolf.io/) | [![](https://camo.githubusercontent.com/19c7d23996d036fbbde7bdcbd4c2568c26a2b0048fe3347f9e12cce8ce10add7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f64617461736574733f7374796c653d736f6369616c)](https://github.com/huggingface/datasets)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.02846)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://huggingface.co/docs/datasets)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/nbroad/intro-to-hugging-face-datasets)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uaIJ96syPnM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/datasets_doc/en/quickstart.ipynb) | 18.03.2024 |
| Evidently | An open-source framework to evaluate, test and monitor ML models in production | - [Elena Samuylova](https://github.com/elenasamuylova)<br>- [Emeli Dral](https://github.com/emeli-dral)<br>- [Olga Filippova](https://github.com/0lgaF) | [![](https://camo.githubusercontent.com/8c8ae15ce0b535a85b90b788596ce98dd327159e43e66dcaea1fa79770d07832/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f65766964656e746c7961692f65766964656e746c793f7374796c653d736f6369616c)](https://github.com/evidentlyai/evidently) <br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.evidentlyai.com/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/0lgaF/my_tab_with_evidently)<br>- [website](https://evidentlyai.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/c/EvidentlyAI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/L4Pv6ExBQPM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/evidentlyai/evidently/blob/main/examples/sample_notebooks/getting_started_tutorial.ipynb) | 15.03.2024 |
| Instructor | Library that makes it a breeze to work with structured outputs from large language models | [Jason Liu](https://jxnl.co/) | [![](https://camo.githubusercontent.com/e9daea3f9c8e116f5f9d47723f87bea22e2c1dc05af0367ef42ad3ee20281d86/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a786e6c2f696e7374727563746f723f7374796c653d736f6369616c)](https://github.com/jxnl/instructor)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/CV8sPM5k5Y)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://python.useinstructor.com/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/jxnlco)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rDP44EVpHTA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dq1Sjb8IGow), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/higlHgYDc5E) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1iBkrEh2G5U8yh8RmI8EkWxjLq6zIIuVm) | 13.03.2024 |
| MetaVoice | 1.2B parameter base model trained on 100K hours of speech for TTS | [MetaVoice](https://themetavoice.xyz/) | [![](https://camo.githubusercontent.com/0190fb2f9d302c07a5aa18914c9bf423aae57f2153ea537f6db80c5c24fcc56e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d657461766f696365696f2f6d657461766f6963652d7372633f7374796c653d736f6369616c)](https://github.com/metavoiceio/metavoice-src) <br>- [demo](https://ttsdemo.themetavoice.xyz/)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/metavoiceio)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/MetaVoiceAI)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Y_k3bHPcPTo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gVKbf31hrYs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1UmjE1mzfG4td0rCjJEaAWGQXpn_GuwwY) | 26.02.2024 |
| OmegaConf | Hierarchical configuration system, with support for merging configurations from multiple sources providing a consistent API regardless of how the configuration was created | [Omry Yadan](https://github.com/omry) | [![](https://camo.githubusercontent.com/f614c6c15649ccf2247c8d8bbdcf27ae41c695a32ceff99a0a07f99e61051b89/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f6d72792f6f6d656761636f6e663f7374796c653d736f6369616c)](https://github.com/omry/omegaconf) <br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://omegaconf.readthedocs.io/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://majianglin2003.medium.com/python-omegaconf-a33be1b748ab)<br>- [slides](https://docs.google.com/presentation/d/e/2PACX-1vT_UIV7hCnquIbLUm4NnkUpXvPEh33IKiUEvPRF850WKA8opOlZOszjKdZ3tPmf8u7hGNP6HpqS-NT5/pub) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/omry/omegaconf/blob/master/docs/notebook/Tutorial.ipynb) | 15.02.2024 |
| Optuna | An automatic hyperparameter optimization software framework, particularly designed for machine learning | - [Takuya Akiba](https://iwiwi.github.io/)<br>- [Shotaro Sano](https://github.com/g-votte)<br>- [Toshihiko Yanase](https://github.com/toshihikoyanase)<br>- [Takeru Ohta](https://github.com/sile)<br>- [Masanori Koyama](https://scholar.google.com/citations?user=oY1gA10AAAAJ) | [![](https://camo.githubusercontent.com/17d1ab68b4c116c5282f8037bb0f2f74dd5074e2d4651c927b69d8e94257dc76/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f7074756e612f6f7074756e613f7374796c653d736f6369616c)](https://github.com/optuna/optuna) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.10902)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/optuna/optuna)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://optuna.readthedocs.io/en/stable/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/optuna/optuna-dashboard)<br>- [website](https://optuna.org/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/J_aymk4YXhg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/tcrcLRopTX0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-UeC4MR3PHM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/oC8zFYcfYXU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/optuna/optuna-examples/blob/main/quickstart.ipynb) | 15.02.2024 |
| Data augmentation | This tutorial demonstrates data augmentation: a technique to increase the diversity of your training set by applying random transformations such as image rotation | [Billy Lamberta](https://github.com/lamberta) | - [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/data-augmentation)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/tf_flowers), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/images/data_augmentation)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Data_augmentation) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb) | 14.02.2024 |
| Stable Cascade | Text to image model introduces an interesting three-stage approach, setting new benchmarks for quality, flexibility, fine-tuning, and efficiency with a focus on further eliminating hardware barriers | [Stability AI](https://stability.ai/research) | [![](https://camo.githubusercontent.com/fdeb6ed99639ce024b59e18fb73313d0d0186316959d3d1e0a3d1dbbda3b4417/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53746162696c6974792d41492f537461626c65436173636164653f7374796c653d736f6369616c)](https://github.com/Stability-AI/StableCascade) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.00637)<br>- [blog post](https://stability.ai/news/introducing-stable-cascade)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/stablediffusion)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/stable-cascade), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/nateraw/parti-prompts)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/intelligent-art/stable-cascade-a-super-easy-local-installation-guide-ce0cbd06d800), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@yushantripleseven/stable-cascade-training-inference-a52e12ecc5fa)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/stabilityai)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Ybu6qTbEsewc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/JuX-uukwdkI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YMxXtaiVHks), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/UgM-z2q3Xe0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/W6YLIyA3Kco), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/X1rLWFRagIw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mkshing/notebooks/blob/main/stable_cascade.ipynb) | 14.02.2024 |
| CleanVision | Automatically detects potential issues in image datasets like images that are: blurry, under/over-exposed, (near) duplicates, etc | [cleanlab](https://cleanlab.ai/about/) | [![](https://camo.githubusercontent.com/8fc1d70a379a2c788b96c305d1fee16f3d2f2f1bf127f216226a35dfe2ede753/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636c65616e6c61622f636c65616e766973696f6e3f7374796c653d736f6369616c)](https://github.com/cleanlab/cleanvision) <br>- [blog post](https://cleanlab.ai/blog/cleanvision/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://cleanvision.readthedocs.io/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cleanlab/cleanvision-examples)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://cleanlab.ai/slack)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/CleanlabAI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/cleanlab/cleanvision/blob/main/docs/source/tutorials/tutorial.ipynb) | 13.02.2024 |
| DynamiCrafter | Animating Open-domain Images with Video Diffusion Priors | - [Jinbo Xing](https://doubiiu.github.io/)<br>- [Menghan Xia](https://menghanxia.github.io/)<br>- [Yong Zhang](https://yzhang2016.github.io/)<br>- [Haoxin Chen](https://scutpaul.github.io/)<br>others[Wangbo Yu](https://github.com/GooDrYu)<br>[Hanyuan Liu](https://github.com/hyliu)<br>[Xintao Wang](https://xinntao.github.io/)<br>[Tien-Tsin Wong](https://ttwong12.github.io/myself.html)<br>[Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ) | [![](https://camo.githubusercontent.com/4bca6ac369d3121a7c341b108b020e49d35cf5dcbfe31041a6c4dfca2f3e5ae1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f446f75626969752f44796e616d69437261667465723f7374796c653d736f6369616c)](https://github.com/Doubiiu/DynamiCrafter)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2310.12190)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/chaojie/ComfyUI-DynamiCrafter), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AILab-CVC/VideoCrafter), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/YingqingHe/ScaleCrafter), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AILab-CVC/TaleCrafter), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AILab-CVC/FreeNoise)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/Doubiiu/DynamiCrafter_1024)<br>- [project](https://doubiiu.github.io/projects/DynamiCrafter/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1aj7gcw/dynamicrafter_gets_updated/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://x.com/noguchis/status/1754488826016432341?s=20)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0NfmIsNAg-g), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PtW7hjCawbo) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/DynamiCrafter-colab/blob/main/DynamiCrafter_colab_576_1024.ipynb) | 12.02.2024 |
| XLA | Accelerated Linear Algebra is an open-source machine learning compiler for GPUs, CPUs, and ML accelerators | [OpenXLA](https://openxla.org/) | [![](https://camo.githubusercontent.com/f10bce78394b4ed1354ea0e5ae22841b2703188d5d218b129a0e2daf65284627/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e786c612f786c613f7374796c653d736f6369616c)](https://github.com/openxla/xla)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@muhammedashraf2661/demystifying-xla-unlocking-the-power-of-accelerated-linear-algebra-9b62f8180dbd), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://runaker.medium.com/one-code-to-rule-them-all-simplifying-ai-development-with-hardware-agnostic-abstraction-layers-a61448bb6d22)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/xla)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/xla)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLlFotmaRrOzs23kqlSF-r8v1dJHz5GxZs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLlFotmaRrOzu8TQsTahDo_Cn7QdntFlUL), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLlFotmaRrOzt8xOwckcXL7vObZmr8PK1y), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/QNSxFXJ-xMM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/openxla/xla/blob/main/docs/tf2xla/tutorials/jit_compile.ipynb) | 02.02.2024 |
| Composer | PyTorch library that enables you to train neural networks faster, at lower cost, and to higher accuracy | [The Mosaic ML Team](https://www.mosaicml.com/team) | [![](https://camo.githubusercontent.com/bec60aa28c7679241561bce51616f925f542a505789b0cdb2921d6c471fbb597/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6f736169636d6c2f636f6d706f7365723f7374796c653d736f6369616c)](https://github.com/mosaicml/composer) <br>- [app](https://app.mosaicml.com/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.05924), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2002.04688)<br>- [blog post](https://www.mosaicml.com/blog/5-best-practices-for-efficient-model-training)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](http://docs.mosaicml.com/)<br>- [![slack](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/slack.svg)](https://join.slack.com/t/mosaicml-community/shared_invite/zt-w0tiddn9-WGTlRpfjcO9J5jyrMub1dg)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/mosaicml)<br>- [website](https://www.mosaicml.com/composer)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Amdahl's_law)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/@mosaicml6047/videos), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/n-1WV5QdIDc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Xi_5wq2MpOw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mosaicml/composer/blob/dev/examples/getting_started.ipynb) | 01.02.2024 |
| CycleGAN | This notebook demonstrates unpaired image to image translation using conditional GAN's | - [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)<br>- [Taesung Park](https://taesung.me/)<br>- [Phillip Isola](https://web.mit.edu/phillipi/)<br>- [Alexei Efros](https://people.eecs.berkeley.edu/~efros/) | [![](https://camo.githubusercontent.com/6304a1266960c58592222e3dc858952bad8977d6044b0e693e6234d4af1dc6b3/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343562e323031372e323434)](https://doi.org/10.1109/ICCV.2017.244)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1703.10593)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/cycle_gan), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/generative/cyclegan) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb) | 17.01.2024 |
| Integrated gradients | This tutorial demonstrates how to implement Integrated Gradients, an Explainable AI technique | - [Mukund Sundararajan](https://scholar.google.com/citations?user=q39nzokAAAAJ)<br>- [Ankur Taly](https://theory.stanford.edu/~ataly/)<br>- [Qiqi Yan](https://scholar.google.com/citations?user=Wn8xr_gAAAAJ) | [![](https://camo.githubusercontent.com/814d4aa92f2474efe88bb11d3f65ccd3f3dac148c3d707cc3a47734f2ce96d44/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616e6b757274616c792f496e74656772617465642d4772616469656e74733f7374796c653d736f6369616c)](https://github.com/ankurtaly/Integrated-Gradients)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1703.01365)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/integrated_gradients)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/codex/explainable-ai-integrated-gradients-for-deep-neural-network-predictions-eb4f96248afb), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients)<br>- [visualizing](https://distill.pub/2020/attribution-baselines/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Linear_interpolation), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Riemann_sum) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/interpretability/integrated_gradients.ipynb) | 17.01.2024 |
| MAGNeT | Masked generative sequence modeling method that operates directly over several streams of audio tokens | - [Alon Ziv](https://www.cs.huji.ac.il/w~alonzi/)<br>- [Itai Gat](https://itaigat.com/)<br>- [Gaël Le Lan](https://github.com/gl3lan)<br>- [Tal Remez](https://talremez.github.io/)<br>others[Felix Kreuk](https://felixkreuk.github.io/)<br>[Alexandre Défossez](https://ai.honu.io/)<br>[Jade Copet](https://scholar.google.com/citations?&user=GRMLwjAAAAAJ)<br>[Gabriel Synnaeve](https://syhw.github.io/)<br>[Yossi Adi](https://www.cs.huji.ac.il/~adiyoss/) | [![](https://camo.githubusercontent.com/f6de7f8a325909c6dc9eb90a8d967ad99338a33a9038ced257526bee33a7cca0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f617564696f63726166743f7374796c653d736f6369616c)](https://github.com/facebookresearch/audiocraft/blob/main/docs/MAGNET.md)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2401.04577), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.09636), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2307.04686)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/FurkanGozukara/Stable-Diffusion/blob/main/Tutorials/AI-Music-Generation-Audiocraft-Tutorial.md#more-info-about-top-k-top-p-temperature-and-classifier-free-guidance-from-chatgpt)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/magnet-medium-10secs), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/magnet-medium-30secs), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/audio-magnet-medium)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://generativeai.pub/metas-ai-magnet-the-next-big-thing-in-text-to-audio-technology-7d524d9459ef)<br>- [project](https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/ArtificialInteligence/comments/19808gf/magnet_masked_audio_generation_using_a_single/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/MAGNeT-colab/blob/main/MAGNET_colab.ipynb) | 16.01.2024 |
| AutoFaiss | Automatically create Faiss knn indices with the most optimal similarity search parameters | [Ctiteo](https://github.com/criteo) | [![](https://camo.githubusercontent.com/5cd5a29de96e0bba6a64b2e35e13eee95fabbce94984e8fe330a2398779a2b1a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f63726974656f2f6175746f66616973733f7374796c653d736f6369616c)](https://github.com/criteo/autofaiss)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://criteo.github.io/autofaiss/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/faiss)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/criteo-engineering/introducing-autofaiss-an-automatic-k-nearest-neighbor-indexing-library-at-scale-c90842005a11)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.python.org/pypi/autofaiss) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/criteo/autofaiss/blob/master/docs/notebooks/autofaiss_getting_started.ipynb) | 12.01.2024 |
| Retrieval based Voice Conversion WebUI | An easy-to-use Voice Conversion framework based on VITS | [RVC-Project](https://github.com/RVC-Project) | [![](https://camo.githubusercontent.com/9391736a167f4c0ff0e102901d310a8fb164ce8da857d41b4ac610d1af14a854/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5256432d50726f6a6563742f52657472696576616c2d62617365642d566f6963652d436f6e76657273696f6e2d57656255493f7374796c653d736f6369616c)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/HcsmBBGyVk)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/auspicious3000/contentvec), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jik876/hifi-gan), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/FFmpeg/FFmpeg), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Anjok07/ultimatevocalremovergui), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvpi/audio-slicer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Dream-High/RMVPE)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/lj1995/VoiceConversionWebUI)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@ja.harr91/decoding-the-sound-of-virality-a-deep-dive-into-adversarial-ai-for-voice-conversion-tasks-on-m1-d60d32cfb2d4)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-JcvdDErkAU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/9TroP5mR3CM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Y8IxVVQBEpc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qZ12-Vm2ryc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5i_Pyw0gH-M) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/Retrieval_based_Voice_Conversion_WebUI.ipynb) | 11.01.2024 |
| Big Vision | This codebase is designed for training large-scale vision models using Cloud TPU VMs or GPU machines | - [Lucas Beyer](http://lucasb.eyer.be/)<br>- [Xiaohua Zhai](https://github.com/xiaohuazhai)<br>- [Alexander Kolesnikov](https://github.com/akolesnikoff) | [![](https://camo.githubusercontent.com/9303aa318442d4bb8c245b5b5c29daedc11af332ce4fb2b54e8b90089168af27/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f6269675f766973696f6e3f7374796c653d736f6369616c)](https://github.com/google-research/big_vision)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.11929), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.04560), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.01601), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.01580), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.08013), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.13035), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.17376), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.07915), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.16999), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2302.08242), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.07159)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/data), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/lit.ipynb) | 03.01.2024 |
| Open Interpreter | An open-source, locally running implementation of OpenAI's Code Interpreter | [Killian Lucas](https://github.com/KillianLucas) | [![](https://camo.githubusercontent.com/592340d8095bb1bf9d191fce244c639a462c686b89b9d02a42bb8417fd583e09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b696c6c69616e4c756361732f6f70656e2d696e7465727072657465723f7374796c653d736f6369616c)](https://github.com/KillianLucas/open-interpreter) <br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/6p3fD6rBVm)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.openinterpreter.com/)<br>- [website](https://openinterpreter.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/SqnXUHwIa3c), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/s-f4lCETxu0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/J-H2un1Adr0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/jaijpff58vw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7KFbG_3dKKs), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/4OhuFjPyZNQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/01tQLn_RRcE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uyfoHQVgeY0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1WKmRXZgsErej2xUriKzxrEAXdxMSgWbb) | 03.01.2024 |
| Seamless Communication | Family of AI models that enable more natural and authentic communication across languages | - [Loïc Barrault](https://loicbarrault.github.io/)<br>- [Yu-An Chung](https://iamyuanchung.github.io/)<br>- [Mariano Coria](https://www.linkedin.com/in/marianocoria)<br>- [David Dale](https://daviddale.ru/)<br>others[Ning Dong](https://scholar.google.com/citations?user=gg1hvjoAAAAJ)<br>[Mark Duppenthaler](https://github.com/mduppes)<br>[Paul-Ambroise Duquenne](https://scholar.google.com/citations?user=Uah8IcAAAAAJ)<br>[Hady Elsahar](https://www.hadyelsahar.io/)<br>[Min-Jae Hwang](https://mjhwang93.github.io/)<br>[Hirofumi Inaguma](https://hirofumi0810.github.io/)<br>[Ilia Kulikov](https://github.com/uralik)<br>[Pengwei Li](https://scholar.google.com/citations?user=hQB3YsYAAAAJ)<br>[Daniel Licht](https://github.com/Lichtphyz)<br>[Jean Maillard](https://scholar.google.com/citations?user=_ewOoK0AAAAJ)<br>[Ruslan Mavlyutov](https://github.com/mavlyutovr)<br>[Kaushik Ram Sadagopan](https://github.com/kauterry)<br>[Abinesh Ramakrishnan](https://github.com/ibanesh)<br>[Tuan Tran](https://antoine-tran.github.io/)<br>[Guillaume Wenzek](https://github.com/gwenzek)<br>[Yilin Yang](https://yilinyang7.github.io/)<br>[Ethan Ye](https://github.com/yeyinthtoon)<br>[Ivan Evtimov](https://ivanevtimov.eu/)<br>[Pierre Fernandez](https://pierrefdz.github.io/)<br>[Robin San Roman](https://scholar.google.com/citations?user=AJ3ir84AAAAJ)<br>[Bokai Yu](https://scholar.google.com/citations?user=7jNmPwUAAAAJ)<br>[Pierre Andrews](https://github.com/Mortimerp9)<br>[Can Balioglu](http://canbalioglu.com/)<br>[Peng-Jen Chen](https://scholar.google.com/citations?user=rOXs9VMAAAAJ)<br>[Marta Costa-jussà](https://costa-jussa.com/)<br>[Maha Elbayad](http://elbayadm.github.io/)<br>[Hongyu Gong](https://github.com/hygong-fb)<br>[Francisco Guzmán](https://guzmanhe.github.io/)<br>[Kevin Heffernan](https://github.com/heffernankevin)<br>[Somya Jain](https://scholar.google.com/citations?user=AmBxU3kAAAAJ)<br>[Justine Kao](https://scholar.google.com/citations?user=Y9BLeTAAAAAJ)<br>[Ann Lee](https://www.stat.cmu.edu/~annlee/)<br>[Xutai Ma](https://github.com/xutaima)<br>[Benjamin Peloquin](https://scholar.google.com/citations?user=5GNAjB8AAAAJ)<br>[Juan Pino](https://scholar.google.com/citations?user=weU_-4IAAAAJ)<br>[Sravya Popuri](https://scholar.google.com/citations?user=MtmqG3UAAAAJ)<br>[Holger Schwenk](https://github.com/hoschwenk)<br>[Anna Sun](https://github.com/annasun28)<br>[Paden Tomasello](https://scholar.google.com/citations?user=sBtWMGYAAAAJ)<br>[Changhan Wang](https://www.changhan.me/)<br>[Skyler Wang](https://www.skylerwang.com/)<br>[Mary Williamson](https://scholar.google.com/citations?user=Ys4xB-QAAAAJ) | [![](https://camo.githubusercontent.com/560e28bbd9fc2af565f44d3b50fc2b76555b823d80ac2798b4f3a125fe64e6e2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7365616d6c6573735f636f6d6d756e69636174696f6e3f7374796c653d736f6369616c)](https://github.com/facebookresearch/seamless_communication) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2312.05187)<br>- [blog post](https://ai.meta.com/research/seamless-communication/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/libsndfile/libsndfile), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/fairseq2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/SimulEval), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/stopes), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/SONAR)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/seamless-m4t-v2-large), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/seamless-expressive), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/seamless-streaming)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://ngwaifoong92.medium.com/beginners-guide-to-seamlessm4t-81efad6e8ca6)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=0padjtkHXTE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rNN7qsoCKBo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/RKEFZ44YOcc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/seamless_communication/blob/main/Seamless_Tutorial.ipynb) | 14.12.2023 |
| colab2pdf | Convert your Colab notebook to a PDF | [Drengskapur](https://github.com/drengskapur) | [![](https://camo.githubusercontent.com/e306d77caaa9e35a88a941263d140c8deaf072c29325010ed2b31ae78dd14e7d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6472656e67736b617075722f636f6c6162327064663f7374796c653d736f6369616c)](https://github.com/drengskapur/colab2pdf) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1zqrIYC0iQ_CZkRqGXgZggrwjtt_4BmpL) | 11.12.2023 |
| Sentence Transformers | Multilingual Sentence, Paragraph, and Image Embeddings using BERT & Co | - [Nils Reimers](https://www.nils-reimers.de/)<br>- [Iryna Gurevych](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp) | [![](https://camo.githubusercontent.com/552d5fbfe743a3153204169575d8724dd0e400f3e75c1a9f919b28114d97351c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f554b504c61622f73656e74656e63652d7472616e73666f726d6572733f7374796c653d736f6369616c)](https://github.com/UKPLab/sentence-transformers)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1908.10084), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.09813), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.08240)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://www.sbert.net/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb) | 07.12.2023 |
| CleanRL | Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features | - [Shengyi Huang](https://costa.sh/)<br>- [Rousslan Dossa](https://dosssman.github.io/)<br>- [Chang Ye](https://github.com/yooceii)<br>- [Jeff Braga](https://github.com/bragajj)<br>others[Dipam Chakraborty](https://github.com/dipamc)<br>[Kinal Mehta](https://kinalmehta.github.io/)<br>[João Araújo](https://github.com/joaogui1) | [![](https://camo.githubusercontent.com/bdadff68899237d24a90abe862960b5c7e3b00938c1adc665216e242eff2bf04/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f767778797a6a6e2f636c65616e726c3f7374796c653d736f6369616c)](https://github.com/vwxyzjn/cleanrl)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1707.06347), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1707.06887), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1812.05905), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1509.02971), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1802.09477), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2009.04416), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.12894)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.cleanrl.dev/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tinkoff-ai/CORL), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Farama-Foundation/Gymnasium), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/baselines), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ikostrikov/jaxrl)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/cleanrl)<br>- [paper](https://www.jmlr.org/papers/v23/21-1342.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/channel/UCDdC6BIFRI0jvcwuhi3aI6w), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dm4HdGujpPs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/vwxyzjn/cleanrl/blob/master/docs/get-started/CleanRL_Huggingface_Integration_Demo.ipynb) | 28.11.2023 |
| Vocos | Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis | [Hubert Siuzdak](https://github.com/hubertsiuzdak) | [![](https://camo.githubusercontent.com/584a2c51c3634cfb6ec3e925d614a56e33badab08f63527ed61eae9fafc86306/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67656d656c6f2d61692f766f636f733f7374796c653d736f6369616c)](https://github.com/gemelo-ai/vocos) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.00814)<br>- [project](https://gemelo-ai.github.io/vocos/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/charactr-platform/vocos/blob/main/notebooks/Bark%2BVocos.ipynb) | 21.11.2023 |
| X—LLM | Easy LLM Finetuning using the most advanced methods | [Boris Zubarev](https://github.com/BobaZooba) | [![](https://camo.githubusercontent.com/ced2b058ba263c3b9179aceb6c90e4331844b3183e00fd5b81d6c95ad7712a41/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f426f62615a6f6f62612f786c6c6d3f7374796c653d736f6369616c)](https://github.com/BobaZooba/xllm)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.18290)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/5znbxBgwZP)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BobaZooba/xllm-demo), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BobaZooba/wgpt), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BobaZooba/shurale)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/TachyHealth), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/BobaZooba/Shurale7b-v1)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/xllm/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1zsNmJFns1PKZy5VE5p5nsQL-mZF7SwHf?usp=sharing) | 15.11.2023 |
| Distil-Whisper | Maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio | - [Sanchit Gandhi](https://github.com/sanchit-gandhi)<br>- [Patrick von Platen](https://github.com/patrickvonplaten)<br>- [Alexander Rush](https://scholar.google.com/citations?&user=LIjnUGgAAAAJ) | [![](https://camo.githubusercontent.com/6eac1f3d471cfbcf08ecbdefda0ddd4b45aa2b49c8ed1779c342fca5eecdc9fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f64697374696c2d776869737065723f7374796c653d736f6369616c)](https://github.com/huggingface/distil-whisper)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2311.00430), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.17192)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/huggingface/safetensors), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Dao-AILab/flash-attention)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/collections/distil-whisper/training-datasets-6538d05c69721489d1db1e49), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSpeechSeq2Seq), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoProcessor), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/v4.34.1/en/model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate.assistant_model), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#bettertransformer)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/prompt-engineering/transcribing-audio-with-python-and-distil-whisper-9b4fec3d53bf)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/17vqtcb/p_distilwhisper_a_distilled_variant_of_whisper/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/46Q6fbdUCbg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/SZtHEKyvuug), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/kI1pA1CADxM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/Distil_Whisper_Benchmark.ipynb) | 08.11.2023 |
| AnimateDiff | Practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning | - [Yuwei Guo](https://github.com/guoyww)<br>- [Ceyuan Yang](https://github.com/limbo0000)<br>- [Anyi Rao](https://anyirao.com/)<br>- [Yaohui Wang](https://wyhsirius.github.io/)<br>others[Yu Qiao](https://mmlab.siat.ac.cn/yuqiao/)<br>[Dahua Lin](http://dahua.site/)<br>[Bo Dai](https://daibo.info/) | [![](https://camo.githubusercontent.com/9ef5b4a3249101bb75425acaadebd6697dc5ca389aadf66194e1d4cb1335d44a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67756f7977772f616e696d617465646966663f7374796c653d736f6369616c)](https://github.com/guoyww/animatediff/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2307.04725)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/continue-revolution/sd-webui-animatediff), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/talesofai/AnimateDiff), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://youtu.be/-wki7IrQ_sU)<br>- [project](https://animatediff.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rdnOhM8L8nE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/LcHAZaJjA5k), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/66JgpI3a650?feature=share) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/AnimateDiff-colab/blob/main/AnimateDiff_colab.ipynb) | 30.10.2023 |
| Intel® Neural Compressor | Aims to provide popular model compression techniques such as quantization, pruning (sparsity), distillation, and neural architecture search on mainstream frameworks such as TensorFlow, PyTorch, ONNX Runtime, and MXNet, as well as Intel extensions such as Intel Extension for TensorFlow and Intel Extension for PyTorch | [intel](https://www.intel.com/content/www/us/en/developer/topic-technology/open/overview.html) | [![](https://camo.githubusercontent.com/3fb902c55b19227472a6bea87aedcbf782ae759ba0684aa4a3481e5578617df5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e74656c2f6e657572616c2d636f6d70726573736f723f7374796c653d736f6369616c)](https://github.com/intel/neural-compressor)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.14592), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2309.05516), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.07715)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.com/invite/Wxk3J3ZJkU)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://github.com/intel/neural-compressor)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/intel/intel-extension-for-tensorflow), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/intel/intel-extension-for-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Lightning-AI/pytorch-lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/pytorch/pytorch-inference-acceleration-with-intel-neural-compressor-842ef4210d7d), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/intel-analytics-software/efficient-text-classification-with-intel-neural-compressor-4853296deeac)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://neurips.cc/virtual/2022/59433)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/tutorials/recipes/intel_neural_compressor_for_pytorch.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/SswQbIHUrvQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5xHKe4wWLes), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/H7Gg-EmGpAI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ie3w_j0Ntsk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/m2LokuUdeVg), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/38wrDHEQZuM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/intel/neural-compressor/blob/master/examples/notebook/onnxruntime/Quick_Started_Notebook_of_INC_for_ONNXRuntime.ipynb) | 27.10.2023 |
| Bark | Transformer-based text-to-audio model | [suno](https://www.suno.ai/) | [![](https://camo.githubusercontent.com/e2a355eb86386917f553ff0fcb7328f3a3447c00dc28b180fbf86716de1ed3e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73756e6f2d61692f6261726b3f7374796c653d736f6369616c)](https://github.com/suno-ai/bark)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.03143), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.02111)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/J2B2vsjKuE)<br>- [examples](https://suno-ai.notion.site/Bark-Examples-5edae8b02a604b54a42244ba45ebc2e2)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/encodec), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/karpathy/nanoGPT)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables#hfhome)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/OnusFM)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/84LzaXAo6vE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rU5Do9yHbwM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/w41-MUfxIWo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_m-MxEpHUQY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1dWWkZzvu7L9Bunq9zvD-W02RFUXoW-Pd) | 25.10.2023 |
| Mistral Transformer | The most powerful language model for its size to date | - [Albert Jiang](https://albertqjiang.github.io/)<br>- [Alexandre Sablayrolles](https://github.com/alexandresablayrolles)<br>- [Arthur Mensch](https://github.com/arthurmensch)<br>- [Chris Bamford](https://griddly.ai/)<br>others[Devendra Chaplot](https://devendrachaplot.github.io/)<br>[Diego Casas](https://github.com/diegolascasas)<br>[Florian Bressand](https://www.linkedin.com/in/florianbressand)<br>[Gianna Lengyel](https://www.linkedin.com/in/gianna-maria-lengyel)<br>[Guillaume Lample](https://github.com/glample)<br>[Lucile Saulnier](https://scholar.google.com/citations?user=Baj_9IsAAAAJ)<br>[Lélio Renard Lavaud](https://github.com/lerela)<br>[Marie-Anne Lachaux](https://scholar.google.com/citations?user=dSEMIJ8AAAAJ)<br>[Pierre Stock](https://github.com/pierrestock)<br>[Teven Scao](https://scholar.google.com/citations?user=ik0_vxsAAAAJ)<br>[Thibaut Lavril](https://scholar.google.com/citations?user=9nPunCEAAAAJ)<br>[Thomas Wang](https://github.com/thomasw21)<br>[Timothée Lacroix](https://scholar.google.com/citations?&user=tZGS6dIAAAAJ)<br>[William Sayed](https://www.linkedin.com/in/william-el-sayed-48672312a) | [![](https://camo.githubusercontent.com/a172c13b3599004db8dc6d41c84a07ddd22f251f4fc0084bbdd70994ab39b25b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d69737472616c61692f6d69737472616c2d7372633f7374796c653d736f6369616c)](https://github.com/mistralai/mistral-src)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2310.06825), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1904.10509), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.05150), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.05685)<br>- [blog post](https://mistral.ai/news/announcing-mistral-7b/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.com/invite/mistralai)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.mistral.ai/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vllm-project/vllm), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lm-sys/FastChat), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ggerganov/ggml), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Dao-AILab/flash-attention), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/skypilot-org/skypilot)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/mistralai)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/mistral-7b-recipes-for-fine-tuning-and-quantization-on-your-computer-631401583f77)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/g7kVVBlCGo0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ASpageg8nPw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/OMIuP6lQXe4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/jnPZApwtE4I), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3SdopNwQJ-c) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/camenduru/Mistral-colab/blob/main/Mistral_colab.ipynb) | 09.10.2023 |
| Fooocus | Image generating software | [Lvmin Zhang](https://lllyasviel.github.io/Style2PaintsResearch/lvmin) | [![](https://camo.githubusercontent.com/7d2e634a8152ba3e72a9ef46a35eb94fcf516a01b64d67b47543148a6e1bf067/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6c6c7961737669656c2f466f6f6f6375733f7374796c653d736f6369616c)](https://github.com/lllyasviel/Fooocus)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2210.00939)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8krykSwOz3E), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/558W8rfnP-Q), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/TJkrzuPdmvE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/NfNwmKM3sxc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/colab.ipynb) | 03.10.2023 |
| Actor-Critic | This tutorial demonstrates how to implement the Actor-Critic method using TensorFlow to train an agent on the Open AI Gym CartPole-V0 environment | - [Vijay Konda](https://scholar.google.com/citations?user=bi-WXQIAAAAJ)<br>- [John Tsitsiklis](https://web.mit.edu/jnt/www/home.html) | - [gym](https://gym.openai.com/)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/1786-actor-critic-algorithms), [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Temporal_difference_learning) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb) | 28.09.2023 |
| MMagic | AIGC toolbox for professional AI researchers and machine learning engineers to explore image and video processing, editing and generation | [OpenMMLab](https://openmmlab.com/) | [![](https://camo.githubusercontent.com/c6de049b85e1c68e2bfd28ba476886fa8919d92ef3b9e47843de4566ba91c0b8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e2d6d6d6c61622f6d6d616769633f7374796c653d736f6369616c)](https://github.com/open-mmlab/mmagic)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/raweFPmdzG)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mmagic.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmgeneration), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmengine/blob/main/mmengine/model/wrappers/seperate_distributed.py), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmcv), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mim)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://openmmlab.medium.com/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/OpenMMLab)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/openmmlab) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/open-mmlab/mmagic/blob/main/demo/mmagic_inference_tutorial.ipynb) | 11.09.2023 |
| SeqIO | Library for processing sequential data to be fed into downstream sequence models | - [Adam Roberts](https://github.com/adarob)<br>- [Hyung Won Chung](https://github.com/hwchung27)<br>- [Anselm Levskaya](https://anselmlevskaya.com/)<br>- [Gaurav Mishra](https://github.com/gauravmishra)<br>others[James Bradbury](https://github.com/jekbradbury)<br>[Daniel Andor](https://github.com/andorardo)<br>[Sharan Narang](https://github.com/sharannarang)<br>[Brian Lester](https://blester125.com/)<br>[Colin Gaffney](https://github.com/cpgaffney1)<br>[Afroz Mohiuddin](https://github.com/afrozenator)<br>[Curtis Hawthorne](https://github.com/cghawthorne)<br>[Aitor Lewkowycz](https://scholar.google.com/citations?user=Yum1ah0AAAAJ)<br>[Alex Salcianu](https://scholar.google.com/citations?user=HSrT1wsAAAAJ)<br>[Marc van Zee](https://github.com/marcvanzee)<br>[Jacob Austin](https://jacobaustin123.github.io/)<br>[Sebastian Goodman](https://github.com/0x0539)<br>[Livio Baldini Soares](https://liviosoares.github.io/)<br>[Haitang Hu](https://hthu.github.io/)<br>[Sasha Tsvyashchenko](https://endl.ch/)<br>[Aakanksha Chowdhery](http://www.achowdhery.com/)<br>[Jasmijn Bastings](https://jasmijn.ninja/)<br>[Jannis Bulian](http://bulian.org/)<br>[Xavier Garcia](https://scholar.google.com/citations?user=Y2Hio6MAAAAJ)<br>[Jianmo Ni](https://nijianmo.github.io/)<br>[Andrew Chen](https://github.com/andrewluchen)<br>[Kathleen Kenealy](https://github.com/kkenealy)<br>[Jonathan Clark](http://www.cs.cmu.edu/~jhclark/)<br>[Stephan Lee](https://github.com/stephanwlee)<br>[Dan Garrette](https://www.dhgarrette.com/)<br>[James Lee-Thorp](https://scholar.google.com/citations?user=qsPv098AAAAJ)<br>[Colin Raffel](https://www.colinraffel.com/)<br>[Noam Shazeer](https://github.com/nshazeer)<br>[Marvin Ritter](https://github.com/Marvin182)<br>[Maarten Bosma](https://scholar.google.com/citations?user=wkeFQPgAAAAJ)<br>[Alexandre Passos](https://www.ic.unicamp.br/~tachard/)<br>[Jeremy Maitin-Shepard](https://research.google/people/JeremyMaitinShepard/)<br>[Noah Fiedel](https://scholar.google.com/citations?user=XWpV9DsAAAAJ)<br>[Mark Omernick](https://github.com/markomernick)<br>[Brennan Saeta](https://github.com/saeta)<br>[Ryan Sepassi](https://ryansepassi.com/)<br>[Alexander Spiridonov](https://research.google/people/AlexanderSpiridonov/)<br>[Joshua Newlan](https://github.com/joshnewlan)<br>[Andrea Gesmundo](https://github.com/agesmundo) | [![](https://camo.githubusercontent.com/d1964528604fc1d0e97ccc94d773f28e71877cf1eadc7f7aab253b359b440074/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f736571696f3f7374796c653d736f6369616c)](https://github.com/google/seqio)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.17189), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.10683), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.04805), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2002.08910)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://seqio.readthedocs.io/en/latest/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/load_data/tfrecord), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/function#autograph_transformations), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/py_function), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/random_numbers#stateless_rngs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/seqio/blob/main/seqio/notebooks/Evaluation.ipynb) | 08.09.2023 |
| MMAction2 | An open-source toolbox for video understanding based on PyTorch | [MMAction2 Contributors](https://openmmlab.com/aboutus) | [![](https://camo.githubusercontent.com/0d436fadccd9b5419367b027081cdcc7086cdf2dd4274afa2061f26ce8c5bc91/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e2d6d6d6c61622f6d6d616374696f6e323f7374796c653d736f6369616c)](https://github.com/open-mmlab/mmaction2)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.13230), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2107.10161), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.17263), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.13586), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.05095), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.13042)<br>- [data](https://sdolivia.github.io/FineGym/), [data](http://www.svcl.ucsd.edu/projects/resound/dataset.html), [data](https://research.google.com/ava/index.html), [data](https://www.deepmind.com/open-source/kinetics)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mmaction2.readthedocs.io/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmcv), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/SwinTransformer/Video-Swin-Transformer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Cogito2012/DEAR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xvjiarui/VFS), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/holistic-video-understanding/HVU-Dataset) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb) | 06.09.2023 |
| Ray | Unified framework for scaling AI and Python applications | - [Philipp Moritz](https://github.com/pcmoritz)<br>- [Robert Nishihara](https://github.com/robertnishihara)<br>- [Stephanie Wang](https://stephanie-wang.github.io/)<br>- [Alexey Tumanov](https://faculty.cc.gatech.edu/~atumanov/)<br>others[Richard Liaw](https://github.com/richardliaw)<br>[Eric Liang](https://github.com/ericl)<br>[Melih Elibol](https://research.nvidia.com/person/melih-elibol)<br>[Zongheng Yang](https://zongheng.me/)<br>[William Paul](https://github.com/Wapaul1)<br>[Michael Jordan](https://people.eecs.berkeley.edu/~jordan/)<br>[Ion Stoica](https://people.eecs.berkeley.edu/~istoica/) | [![](https://camo.githubusercontent.com/279f21209daffaf2ca2523725b48849a6ad98e437b8c27c9808c1512df143efd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7261792d70726f6a6563742f7261793f7374796c653d736f6369616c)](https://github.com/ray-project/ray)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1712.05889), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.05072), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1712.09381), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1807.05118), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1703.03924)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.ray.io/en/latest/index.html)<br>- [website](https://www.ray.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/LmROEotKhJA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/uzt-CwohQC8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/XME90SGL6Vs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ray-project/ray/blob/master/doc/source/tune/examples/optuna_example.ipynb) | 06.09.2023 |
| Home Robot | Low-level API for controlling various home robots | [Chris Paxton](https://cpaxton.github.io/) | [![](https://camo.githubusercontent.com/03eb3aaddc8d2334c267cb812234322c76ec3f4dfaf134dd7327bada48f65452/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f686f6d652d726f626f743f7374796c653d736f6369616c)](https://github.com/facebookresearch/home-robot)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cpaxton/contact_graspnet/tree/cpaxton/devel), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/fairo), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hello-robot/stretch_body), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hello-robot/stretch_firmware), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hello-robot/stretch_ros), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hello-robot/stretch_ros2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hello-robot/stretch_web_interface), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/RoboStack/ros-noetic), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/codekansas/stretch-robot) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/home-robot/blob/master/src/home_robot_sim/notebooks/velocity_control_sim.ipynb) | 30.08.2023 |
| Neural Tangents | Library designed to enable research into infinite-width neural networks | - [Roman Novak](https://github.com/romanngg)<br>- [Lechao Xiao](https://sites.google.com/site/lechaoxiao/)<br>- [Jiri Hron](https://sites.google.com/view/jirihron)<br>- [Jaehoon Lee](https://jaehlee.github.io/)<br>others[Alexander Alemi](https://www.alexalemi.com/)<br>[Jascha Sohl-Dickstein](https://sohldickstein.com/)<br>[Samuel Schoenholz](https://scholar.google.com/citations?user=mk-zQBsAAAAJ) | [![](https://camo.githubusercontent.com/365e3230286be8b96239d181bd4b7183b2697fea2b45a464e031d9daf23fbb89/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6e657572616c2d74616e67656e74733f7374796c653d736f6369616c)](https://github.com/google/neural-tangents) <br>- [ICLR](https://iclr.cc/virtual_2020/poster_SklD9yrFPS.html)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1912.02803), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1605.07146), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1902.06720), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1806.07572), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2001.07301), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/2003.02237)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://neural-tangents.readthedocs.io/en/latest/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/infinitely-wide-neural-networks-neural-tangents-explained-d6c6d896fcbf)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/neural-tangents/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Neural_network_Gaussian_process), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Neural_tangent_kernel)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/VUX2bsrYag8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/neural-tangents/blob/main/notebooks/neural_tangents_cookbook.ipynb) | 29.08.2023 |
| Stable Diffusion 2 | New stable diffusion model at 768x768 resolution. Same number of parameters in the U-Net as 1.5, but uses OpenCLIP-ViT/H as the text encoder and is trained from scratch | - [Robin Rombach](https://github.com/rromb)<br>- [Andreas Blattmann](https://github.com/ablattmann)<br>- [Dominik Lorenz](https://github.com/qp-qp)<br>- [Patrick Esser](https://github.com/pesser)<br>others[Björn Ommer](https://ommer-lab.com/people/ommer/)<br>[qunash](https://github.com/qunash) | [![](https://camo.githubusercontent.com/3afb12adc6f6de4f662f92a76ac00bb2673378d06ac3a76a108eaf8e7624db17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53746162696c6974792d41492f737461626c65646966667573696f6e3f7374796c653d736f6369616c)](https://github.com/Stability-AI/stablediffusion)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10752), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.00512), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.02502), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.01073), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.09778), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2206.00927)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/qunash/stable-diffusion-2-gui), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/isl-org/MiDaS), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/denoising-diffusion-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/runwayml/stable-diffusion/blob/main/scripts/inpaint_st.py), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/crowsonkb/k-diffusion)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/stable-diffusion-2-1), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/stable-diffusion-2-1-base), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/stable-diffusion-2-depth), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/HytucGhwTRs) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/qunash/stable-diffusion-2-gui/blob/main/stable_diffusion_2_0.ipynb) | 26.08.2023 |
| DALL·E Mini | Generate images from a text prompt | - [Boris Dayma](https://github.com/borisdayma)<br>- [Suraj Patil](https://github.com/patil-suraj)<br>- [Pedro Cuenca](https://github.com/pcuenca)<br>- [Khalid Saifullah](https://khalidsaifullaah.github.io/)<br>others[Tanishq Abraham](https://github.com/tmabraham)<br>[Phúc H. Lê Khắc](https://lkhphuc.com/)<br>[Luke Melas](https://lukemelas.github.io/)<br>[Ritobrata Ghosh](https://ghosh-r.github.io/) | [![](https://camo.githubusercontent.com/acc3d93bf7e1933a5fbecd0129aedd07414e607cf961842687c22a911cac7b18/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626f7269736461796d612f64616c6c652d6d696e693f7374796c653d736f6369616c)](https://github.com/borisdayma/dalle-mini)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.08981), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.09841), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.13461), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.00020), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.09841), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1807.04015)<br>- [blog post](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA)<br>- [data](https://aclanthology.org/P18-1238/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/huggingface/transformers/tree/master/examples/research_projects/jax-projects), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/CLIP/blob/main/data/yfcc100m.md)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/flax-community/dalle-mini) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb) | 22.08.2023 |
| Classify text with BERT | This tutorial contains complete code to fine-tune BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews | [Anirudh Dubey](https://github.com/anirudh161) | [![](https://camo.githubusercontent.com/6ca17dd7aec85150f2a72bdf5a7aa9823a7687123bc8e0b0c2c0c9f6a00a3152/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f4e31392d31343233)](https://doi.org/10.18653/v1/N19-1423)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.04805), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1711.05101)<br>- [data](https://ai.stanford.edu/~amaas/data/sentiment/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/text-classification)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://tfhub.dev/google/collections/bert/1), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb) | 08.08.2023 |
| Kandinsky 2.1 | As text and image encoder it uses CLIP model and diffusion image prior between latent spaces of CLIP modalities | - [Arseniy Shakhmatov](https://github.com/cene555)<br>- [Anton Razzhigaev](https://github.com/razzant)<br>- [Aleksandr Nikolich](https://github.com/AlexWortega)<br>- [Vladimir Arkhipkin](https://github.com/oriBetelgeuse)<br>others[Igor Pavlov](https://github.com/boomb0om)<br>[Andrey Kuznetsov](https://github.com/kuznetsoffandrey)<br>[Denis Dimitrov](https://github.com/denndimitrov) | [![](https://camo.githubusercontent.com/d5cd7a452a964d29994649bb9a060109b82df135368641f31d699e60a3ab4c62/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61692d666f72657665722f4b616e64696e736b792d323f7374796c653d736f6369616c)](https://github.com/ai-forever/Kandinsky-2) <br>- [blog post](https://habr.com/ru/companies/sberbank/articles/725282/)<br>- [demo](https://editor.fusionbrain.ai/)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/sberbank-ai/Kandinsky_2.1)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/LZvp4SWcCao), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/IoPhRE37XSU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dYt9xJ7dnpU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rN2J5TL2RZ0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1xSbu-b-EwYd6GdaFPRVgvXBX_mciZ41e) | 07.08.2023 |
| SoftVC VITS | Singing Voice Conversion | [svc develop team](https://github.com/svc-develop-team) | [![](https://camo.githubusercontent.com/9ac86000a0028a9a069d345ec3fed87484b2657b3a38d0c0ebb5f49bfa22065f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7376632d646576656c6f702d7465616d2f736f2d766974732d7376633f7374796c653d736f6369616c)](https://github.com/svc-develop-team/so-vits-svc)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NaruseMioShirakana/MoeVoiceStudio), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/auspicious3000/contentvec), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yxlllc/DDSP-SVC), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/flutydeer/audio-slicer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvpi/audio-slicer)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/NaruseMioShirakana/MoeSS-SUBModel/tree/main) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/svc-develop-team/so-vits-svc/blob/4.1-Stable/sovits4_for_colab.ipynb) | 31.07.2023 |
| threestudio | Unified framework for 3D content creation from text prompts, single images, and few-shot images, by lifting 2D text-to-image generation models | - [Yuan-Chen Guo](https://github.com/bennyguo)<br>- [Ying-Tian Liu](https://github.com/thuliu-yt16)<br>- [Ruizhi Shao](https://github.com/DSaurus)<br>- [Christian Laforte](https://github.com/claforte)<br>others[Vikram Voleti](https://github.com/voletiv)<br>[Guan Luo](https://github.com/logan0601)<br>[Chia-Hao Chen](https://scholar.google.com/citations?user=X0zirvMAAAAJ)<br>[Zi-Xin Zou](https://github.com/zouzx)<br>[Chen Wang](https://cwchenwang.github.io/)<br>[Yanpei Cao](https://yanpei.me/)<br>[Song-Hai Zhang](https://scholar.google.com/citations?user=AWtV-EQAAAAJ) | [![](https://camo.githubusercontent.com/d788258809e74e2f104491958b3fac60260812ad7f63bc658243023ad69338fb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f746872656573747564696f2d70726f6a6563742f746872656573747564696f3f7374796c653d736f6369616c)](https://github.com/threestudio-project/threestudio)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.15413), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.16213), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2211.10440)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/ejer2MAB8N)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DSaurus/Tensor4D), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/eladrich/latent-nerf), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Gorilla-Lab-SCUT/Fantasia3D), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cvlab-columbia/zero123), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/guochengqian/Magic123), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ayaanzhaque/instruct-nerf2nerf), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/KAIR-BAIR/nerfacc), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Lightning-AI/lightning), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ashawkey/fantasia3d.unofficial)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/huggingface_hub/v0.14.1/guides/download#download-an-entire-repository)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/StableDiffusion/comments/1635cb0/threestudio_a_unified_framework_for_3d_content/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gT8Xvx5b6IE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/threestudio-project/threestudio/blob/main/threestudio.ipynb) | 28.07.2023 |
| Image captioning | Given an image our goal is to generate a caption | - [Kelvin Xu](https://kelvinxu.github.io/)<br>- [Jimmy Ba](https://jimmylba.github.io/)<br>- [Ryan Kiros](https://github.com/ryankiros)<br>- [Kyunghyun Cho](https://kyunghyuncho.me/)<br>others[Aaron Courville](https://mila.quebec/en/directory/aaron-courville)<br>[Ruslan Salakhutdinov](https://www.cs.cmu.edu/~rsalakhu/)<br>[Richard Zemel](https://www.cs.columbia.edu/~zemel/)<br>[Yoshua Bengio](https://yoshuabengio.org/) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1502.03044)<br>- [data](https://cocodataset.org/#home)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@labbikarmacharya/paper-review-show-attend-and-tell-neural-image-caption-generation-with-visual-attention-03928d8fe17b)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/text/tutorials/image_captioning) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb) | 25.07.2023 |
| Word2Vec | Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets | - [Tomas Mikolov](https://scholar.google.com/citations?user=oBu8kMMAAAAJ)<br>- [Kai Chen](https://scholar.google.com/citations?user=TKvd_Z4AAAAJ)<br>- [Greg Corrado](https://research.google/people/gregcorrado/)<br>- [Jeffrey Dean](https://research.google/people/jeff/) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1301.3781)<br>- [link](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@harshvivek14/summary-of-efficient-estimation-of-word-representations-in-vector-space-8f96d38f4b13), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@towardsautonomy/word2vec-part1-5436c846c3fa)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)<br>- [projector](http://projector.tensorflow.org/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/method/cbow-word2vec), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/method/skip-gram-word2vec)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Word2vec), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Zipf%27s_law) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb) | 25.07.2023 |
| Word embeddings | This tutorial contains an introduction to word embeddings | [Billy Lamberta](https://github.com/lamberta) | - [data](http://ai.stanford.edu/~amaas/data/sentiment/)<br>- [projector](http://projector.tensorflow.org/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/text/guide/word_embeddings) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/word_embeddings.ipynb) | 25.07.2023 |
| Tortoise | A multi-voice TTS system trained with an emphasis on quality | [James Betker](https://nonint.com/) | [![](https://camo.githubusercontent.com/948a2404c67b4420de301bf14c46731cca79e9ce06f22b0e032ef3747b803b53/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e656f6e626a622f746f72746f6973652d7474733f7374796c653d736f6369616c)](https://github.com/neonbjb/tortoise-tts)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.12092), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.09672), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.07889)<br>- [examples](https://nonint.com/static/tortoise_v2_examples.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/neonbjb/DL-Art-School)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/patrickvonplaten), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/osanseviero/tortoisse-tts)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/J3-jfS29RF4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/neonbjb/tortoise-tts/blob/main/tortoise_tts.ipynb) | 15.07.2023 |
| Petals | Run 100B+ language models at home, BitTorrent-style | [BigScience](https://bigscience.huggingface.co/) | [![](https://camo.githubusercontent.com/3efb7cc0071e1446f2cd1263ec9a298de7eb73aec5e71d818470bf187cf19d43/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f626967736369656e63652d776f726b73686f702f706574616c733f7374796c653d736f6369616c)](https://github.com/bigscience-workshop/petals)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.01188), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.07258)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/borzunov/chat.petals.ml), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/timDettmers/bitsandbytes)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/bigscience/bloom)<br>- [project](https://petals.ml/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/BitTorrent) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ) | 05.07.2023 |
| Epistemic Neural Networks | A library for neural networks that know what they don't know | - [Ian Osband](http://iosband.github.io/)<br>- [Zheng Wen](http://zheng-wen.com/)<br>- [Seyed Mohammad Asghari](https://github.com/mohammadasghari)<br>- [Vikranth Dwaracherla](https://github.com/dvikranth)<br>others[Morteza Ibrahimi](https://github.com/mibrahimi)<br>[Xiuyuan Lu](https://scholar.google.com/citations?user=SPL_2lIAAAAJ)<br>[Benjamin Van Roy](https://web.stanford.edu/~bvr/) | [![](https://camo.githubusercontent.com/563ae5ffd9f043e2a20c087ec941fe0c7d5ff63e030a48581a99fdc546bf9e11/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f656e6e3f7374796c653d736f6369616c)](https://github.com/deepmind/enn)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2107.08924)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/syncedreview/deepminds-epistemic-neural-networks-open-new-avenues-for-uncertainty-modelling-in-large-and-fa83ab00aba3)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/j8an0dKcX4A) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/enn/blob/master/enn/colabs/enn_demo.ipynb) | 27.06.2023 |
| DeepFloyd IF | State-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding | - [Alex Shonenkov](https://linktr.ee/shonenkovAI)<br>- [Misha Konstantinov](https://github.com/zeroshot-ai)<br>- [Daria Bakshandaeva](https://github.com/Gugutse)<br>- [Christoph Schuhmann](http://christoph-schuhmann.de/)<br>others[Ksenia Ivanova](https://github.com/ivksu)<br>[Nadiia Klokova](https://github.com/vauimpuls) | [![](https://camo.githubusercontent.com/4975f1d479f245fd90cf288993b34bcc09ad5d4aab810e9b1334e81d48e0ec36/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565702d666c6f79642f49463f7374796c653d736f6369616c)](https://github.com/deep-floyd/IF)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.11487)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/umz62Mgr)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/DeepFloyd), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/diffusers/optimization/fp16#model-offloading-for-fast-inference-and-memory-savings), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-speed), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-memory), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/blog/if), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/diffusers/main/en/api/pipelines/if)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/shonenkov/deepfloyd-if-4-3b-generator-of-pictures)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/deepfloydai)<br>- [website](https://deepfloyd.ai/deepfloyd-if)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/4Zkipll5Rjc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/tq5ZXZWwTPA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rLtfd1TvYJk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/deepfloyd_if_free_tier_google_colab.ipynb) | 26.06.2023 |
| normflows | PyTorch implementation of discrete normalizing flows | - [Vincent Stimper](https://is.mpg.de/person/vstimper)<br>- [David Liu](https://davindicode.github.io/)<br>- [Andrew Campbell](https://github.com/andrew-cr)<br>- [Vincent Berenz](http://vincentberenz.is.tuebingen.mpg.de/)<br>others[Lukas Ryll](https://github.com/lukasryll)<br>[Bernhard Schölkopf](https://scholar.google.com/citations?user=DZ-fHPgAAAAJ)<br>[José Miguel Hernández-Lobato](https://jmhl.org/) | [![](https://camo.githubusercontent.com/9bb1d18695d664566be0bfee1ba5e723495882c8ac92988554826a7f7ac0a879/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f56696e63656e745374696d7065722f6e6f726d616c697a696e672d666c6f77733f7374796c653d736f6369616c)](https://github.com/VincentStimper/normalizing-flows)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2302.12014)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://vincentstimper.github.io/normalizing-flows/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/VincentStimper/resampled-base-flows), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/VincentStimper/hmc-hyperparameter-tuning)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Von_Mises_distribution) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/VincentStimper/normalizing-flows/blob/master/examples/paper_example_nsf_colab.ipynb) | 26.06.2023 |
| MMPose | Toolbox for pose estimation based on PyTorch | [OpenMMLab](https://openmmlab.com/) | [![](https://camo.githubusercontent.com/0dcbecfcd2b2359a2f39cfa4a2e0fe17ff8281a7c2586c57f172ab01eb1d0532/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e2d6d6d6c61622f6d6d706f73653f7374796c653d736f6369616c)](https://github.com/open-mmlab/mmpose)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.com/channels/1037617289144569886/1072798105428299817)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mmpose.readthedocs.io/en/latest/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://openmmlab.medium.com/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/mmpose/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/OpenMMLab)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/openmmlab), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nFcZ2H1Ix3w) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/open-mmlab/mmpose/blob/master/demo/MMPose_Tutorial.ipynb) | 19.06.2023 |
| MyoSuite | A collection of musculoskeletal environments and tasks simulated with the MuJoCo physics engine and wrapped in the OpenAI gym API to enable the application of Machine Learning to bio-mechanic control problems | - [Vittorio Caggiano](https://github.com/Vittorio-Caggiano)<br>- [Huawei Wang](https://huaweiwang.github.io/)<br>- [Guillaume Durandau](https://people.utwente.nl/g.v.durandau)<br>- [Massimo Sartori](https://people.utwente.nl/m.sartori)<br>- [Vikash Kumar](https://vikashplus.github.io/) | [![](https://camo.githubusercontent.com/416223edb8f000c8d74436d0bb384b066e8d4a8d58ffd631269a6bead90cb4a7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f6d796f73756974653f7374796c653d736f6369616c)](https://github.com/facebookresearch/myosuite)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.13600)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://myosuite.readthedocs.io/en/latest/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1U6vo6Q_rPhDaq6oUMV7EAZRm6s0fD1wn) | 16.06.2023 |
| Audiocraft | PyTorch library for deep learning research on audio generation | - [Jade Copet](https://scholar.google.com/citations?&user=GRMLwjAAAAAJ)<br>- [Felix Kreuk](https://felixkreuk.github.io/)<br>- [Itai Gat](https://itaigat.com/)<br>- [Tal Remez](https://talremez.github.io/)<br>others[David Kant](https://www.linkedin.com/in/david-kant-339a3b1b7)<br>[Gabriel Synnaeve](https://syhw.github.io/)<br>[Yossi Adi](https://www.cs.huji.ac.il/~adiyoss/)<br>[Alexandre Défossez](https://ai.honu.io/) | [![](https://camo.githubusercontent.com/f6de7f8a325909c6dc9eb90a8d967ad99338a33a9038ced257526bee33a7cca0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f617564696f63726166743f7374796c653d736f6369616c)](https://github.com/facebookresearch/audiocraft)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2306.05284), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.11325)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/encodec), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/camenduru/MusicGen-colab)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/facebook/musicgen-large)<br>- [project](https://ai.honu.io/papers/musicgen/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/v-YpvPkhdO4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=EGfxuTy9Eeo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/la2fGS0dW98), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/v-YpvPkhdO4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1fxGqfg96RBUvGxZ1XXN07s3DthrKUl4-) | 11.06.2023 |
| Detectron2 | FAIR's next-generation platform for object detection and segmentation | [Yuxin Wu](http://ppwwyyxx.com/) | [![](https://camo.githubusercontent.com/17766b0343c7c0ac72ea121d78d8d2068ba358a9584406cdc786e2a2ad0707f0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f646574656374726f6e323f7374796c653d736f6369616c)](https://github.com/facebookresearch/detectron2) <br>- [blog post](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://detectron2.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) | 26.05.2023 |
| Reverb | Efficient and easy-to-use data storage and transport system designed for machine learning research | - [Albin Cassirer](https://github.com/acassirer)<br>- [Gabriel Barth-Maron](https://github.com/fastturtle)<br>- [Eugene Brevdo](https://ebrevdo.github.io/)<br>- [Sabela Ramos](https://github.com/sabelaraga)<br>others[Toby Boyd](https://github.com/tfboyd)<br>[Thibault Sottiaux](https://github.com/thso) | [![](https://camo.githubusercontent.com/342cc6ac08da6eb3b12e0be3f9005f4014400af30c1c06e435d47f7055a9ad3b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d646565706d696e642f7265766572623f7374796c653d736f6369616c)](https://github.com/google-deepmind/reverb)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.04736), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1801.01290), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1509.02971), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1707.01495), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1511.05952), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1804.08617), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1802.01561), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1707.06347)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/dm-reverb/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/reinforcementlearning/comments/lhnrkd/reverb_a_framework_for_experience_replay/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/reverb/blob/master/examples/demo.ipynb) | 23.05.2023 |
| MMDetection | Open source object detection toolbox based on PyTorch | [OpenMMLab](https://openmmlab.com/) | [![](https://camo.githubusercontent.com/68b0f0a824d1ce240275fa6f85668a366029403a575fc71d41dfdccc91ae97e4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e2d6d6d6c61622f6d6d646574656374696f6e3f7374796c653d736f6369616c)](https://github.com/open-mmlab/mmdetection)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1906.07155), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2401.02361), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.07784)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.com/channels/1037617289144569886/1046608014234370059)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mmdetection.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tusen-ai/simpledet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmcv), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmengine)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://openmmlab.medium.com/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/real-time-instance-segmentation-on-mscoco?p=rtmdet-an-empirical-study-of-designing-real), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/object-detection-in-aerial-images-on-dota-1?p=rtmdet-an-empirical-study-of-designing-real), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/object-detection-in-aerial-images-on-hrsc2016?p=rtmdet-an-empirical-study-of-designing-real)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/mmdet)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/OpenMMLab)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/openmmlab), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5kgWyo6Sg4E), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/4SuwN4xSM3Q), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/SWB2pTY3UDM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/AEIDB6Dd6bM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7c2JKPMVPm0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/open-mmlab/mmdetection/blob/main/demo/MMDet_Tutorial.ipynb) | 17.05.2023 |
| ChatRWKV | Like ChatGPT but powered by RWKV (100% RNN) language model, which is the only RNN that can match transformers in quality and scaling, while being faster and saves VRAM | - [Bo Peng](https://github.com/BlinkDL)<br>- [Eric Alcaide](https://hypnopump.github.io/)<br>- [Quentin Anthony](https://quentin-anthony.github.io/)<br>- [Alon Albalak](https://alon-albalak.github.io/)<br>others[Samuel Arcadinho](https://github.com/SSamDav)<br>[Matteo Grella](http://www.matteogrella.com/)<br>[Kranthi Kiran](https://kranthigv.github.io/)<br>[Haowen Hou](https://github.com/howard-hou)<br>[Przemyslaw Kazienko](https://kazienko.eu/en)<br>[Jan Kocon](https://github.com/KoconJan)<br>[Bartlomiej Koptyra](https://github.com/bkoptyra)<br>[Ipsit Mantri](https://ipsitmantri.github.io/)<br>[Ferdinand Mom](https://3outeille.github.io/)<br>[Xiangru Tang](https://github.com/tangxiangru)<br>[Johan Wind](https://johanwind.github.io/)<br>[Stanisław Woźniak](https://www.researchgate.net/profile/Stanislaw-Wozniak-3)<br>[Qihang Zhao](https://www.researchgate.net/profile/Qihang-Zhao-2)<br>[Peng Zhou](https://pengzhou.sites.ucsc.edu/)<br>[Jian Zhu](https://lingjzhu.github.io/)<br>[Rui-Jie Zhu](https://scholar.google.com/citations?user=08ITzJsAAAAJ) | [![](https://camo.githubusercontent.com/64b2349c0a6f40ac4de84cd2d52f31d6514d859813ca6a50dd99ed6ff291dc10/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f426c696e6b444c2f4368617452574b563f7374796c653d736f6369616c)](https://github.com/BlinkDL/ChatRWKV)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.13048)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/bDSBUMeFpc)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/saharNooby/rwkv.cpp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Blealtan/RWKV-LM-LoRA), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/josStorer/RWKV-Runner)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/BlinkDL)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/1135aew/r_rwkv4_14b_release_and_chatrwkv_a_surprisingly/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/BlinkDL_AI)<br>- [website](https://www.rwkv.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/UeAD1qWNb1U) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_ChatRWKV.ipynb) | 08.05.2023 |
| PyGlove | General-purpose library for Python object manipulation | - [Daiyi Peng](https://github.com/daiyip)<br>- [Xuanyi Dong](https://xuanyidong.com/)<br>- [Esteban Real](https://www.estebanreal.com/)<br>- [Mingxing Tan](https://scholar.google.com/citations?user=6POeyBoAAAAJ)<br>others[Yifeng Lu](https://github.com/yifenglou)<br>[Gabriel Bender](https://scholar.google.com/citations?user=-kPFcUUAAAAJ)<br>[Hanxiao Liu](https://quark0.github.io/)<br>[Adam Kraft](https://adamwkraft.github.io/)<br>[Chen Liang](https://crazydonkey200.github.io/)<br>[Quoc Le](https://cs.stanford.edu/~quocle/) | [![](https://camo.githubusercontent.com/6327ad4683a9e3b436dd31e65368d6d51452399a949c10a63036b8a762a24758/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f7079676c6f76653f7374796c653d736f6369616c)](https://github.com/google/pyglove)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2101.08809)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://pyglove.readthedocs.io/en/latest/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://synced.medium.com/google-brain-introduces-symbolic-programming-pyglove-library-to-reformulate-automl-bde1d60cb8f6)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2020/hash/012a91467f210472fab4e11359bbfef6-Abstract.html)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/pyglove/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/Python/comments/lc9r1d/researchers_from_google_brain_introduces_symbolic/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/pyglove/blob/main/docs/notebooks/ml/efficiently_exchange_ml_ideas_as_code.ipynb) | 06.05.2023 |
| Python Data Science Handbook | Jupyter notebook version of the Python Data Science Handbook by Jake VanderPlas | [Jake Vanderplas](http://vanderplas.com/) | [![](https://camo.githubusercontent.com/b2279f99106dd1c53cc266774c7d9c7f5ba63be4b9d32a7cf57931a0f5154719/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a616b657664702f507974686f6e44617461536369656e636548616e64626f6f6b3f7374796c653d736f6369616c)](https://github.com/jakevdp/PythonDataScienceHandbook) <br>- [project](https://jakevdp.github.io/PythonDataScienceHandbook/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb) | 06.05.2023 |
| PGMax | General factor graphs for discrete probabilistic graphical models, and hardware-accelerated differentiable loopy belief propagation in JAX | - [Guangyao Zhou](https://stanniszhou.github.io/)<br>- [Nishanth Kumar](http://nishanthjkumar.com/)<br>- [Antoine Dedieu](https://github.com/antoine-dedieu)<br>- [Miguel Lázaro-Gredilla](https://www.tsc.uc3m.es/~miguel/)<br>others[Shrinu Kushagra](https://cs.uwaterloo.ca/~skushagr/)<br>[Dileep George](https://dileeplearning.github.io/) | [![](https://camo.githubusercontent.com/f69e752f6a892986ecc60a3f2a574a979b0023e598945f3a6e2a95fd07793edc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f50474d61783f7374796c653d736f6369616c)](https://github.com/deepmind/PGMax)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.04110)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Belief_propagation) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/PGMax/blob/main/examples/rcn.ipynb) | 05.05.2023 |
| StableLM | Stability AI Language Models | [Stability AI](https://stability.ai/research) | [![](https://camo.githubusercontent.com/893b85954f60a19d68f3ec84cccb0b1db48453d3c8c5def846dd1be77dca2326/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53746162696c6974792d41492f537461626c654c4d3f7374796c653d736f6369616c)](https://github.com/Stability-AI/StableLM) <br>- [blog post](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/llama), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tatsu-lab/stanford_alpaca), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nomic-ai/gpt4all), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/databrickslabs/dolly), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/anthropics/hh-rlhf), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ggerganov/llama.cpp)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/lmsys/vicuna-13b-delta-v0), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/RyokoAI/ShareGPT52K), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/stabilityai)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/dypPSs4t77g), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nWf1StvtoRw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Hg-s2RTaTFE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qXtJjoEfTnA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Stability-AI/StableLM/blob/main/notebooks/stablelm-alpha.ipynb) | 27.04.2023 |
| TTS | A library for advanced Text-to-Speech generation, built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality | - [Eren Gölge](https://github.com/erogol)<br>- [Aya-AlJafari](https://github.com/Aya-AlJafari)<br>- [Edresson Casanova](https://github.com/Edresson)<br>- [Josh Meyer](http://jrmeyer.github.io/)<br>others[Kelly Davis](https://github.com/kdavis-coqui)<br>[Reuben Morais](https://github.com/reuben) | [![](https://camo.githubusercontent.com/985a8454c38b97b8047e479b3b1fff8a85457a90ec642febc8d8a241b0bd52d9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f7175692d61692f5454533f7374796c653d736f6369616c)](https://github.com/coqui-ai/TTS) <br>- [blog post](https://coqui.ai/blog/tts/solving-attention-problems-of-tts-models-with-double-decoder-consistency)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://tts.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/coqui-ai/TTS-papers)<br>- [samples](https://erogol.github.io/ddc-samples/)<br>- [website](https://coqui.ai/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ADnBCz0Wd1U), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Yglxf2WbkLU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/alpI-DnVlO0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/coqui-ai/TTS/blob/dev/notebooks/Tutorial_2_train_your_first_TTS_model.ipynb) | 26.04.2023 |
| OpenCLIP | An open source implementation of CLIP | - [Ross Wightman](https://rwightman.com/)<br>- [Cade Gordon](https://cadegordon.io/)<br>- [Vaishaal Shankar](http://vaishaal.com/) | [![](https://camo.githubusercontent.com/34ebaa7d23a589122895ddcfc780ceb9c940360ff785da71a7dfd40a00c6b3ce/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6c666f756e646174696f6e732f6f70656e5f636c69703f7374796c653d736f6369616c)](https://github.com/mlfoundations/open_clip)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.01903), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.00020), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.02114), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2107.04649), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1902.10811), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2107.04649)<br>- [data](https://ai.google.com/research/ConceptualCaptions/download), [data](https://laion.ai/blog/laion-5b/), [data](https://laion.ai/blog/laion-400-open-dataset/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mlfoundations/wise-ft), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/webdataset/webdataset), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/webdataset/tarp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-research-datasets/conceptual-12m)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/laion/laion2B-en), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb) | 16.04.2023 |
| Stable Baselines3 | Set of reliable implementations of reinforcement learning algorithms in PyTorch | - [Antonin Raffin](https://araffin.github.io/)<br>- [Ashley Hill](https://hill-a.me/)<br>- [Adam Gleave](https://www.gleave.me/)<br>- [Anssi Kanervisto](https://github.com/Miffyli)<br>others[Maximilian Ernestus](https://github.com/ernestum)<br>[Noah Dormann](https://github.com/ndormann) | [![](https://camo.githubusercontent.com/50c9ac255fe1db2d398740e0d9b140153bab01e503c1dc130576a6db9898cd2c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f444c522d524d2f737461626c652d626173656c696e6573333f7374796c653d736f6369616c)](https://github.com/DLR-RM/stable-baselines3)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://stable-baselines3.readthedocs/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hill-a/stable-baselines), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/gym/wiki/Environments)<br>- [paper](https://jmlr.org/papers/v22/20-1364.html)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/reinforcementlearning/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLQVvvaa0QuDf0O2DWwLZBfJeYY-JOeZB1) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb) | 14.04.2023 |
| RL Baselines3 Zoo | Training Framework for Stable Baselines3 Reinforcement Learning Agents | [Antonin Raffin](https://araffin.github.io/) | [![](https://camo.githubusercontent.com/58ab4dba9e0cbfd83b4490e6447fd4b04e98718a7367ce95809b2fa485d9fb20/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f444c522d524d2f726c2d626173656c696e6573332d7a6f6f3f7374796c653d736f6369616c)](https://github.com/DLR-RM/rl-baselines3-zoo)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.05719)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://stable-baselines3.readthedocs.io/en/master/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/DLR-RM/rl-baselines3-zoo), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/roboschool), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Farama-Foundation/Minigrid)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/sb3) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb) | 14.04.2023 |
| Grounded-SAM | Marrying Grounding DINO with Segment Anything & Stable Diffusion & Recognize Anything - Automatically Detect, Segment and Generate Anything | [IDEA-Research](https://www.idea.edu.cn/) | [![](https://camo.githubusercontent.com/a6feb0326dc5825bc17354675a5c5cf4e0b32f66fe541a0634748c4923204f75/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f494445412d52657365617263682f47726f756e6465642d5365676d656e742d416e797468696e673f7374796c653d736f6369616c)](https://github.com/IDEA-Research/Grounded-Segment-Anything)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2304.02643), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2303.05499)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/MasterBin-IIAU/UNINEXT), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IDEA-Research/OSX), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/dvlab-research/VoxelNeXt), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/UX-Decoder/Semantic-SAM), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/IDEA-Research/OpenSeeD), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/sail-sg/EditAnything), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/feizc/IEA), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Li-Qingyun/sam-mmrotate), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/VainF/Awesome-Anything), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/RockeyCoss/Prompt-Segment-Anything)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/oEQYStnF2l8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gKTYMfwPo4M), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0Fpb8TBH0nM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/GuEDDBWrN24) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/betogaona7/Grounded-Segment-Anything/blob/main/grounded_sam_colab_demo.ipynb) | 12.04.2023 |
| TFDS | Collection of ready-to-use datasets for use with TensorFlow, Jax, and other Machine Learning frameworks | [Google](https://www.tensorflow.org/) | [![](https://camo.githubusercontent.com/996a848750a62194157abe71fd25d3c7ced2760e8804d6042001c61b044e97e0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f64617461736574733f7374796c653d736f6369616c)](https://github.com/tensorflow/datasets)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/youre-importing-data-wrong-c171f52eea00)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YrMy-BAqk8k), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/6th3rahsw9Y), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3HYy0SPd7TE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MvcK-MaXbHk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb) | 11.04.2023 |
| Optimum | Extension of Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use | [Hugging Face](https://huggingface.co/) | [![](https://camo.githubusercontent.com/2d73f01adcb97d3a3e7a0c85877850514ec7ad86e9b6d69199631196ad3a5ae0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f6f7074696d756d3f7374796c653d736f6369616c)](https://github.com/huggingface/optimum)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openvinotoolkit/nncf)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/optimum/index), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/main_classes/trainer)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/UJnfePM0Ur8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/b1Gk9q9empA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/_AKFDOnrZz8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb) | 06.04.2023 |
| MMOCR | Open source toolkit based on PyTorch and MMDetection, supporting numerous OCR-related models, including text detection, text recognition, and key information extraction | - [Zhanghui Kuang](https://jeffreykuang.github.io/)<br>- [Hongbin Sun](https://github.com/cuhk-hbsun)<br>- [Zhizhong Li](https://zhizhong.li/)<br>- [Xiaoyu Yue](https://yuexy.github.io/#/)<br>others[Tsui Hin Lin](https://dl.acm.org/profile/99659894554)<br>[Jianyong Chen](https://github.com/HolyCrap96)<br>[Huaqiang Wei](https://github.com/weihuaqiang)<br>[Yiqin Zhu](https://scholar.google.com/citations?user=ZH9cp50AAAAJ)<br>[Tong Gao](https://github.com/gaotongxiao)<br>[Wenwei Zhang](https://zhangwenwei.cn/)<br>[Kai Chen](https://chenkai.site/)<br>[Wayne Zhang](https://www.statfe.com/)<br>[Dahua Lin](http://dahua.site/) | [![](https://camo.githubusercontent.com/87532809f0f9fbfab3518e3b7c7bac9e0960fe6c816549c6910918cee4be16f1/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333437343038352e33343738333238)](https://doi.org/10.1145/3474085.3478328)[![](https://camo.githubusercontent.com/c363b0ff395aeab38567d84ca8d5ef053cf80ff0e852380a92ed05c427416548/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e2d6d6d6c61622f6d6d6f63723f7374796c653d736f6369616c)](https://github.com/open-mmlab/mmocr)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.06543)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/raweFPmdzG)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mmocr.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmengine), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmcv)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://openmmlab.medium.com/mmocr-a-comprehensive-toolbox-for-text-detection-recognition-and-understanding-795befa726b8)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/mmocr/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/OpenMMLab)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/U7VYfHeE0KQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Snyu-o8ZdDk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/g7qfSYkkpUA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/openmmlab) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/open-mmlab/mmocr/blob/master/demo/tutorial.ipynb) | 06.04.2023 |
| MMSegmentation | Open source semantic segmentation toolbox based on PyTorch | [OpenMMLab](https://openmmlab.com/) | [![](https://camo.githubusercontent.com/5cbb4a66f7ac3776b7309303313fddc6a38dbe5ebc4607eff3c3184e4f7d59df/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e2d6d6d6c61622f6d6d7365676d656e746174696f6e3f7374796c653d736f6369616c)](https://github.com/open-mmlab/mmsegmentation)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/raweFPmdzG)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mmsegmentation.readthedocs.io/en/main/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://openmmlab.medium.com/), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://mducducd33.medium.com/sematic-segmentation-using-mmsegmentation-bcf58fb22e42)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/mmsegmentation)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/OpenMMLab)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/openmmlab) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/open-mmlab/mmsegmentation/blob/main/demo/MMSegmentation_Tutorial.ipynb) | 31.03.2023 |
| LAVIS | Python deep learning library for LAnguage-and-VISion intelligence research and applications | - [Dongxu Li](https://github.com/dxli94)<br>- [Junnan Li](https://github.com/LiJunnan1992)<br>- [Hung Le](https://sites.google.com/view/henryle2018/home)<br>- [Guangsen Wang](https://github.com/guangsen-wang)<br>others[Silvio Savarese](https://scholar.google.com/citations?user=ImpbxLsAAAAJ)<br>[Steven Hoi](https://sites.google.com/view/stevenhoi) | [![](https://camo.githubusercontent.com/5bdc620e8da4e3cb5877175dafee1ccb6311520893259144e337fccf4ef02b0b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73616c6573666f7263652f4c415649533f7374796c653d736f6369616c)](https://github.com/salesforce/LAVIS)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.09019), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.06500), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2301.12597), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2212.10846), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2210.08773)<br>- [blog post](https://blog.salesforceairesearch.com/lavis-language-vision-library/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://opensource.salesforce.com/LAVIS//latest/index.html)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Merlion) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2llm-vqa/img2llm_vqa.ipynb) | 24.03.2023 |
| AudioLM | Framework for high-quality audio generation with long-term consistency | - [Phil Wang](https://lucidrains.github.io/)<br>- [Zalán Borsos](https://zalanborsos.com/)<br>- [Raphaël Marinier](https://github.com/RaphaelMarinier)<br>- [Damien Vincent](https://www.linkedin.com/in/damien-vincent-1958381)<br>others[Eugene Kharitonov](https://eugene-kharitonov.github.io/)<br>[Olivier Pietquin](https://research.google/people/105812)<br>[Matt Sharifi](https://scholar.google.com/citations?user=GeQNBz0AAAAJ)<br>[Olivier Teboul](https://scholar.google.com/citations?user=ep0OfyAAAAAJ)<br>[David Grangier](http://david.grangier.info/)<br>[Marco Tagliasacchi](https://scholar.google.com/citations?user=zwH1rZQAAAAJ)<br>[Neil Zeghidour](https://github.com/lienz) | [![](https://camo.githubusercontent.com/0829ee537d0711c8057d6da1cb5b23161d00a0a4f052f0fd0740df60850f0bb1/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f5441534c502e323032332e33323838343039)](https://doi.org/10.1109/TASLP.2023.3288409)[![](https://camo.githubusercontent.com/d70adeb5594f6f615ca1aafd92777387b2041a0cd027da5a18b8e84f0e7d6630/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c756369647261696e732f617564696f6c6d2d7079746f7263683f7374796c653d736f6369616c)](https://github.com/lucidrains/audiolm-pytorch)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2209.03143), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2107.03312), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.02765), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.19466), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2002.05202), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1911.02150), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.12598), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.13290), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2210.13432), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.09883), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.05707), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2210.13438)<br>- [blog post](https://blog.research.google/2022/10/audiolm-language-modeling-approach-to.html)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/xBPBXfcFHd)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/encodec), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/musiclm-pytorch)<br>- [project](https://google-research.github.io/seanet/audiolm/examples/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Vucewi_kPEU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/behUbh0koZk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/olNvmUCmY8o) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/lucidrains/audiolm-pytorch/blob/main/audiolm_pytorch_demo.ipynb) | 23.03.2023 |
| pymdp | Package for simulating Active Inference agents in Markov Decision Process environments | - [Conor Heins](https://github.com/conorheins)<br>- [Alec Tschantz](https://github.com/alec-tschantz)<br>- [Beren Millidge](https://www.beren.io/)<br>- [Brennan Klein](https://github.com/jkbren)<br>others[Arun Niranjan](https://github.com/Arun-Niranjan)<br>[Daphne Demekas](https://github.com/daphnedemekas) | [![](https://camo.githubusercontent.com/1090df52dad4847fd101646152bd3431965b673867107c0661e442d9d877cd01/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e6665722d6163746976656c792f70796d64703f7374796c653d736f6369616c)](https://github.com/infer-actively/pymdp)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.03904)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://pymdp-rtd.readthedocs.io/en/stable/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/infer-actively/pymdp/blob/master/docs/notebooks/active_inference_from_scratch.ipynb) | 19.03.2023 |
| MMSelfSup | Open-source framework for visual pre-training | - [Xiaohang Zhan](https://xiaohangzhan.github.io/)<br>- [Jiahao Xie](https://jiahao000.github.io/)<br>- [Enze Xie](https://xieenze.github.io/)<br>- [Xiangxiang Chu](https://github.com/cxxgtxy)<br>- [Zijian He](https://github.com/scnuhealthy) | [![](https://camo.githubusercontent.com/792d5ed237703c1b3487c5c950b3cb4c6bfdaf19fb6ee23297e33ac5e76e1d16/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e2d6d6d6c61622f6d6d73656c667375703f7374796c653d736f6369616c)](https://github.com/open-mmlab/mmselfsup)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.com/channels/1037617289144569886/1072798105428299817)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mmselfsup.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmengine)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://openmmlab.medium.com/rebirth-openselfsup-is-upgraded-to-mmselfsup-67546f2d9e6)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/mmselfsup/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/OpenMMLab)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/openmmlab) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/open-mmlab/mmselfsup/blob/master/demo/mmselfsup_colab_tutorial.ipynb) | 14.03.2023 |
| Tzer | Coverage-Guided Tensor Compiler Fuzzing with Joint IR-Pass Mutation | - [Jiawei Liu](https://jiawei-site.github.io/)<br>- [Yuxiang Wei](https://yuxiang.cs.illinois.edu/)<br>- [Sen Yang](https://github.com/syang-ng)<br>- [Yinlin Deng](https://dengyinlin.github.io/)<br>- [Lingming Zhang](http://lingming.cs.illinois.edu/) | [![](https://camo.githubusercontent.com/9b737ba965c8721a6ff8093a8acaee024aea5a1f212eabf61a650833c6d8c83b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6973652d756975632f747a65723f7374796c653d736f6369616c)](https://github.com/ise-uiuc/tzer)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.09947)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/repository/docker/tzerbot/oopsla)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://tzer.readthedocs.io/en/latest/index.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ganler/memcov) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ise-uiuc/tzer/blob/main/bug-report.ipynb) | 09.03.2023 |
| ArtLine | A Deep Learning based project for creating line art portraits | [Vijish Madhavan](https://github.com/vijishmadhavan) | [![](https://camo.githubusercontent.com/8d46e0335c65b0b1298a64c11f95f7571e9163824b132b85f23249d825133887/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76696a6973686d6164686176616e2f4172744c696e653f7374796c653d736f6369616c)](https://github.com/vijishmadhavan/ArtLine)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1805.08318), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1710.10196), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1707.02921), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1603.08155)<br>- [data](https://cg.cs.tsinghua.edu.cn/people/~Yongjin/APDrawingDB.zip)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/yiranran/APDrawingGAN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jantic/DeOldify) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/ArtLine/blob/main/ControlNet_%2BArtLine_.ipynb) | 03.03.2023 |
| SAHI | A lightweight vision library for performing large scale object detection & instance segmentation | - [Fatih Cagatay Akyon](https://github.com/fcakyon)<br>- [Sinan Onur ALTINUÇ](https://github.com/sinanonur)<br>- [Alptekin Temizel](https://blog.metu.edu.tr/atemizel/)<br>- [Cemil Cengiz](https://scholar.google.com/citations?user=1Ull07EAAAAJ)<br>others[Devrim Çavuşoğlu](https://github.com/devrimcavusoglu)<br>[Kadir Şahin](https://github.com/ssahinnkadir)<br>[Oğulcan Eryüksel](https://github.com/oulcan) | [![](https://camo.githubusercontent.com/1885819521f9ab2a57cf396f4a28c2f0c90789d079013cac0956684748df2f27/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4943495034363537362e323032322e39383937393930)](https://doi.org/10.1109/ICIP46576.2022.9897990)[![](https://camo.githubusercontent.com/0347894aaa55676cf2eafc5e350d024001d939b4875da57fe15c527b93fa2111/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f6273732f736168693f7374796c653d736f6369616c)](https://github.com/obss/sahi)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.06934)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fcakyon/small-object-detection-benchmark)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/models?pipeline_tag=object-detection&sort=downloads)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/remekkinas/sahi-slicing-aided-hyper-inference-yv5-and-yx)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/codable/sahi-a-vision-library-for-performing-sliced-inference-on-large-images-small-objects-c8b086af3b80), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/codable/convert-any-dataset-to-coco-object-detection-format-with-sahi-95349e1fe2b7) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/obss/sahi/blob/main/demo/inference_for_yolov5.ipynb) | 23.02.2023 |
| AmpliGraph | A suite of neural machine learning models for relational Learning, a branch of machine learning that deals with supervised learning on knowledge graphs | - [Luca Costabello](https://luca.costabello.info/)<br>- [Adrianna Janik](https://github.com/adrijanik)<br>- [Chan Le Van](https://github.com/chanlevan)<br>- [Nicholas McCarthy](https://github.com/NicholasMcCarthy)<br>others[Rory McGrath](http://www.rorymcgrath.ie/)<br>[Sumit Pai](https://github.com/sumitpai) | [![](https://camo.githubusercontent.com/0a1b2831111fea9e8792bd0de429aebfd92033baa24310748cc1cb04c35212b7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f416363656e747572652f416d706c6947726170683f7374796c653d736f6369616c)](https://github.com/Accenture/AmpliGraph)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/1702.05563), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/1705.10744), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.08683), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](http://arxiv.org/abs/1612.03975), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1912.10000), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1412.6575)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.ampligraph.org/)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html), [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gX_KHaU8ChI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Accenture/AmpliGraph/blob/main/docs/tutorials/AmpliGraphBasicsTutorial.ipynb) | 23.02.2023 |
| NMT with attention | This notebook trains a seq2seq model for Spanish to English translation | - [Minh-Thang Luong](https://nlp.stanford.edu/~lmthang/)<br>- [Hieu Pham](https://huyhieupham.github.io/)<br>- [Christopher Manning](https://nlp.stanford.edu/~manning/) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1508.04025), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1409.0473)<br>- [data](http://www.manythings.org/anki/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/text/tutorials/nmt_with_attention)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Neural_machine_translation) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb) | 15.02.2023 |
| GLUE using BERT on TPU | This tutorial contains complete end-to-end code to train models on a TPU | [Anirudh Dubey](https://github.com/anirudh161) | - [GLUE](https://gluebenchmark.com/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.04805)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/tpu), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/text/tutorials/bert_glue) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb) | 15.02.2023 |
| TensorBoard | Suite of web applications for inspecting and understanding your TensorFlow runs and graphs | [Yuan Tang](https://terrytangyuan.github.io/) | [![](https://camo.githubusercontent.com/0c4042affb5d3e2b3416a4c053927496d46176d3ebdf76ad123953f4b08a295e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f74656e736f72626f6172643f7374796c653d736f6369616c)](https://github.com/tensorflow/tensorboard)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tensorboard/get_started), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/summary), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/nn/relu), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/summary_iterator)<br>- [website](https://tensorboard.dev/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Reservoir_sampling)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/eBbEDRsCmv4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BqgTU7_cBnk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qEQ-_EId-D0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/3bownM3L5zM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/scalars_and_keras.ipynb) | 10.02.2023 |
| High-performance Simulation with Kubernetes | This tutorial will describe how to set up high-performance simulation using a TFF runtime running on Kubernetes | [Jason Roselander](https://github.com/roselander) | - [GKE](https://cloud.google.com/kubernetes-engine/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/federated-learning)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-federated/)<br>- [shell](https://cloud.google.com/shell/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/high_performance_simulation_with_kubernetes.ipynb) | 31.01.2023 |
| Compel | Text prompt weighting and blending library for transformers-type text embedding systems | [Damian Stewart](http://damianstewart.com/) | [![](https://camo.githubusercontent.com/2e631e07933636a77954bb87c8de1e89aeac639f583571678bbd21ad6c8134be/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f64616d69616e303831352f636f6d70656c3f7374796c653d736f6369616c)](https://github.com/damian0815/compel)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/invoke-ai/InvokeAI/issues/2832)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/cactusfriend/nightmare-invokeai-prompts) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/damian0815/compel/blob/main/compel-demo.ipynb) | 26.01.2023 |
| DALL·E Flow | An interactive workflow for generating high-definition images from text prompt | - [Han Xiao](https://hanxiao.io/)<br>- [Delgermurun Purevkhuu](https://delgermurun.com/)<br>- [Alex Cureton-Griffiths](http://blog.alexcg.net/) | [![](https://camo.githubusercontent.com/c932b7f184153b9602549d39064b02b57cabe71933737423ca4df9acab440bb2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a696e612d61692f64616c6c652d666c6f773f7374796c653d736f6369616c)](https://github.com/jina-ai/dalle-flow)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Jack000/glid-3-xl), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jina-ai/docarray)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/c/jina-ai) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jina-ai/dalle-flow/blob/main/client.ipynb) | 26.01.2023 |
| Diffusers | Provides pretrained diffusion models across multiple modalities, such as vision and audio, and serves as a modular toolbox for inference and training of diffusion models | [Hugging Face](https://huggingface.co/) | [![](https://camo.githubusercontent.com/5b1fb7f0bf9a83fe4ec33660a8bad74610d3277b44db54a7465a143f7c31d8d4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f6469666675736572733f7374796c653d736f6369616c)](https://github.com/huggingface/diffusers)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.11239), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.11239), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.02502), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.09778), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.13902)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hojonathanho/diffusion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/pesser/pytorch_diffusion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ermongroup/ddim), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/heejkoo/Awesome-Diffusion-Models)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/CompVis/text2img-latent-diffusion), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/CompVis/celeba-latent-diffusion), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/fusing/celeba-diffusion), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/huggingface/diffuse-the-rest), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/Shuang59/Composable-Diffusion)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/hugging-face-just-released-the-diffusers-library-846f32845e65)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/UzkdOg7wWmI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) | 17.01.2023 |
| Sample Factory | One of the fastest RL libraries focused on very efficient synchronous and asynchronous implementations of policy gradients | - [Aleksei Petrenko](https://alex-petrenko.github.io/)<br>- [Zhehui Huang](https://zhehui-huang.github.io/)<br>- [Tushar Kumar](https://github.com/tushartk)<br>- [Gaurav Sukhatme](http://robotics.usc.edu/~gaurav/)<br>- [Vladlen Koltun](http://vladlen.info/) | [![](https://camo.githubusercontent.com/8e8c6dad35501ce81be4ecd8254f28e985dd77e60d4a5d66b867465fe17e6cb7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616c65782d70657472656e6b6f2f73616d706c652d666163746f72793f7374796c653d736f6369616c)](https://github.com/alex-petrenko/sample-factory) <br>- [ICML](http://proceedings.mlr.press/v119/petrenko20a.html)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.11751)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://www.samplefactory.dev/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/alex-petrenko/faster-fifo)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/lLG17LKKSZc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/alex-petrenko/sample-factory/blob/master/sf_examples/notebooks/samplefactory_hub_example.ipynb) | 17.01.2023 |
| Open-Assistant | Chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so | - [Andreas Köpf](https://github.com/andreaskoepf)<br>- [Yannic Kilcher](https://github.com/yk)<br>- [Huu Nguyen](https://github.com/ontocord)<br>- [Christoph Schuhmann](http://christoph-schuhmann.de/)<br>others[Keith Stevens](https://fozziethebeat.github.io/)<br>[Abdullah Barhoum](https://github.com/AbdBarho)<br>[Nguyen Minh Duc](https://github.com/notmd)<br>[Oliver Stanley](https://olliestanley.github.io/)<br>[James Melvin Ebenezer](https://github.com/melvinebenezer) | [![](https://camo.githubusercontent.com/d4cede2e2e5b93a58295ff3543844a79874e157b9f71450c95c7395930eaa72c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4c41494f4e2d41492f4f70656e2d417373697374616e743f7374796c653d736f6369616c)](https://github.com/LAION-AI/Open-Assistant) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.02155)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://projects.laion.ai/Open-Assistant/)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/OpenAssistant)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://generativeai.pub/open-assistant-a-free-and-open-source-alternative-to-chatgpt-67d15229813)<br>- [website](https://open-assistant.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/64Izfm24FKA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/ddG2fM9i4Kk), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/FQIHLFLrTw0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/LAION-AI/Open-Assistant/blob/main/notebooks/data-augmentation/stackexchange-builder/stackexchange-builder.ipynb) | 14.01.2023 |
| panda-gym | Set of robotic environments based on PyBullet physics engine and gymnasium | - [Quentin Gallouédec](https://gallouedec.com/)<br>- [Nicolas Cazin](https://github.com/NicolasCAZIN)<br>- [Emmanuel Dellandréa](http://perso.ec-lyon.fr/emmanuel.dellandrea/)<br>- [Liming Chen](https://sites.google.com/view/limingchen/accueil) | [![](https://camo.githubusercontent.com/2ec76d79d722a38b2f04fb6446d82faecde181264b84ff588628e15ebb38fd14/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7167616c6c6f75656465632f70616e64612d67796d3f7374796c653d736f6369616c)](https://github.com/qgallouedec/panda-gym)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.13687)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://panda-gym.readthedocs.io/en/latest/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/panda-gym/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/BgvpoSP45hA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/qgallouedec/panda-gym/blob/master/examples/PickAndPlace.ipynb) | 02.01.2023 |
| BANMo | Given multiple casual videos capturing a deformable object, BANMo reconstructs an animatable 3D model, including an implicit canonical 3D shape, appearance, skinning weights, and time-varying articulations, without pre-defined shape templates or registered cameras | - [Gengshan Yang](https://gengshan-y.github.io/)<br>- [Minh Vo](https://minhpvo.github.io/)<br>- [Natalia Neverova](https://nneverova.github.io/)<br>- [Deva Ramanan](http://www.cs.cmu.edu/~deva/)<br>others[Andrea Vedaldi](https://www.robots.ox.ac.uk/~vedaldi/)<br>[Hanbyul Joo](https://jhugestar.github.io/) | [![](https://camo.githubusercontent.com/b6d0bca56fd7924e86b61c4eb1e6aed605ee6f651eebbeec28717a6f692736fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f62616e6d6f3f7374796c653d736f6369616c)](https://github.com/facebookresearch/banmo)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.12761)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kwea123/nerf_pl), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/gengshan-y/rigidmask), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ShichenLiu/SoftRas), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ThibaultGROUEIX/ChamferDistancePytorch)<br>- [project](https://banmo-www.github.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1NUa-yvFGA0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/jDTy-liFoCQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1dQJn1vsuz0DkyRZbOA1SulkVQ0V1kMUP) | 30.12.2022 |
| tensor\_parallel | Run large PyTorch models on multiple GPUs in one line of code with potentially linear speedup | [Andrei Panferov](https://blog.panferov.org/) | [![](https://camo.githubusercontent.com/905d12db8a19d185fb01d3dc1cdc5ecfbff32d73dfc610bd4ab1569a4f60203a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f426c61636b53616d6f72657a2f74656e736f725f706172616c6c656c3f7374796c653d736f6369616c)](https://github.com/BlackSamorez/tensor_parallel)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/DeepSpeed), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/fairscale), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/Megatron-LM), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tunib-ai/parallelformers), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/alpa-projects/alpa)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/transformers/model_doc/gpt2)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/blacksamorez/tensor-parallel-int4-llm/), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/muellerzr/multi-gpu-and-accelerate)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensor-parallel/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/BlackSamorez/tensor_parallel/blob/master/examples/training_flan-t5-xl.ipynb) | 29.12.2022 |
| TPU | Reference models and tools for Cloud TPUs | [Google](https://cloud.google.com/) | [![](https://camo.githubusercontent.com/35bafdd89b29b86b86a6191cbebff6bcf18fee45d50e0a3b0dca20ac67522faf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f7470753f7374796c653d736f6369616c)](https://github.com/tensorflow/tpu) <br>- [website](https://cloud.google.com/tpu/)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/W7A-9MYvPwI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/MXxN4fv01c8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/FsxthdQ_sL4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/zEOtG-ChmZE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/kBjYK3K3P6M), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/8j1MWZGNoXM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hszd5UqnfLk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/keras_mnist_tpu.ipynb) | 20.12.2022 |
| rliable | Library for reliable evaluation, even with a handful of runs, on reinforcement learning and machine learnings benchmarks | - [Rishabh Agarwal](https://agarwl.github.io/)<br>- [Max Schwarzer](https://scholar.google.com/citations?user=YmWRSvgAAAAJ)<br>- [Pablo Castro](https://psc-g.github.io/)<br>- [Aaron Courville](https://mila.quebec/en/directory/aaron-courville)<br>- [Marc Bellemare](http://www.marcgbellemare.info/) | [![](https://camo.githubusercontent.com/7805544932645731f590a7c2937ee5f2adb42dd63eb6d83308d51cc912a1b860/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f726c6961626c653f7374796c653d736f6369616c)](https://github.com/google-research/rliable) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://psc-g.github.io/)<br>- [blog post](https://research.google/blog/rliable-towards-reliable-evaluation-reporting-in-reinforcement-learning/), [blog post](https://araffin.github.io/post/rliable/)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html)<br>- [podcast](https://podcasts.apple.com/dk/podcast/deep-reinforcement-learning-at-the-edge-of/id1116303051?i=1000551066163)<br>- [poster](https://agarwl.github.io/rliable/pdfs/Precipice_poster.pdf)<br>- [project](https://agarwl.github.io/rliable/)<br>- [slides](https://agarwl.github.io/rliable/assets/slides_mlc.pdf)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://x.com/agarwl_/status/1432800830621687817)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/XSY9JwqD-bw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/gO33pSls-jI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/HDyK3oNN2i0), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/mqcnHYwWzD8), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/E00gxHrHzZ4), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/M3OzJDAjz3o) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1a0pSD-1tWhMmeJeeoyZM1A-HCW3yf1xR) | 19.12.2022 |
| TF-Agents | A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning | - [Sergio Guadarrama](https://github.com/sguada)<br>- [Anoop Korattikara](https://github.com/kbanoop)<br>- [Oscar Ramirez](https://github.com/oars)<br>- [Pablo Castro](https://psc-g.github.io/)<br>others[Ethan Holly](https://github.com/eholly-g)<br>[Sam Fishman](http://sam.fish/)<br>[Ke Wang](https://scholar.google.com/citations?user=QRYX59sAAAAJ)<br>[Ekaterina Gonina](https://github.com/egonina)<br>[Neal Wu](https://twitter.com/WuNeal)<br>[Efi Kokiopoulou](https://github.com/efiko)<br>[Luciano Sbaiz](https://scholar.google.com/citations?user=fKBmhcUAAAAJ)<br>[Jamie Smith](https://scholar.google.com/citations?user=jk17mo8AAAAJ)<br>[Gábor Bartók](https://github.com/bartokg)<br>[Jesse Berent](https://www.linkedin.com/in/jesse-berent-a1b6875)<br>[Chris Harris](https://www.linkedin.com/in/charris)<br>[Vincent Vanhoucke](https://vincent.vanhoucke.com/)<br>[Eugene Brevdo](https://ebrevdo.github.io/) | [![](https://camo.githubusercontent.com/de012f4eb79d36b96cfdf3828b9eb2774c03585cb49a0749556c12ff48b86b31/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f6167656e74733f7374796c653d736f6369616c)](https://github.com/tensorflow/agents)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://www.tensorflow.org/agents/api_docs/python/tf_agents)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/introduction-to-tf-agents-a-library-for-reinforcement-learning-in-tensorflow-68ab9add6ad6), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/analytics-vidhya/tf-agents-a-flexible-reinforcement-learning-library-for-tensorflow-5f125420f64b)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/agents)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/2nKD6zFQ8xI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-TTziY7EmUA), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/52DTXidSVWc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/U7g7-Jzj9qo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/tAOApRQAgpc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/X4eruXqNbDc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/g0yDlAbi6Pc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/VmZI_YkfPBM), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/7QFSziiAnxI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/0_intro_rl.ipynb) | 15.12.2022 |
| PyG | Library built upon PyTorch to easily write and train Graph Neural Networks for a wide range of applications related to structured data | - [Matthias Fey](https://rusty1s.github.io/#/)<br>- [Jan Eric Lenssen](https://github.com/janericlenssen) | [![](https://camo.githubusercontent.com/2c703cf7cf6bbbb5f4a9ff20a763b6e9af2fa1bd5ae22ed07fa68ff64c1cf484/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7079672d7465616d2f7079746f7263685f67656f6d65747269633f7374796c653d736f6369616c)](https://github.com/pyg-team/pytorch_geometric)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1903.02428), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1801.07829), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1609.02907), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.03123), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1905.05178), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.08566), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.10903), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1905.07953)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://pytorch-geometric.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/snap-stanford/ogb/tree/master/examples), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/pyg-team/pyg-lib), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rusty1s/pytorch_scatter), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rusty1s/pytorch_sparse), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rusty1s/pytorch_cluster), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/AntonioLonga/PytorchGeometricTutorial)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html), [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html), [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://nips.cc/virtual/2020/public/poster_3fe230348e9a12c13120749e3f9fa4cd.html)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLGMXrbDNfqTzqxB1IGgimuhtfAhGd8lHF), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLGMXrbDNfqTwPxitLVHEbT9Pd6-oR_cud), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-UjytpbqX4A) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8) | 08.12.2022 |
| ruGPT3 | Example of inference of RuGPT3XL | [Anton Emelyanov](https://github.com/king-menin) | [![](https://camo.githubusercontent.com/679585fa3acc993e00f385f14a902c6cecf11245c278e07af9c5a33ff2bf6833/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61692d666f72657665722f72752d677074733f7374796c653d736f6369616c)](https://github.com/ai-forever/ru-gpts) <br>- [cristofari](https://sbercloud.ru/ru/christofari)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate)<br>- [sparse attention](https://www.deepspeed.ai/tutorials/sparse-attention/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ai-forever/ru-gpts/blob/master/examples/ruGPT3XL_generation.ipynb) | 07.12.2022 |
| Mubert | Prompt-based music generation via Mubert API | [Ilya Belikov](https://github.com/ferluht) | [![](https://camo.githubusercontent.com/1c256b7ffacbe8eafaf2b662c951a9618e5d6566a72bd7f0f1157e97da70bfe1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4d756265727441492f4d75626572742d546578742d746f2d4d757369633f7374796c653d736f6369616c)](https://github.com/MubertAI/Mubert-Text-to-Music) <br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mubert2.docs.apiary.io/)<br>- [project](https://mubert.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YJu0iXn-T_U), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/5UsaxJsFvAI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/B0kkIpWifG4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ferluht/Mubert-Text-to-Music/blob/main/Mubert_Text_to_Music.ipynb) | 18.10.2022 |
| RuDOLPH | A fast and light text-image-text transformer designed for a quick and easy fine-tuning setup for the solution of various tasks: from generating images by text description and image classification to visual question answering and more | - [Alex Shonenkov](https://github.com/shonenkov)<br>- [Misha Konstantinov](https://github.com/zeroshot-ai) | [![](https://camo.githubusercontent.com/4ea4cfc594204f41b0ce89401a7bc6b5d21b578024c628363c6b033be279356e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61692d666f72657665722f72752d646f6c70683f7374796c653d736f6369616c)](https://github.com/ai-forever/ru-dolph)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.14165), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2102.12092), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.00020)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/rudolph/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ai-forever/ru-dolph/blob/master/jupyters/RUDOLPH_tune_i2t_pl.ipynb) | 06.10.2022 |
| Batch RL | Offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games | - [Rishabh Agarwal](https://agarwl.github.io/)<br>- [Dale Schuurmans](https://webdocs.cs.ualberta.ca/~dale/)<br>- [Mohammad Norouzi](https://norouzi.github.io/) | [![](https://camo.githubusercontent.com/0a49b2ff3bd3c4d78d033cf0baa49597be7d16314560820ecc7580b332335392/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f62617463685f726c3f7374796c653d736f6369616c)](https://github.com/google-research/batch_rl) <br>- [DQN](https://www.nature.com/articles/nature14236?wm=book_wap_0005)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.04543), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1709.06009)<br>- [blog post](https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html)<br>- [data](https://console.cloud.google.com/storage/browser/atari-replay-datasets), [data](https://research.google/resources/datasets/dqn-replay/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/atari-py/tree/0.2.5/atari_py/atari_roms), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mgbellemare/Arcade-Learning-Environment), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mila-iqia/SGI/blob/master/src/offline_dataset.py), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/kzl/decision-transformer/tree/master/atari)<br>- [project](https://offline-rl.github.io/)<br>- [slides](https://docs.google.com/presentation/d/1ROltXr6FIeYKrnGl0tKHGWI0pL4Zo8CnvAK2-cdpQyY)<br>- [talk](https://slideslive.com/38928373/an-optimistic-perspective-on-offline-deep-reinforcement-learning)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/install/install_linux) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1ktlNni_vwFpFtCgUez-RHW0OdGc2U_Wv) | 04.10.2022 |
| EfficientDet | New family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints | - [Mingxing Tan](https://scholar.google.com/citations?user=6POeyBoAAAAJ)<br>- [Ruoming Pang](https://scholar.google.com/citations?user=1fsmwB8AAAAJ)<br>- [Quoc Le](https://cs.stanford.edu/~quocle/) | [![](https://camo.githubusercontent.com/e84d9e5ee1a3139135899cd624f70e3204f1daf1e4de4528961fea44c693d6aa/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3031303739)](https://doi.org/10.1109/CVPR42600.2020.01079)[![](https://camo.githubusercontent.com/8ae44b767fda77289686df670643028ded5f184b89750df099c94ef4bd3e93e5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6175746f6d6c3f7374796c653d736f6369616c)](https://github.com/google/automl/tree/master/efficientdet)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1911.09070), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.13886), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1905.11946), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1804.02767)<br>- [blog post](https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://tfhub.dev/s?network-architecture=efficientdet)<br>- [tutorial](https://cloud.google.com/tpu/docs/tutorials/efficientnet)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/yJg1FX2goCo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/OsA3zH5NKYc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qZobxWXlJ0g) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/automl/blob/master/efficientdet/tf2/tutorial.ipynb) | 27.09.2022 |
| RL Games | High performance RL library | - [Denys Makoviichuk](https://github.com/Denys88)<br>- [Viktor Makoviychuk](https://github.com/ViktorM) | [![](https://camo.githubusercontent.com/4c4d3a9997a30e008095197a105f42be10ef4e0033b73107a89a502891b3ca4f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f44656e797338382f726c5f67616d65733f7374796c653d736f6369616c)](https://github.com/Denys88/rl_games)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/hnYRq7DsQh)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/isaac-sim/IsaacGymEnvs), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/cule), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/tiny-cuda-nn)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/rl-games/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Denys88/rl_games/blob/master/notebooks/brax_training.ipynb) | 27.09.2022 |
| ACME | A library of reinforcement learning components and agents | - [Matt Hoffman](https://www.mwhoffman.com/)<br>- [Bobak Shahriari](https://github.com/bshahr)<br>- [John Aslanides](https://www.aslanides.io/)<br>- [Gabriel Barth-Maron](https://github.com/fastturtle)<br>others[Feryal Behbahani](https://feryal.github.io/)<br>[Tamara Norman](https://github.com/tamaranorman)<br>[Abbas Abdolmaleki](https://scholar.google.com/citations?user=cCYTVWQAAAAJ)<br>[Albin Cassirer](https://github.com/acassirer)<br>[Fan Yang](https://github.com/ddmbr)<br>[Kate Baumli](https://github.com/katebaumli)<br>[Sarah Henderson](https://www.linkedin.com/in/sarah-henderson-agilecoach/)<br>[Alex Novikov](https://scholar.google.ru/citations?user=jMUkLqwAAAAJ)<br>[Sergio Gómez Colmenarejo](https://scholar.google.ru/citations?user=0Dkf68EAAAAJ)<br>[Serkan Cabi](https://scholar.google.ru/citations?&user=l-HhJaUAAAAJ)<br>[Caglar Gulcehre](https://www.caglarg.com/)<br>[Tom Le Paine](http://tomlepaine.github.io/)<br>[Andrew Cowie](https://scholar.google.ru/citations?&user=aTvi5mUAAAAJ)<br>[Ziyu Wang](https://ziyuw.github.io/)<br>[Bilal Piot](https://scholar.google.ru/citations?&user=fqxNUREAAAAJ)<br>[Nando de Freitas](https://github.com/nandodf) | [![](https://camo.githubusercontent.com/d66d01c412cbaca9aa68e70690a9f2417487c8790576ed220114757e66cd8faa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f61636d653f7374796c653d736f6369616c)](https://github.com/deepmind/acme) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.00979)<br>- [blog post](https://www.deepmind.com/publications/acme-a-new-framework-for-distributed-reinforcement-learning)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://dm-acme.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/dm_env)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/NUwDr42bPOw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/J1XCWjuyRaI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/pFMuQWpHI5k) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/acme/blob/master/examples/tutorial.ipynb) | 26.09.2022 |
| RWKV | Reinventing RNNs for the Transformer Era | - [Bo Peng](https://github.com/BlinkDL)<br>- [Eric Alcaide](https://hypnopump.github.io/)<br>- [Quentin Anthony](https://quentin-anthony.github.io/)<br>- [Alon Albalak](https://alon-albalak.github.io/)<br>others[Samuel Arcadinho](https://github.com/SSamDav)<br>[Matteo Grella](http://www.matteogrella.com/)<br>[Kranthi Kiran](https://kranthigv.github.io/)<br>[Haowen Hou](https://github.com/howard-hou)<br>[Przemyslaw Kazienko](https://kazienko.eu/en)<br>[Jan Kocon](https://github.com/KoconJan)<br>[Bartlomiej Koptyra](https://github.com/bkoptyra)<br>[Ipsit Mantri](https://ipsitmantri.github.io/)<br>[Ferdinand Mom](https://3outeille.github.io/)<br>[Xiangru Tang](https://github.com/tangxiangru)<br>[Johan Wind](https://johanwind.github.io/)<br>[Stanisław Woźniak](https://www.researchgate.net/profile/Stanislaw-Wozniak-3)<br>[Qihang Zhao](https://www.researchgate.net/profile/Qihang-Zhao-2)<br>[Peng Zhou](https://pengzhou.sites.ucsc.edu/)<br>[Jian Zhu](https://lingjzhu.github.io/)<br>[Rui-Jie Zhu](https://scholar.google.com/citations?user=08ITzJsAAAAJ) | [![](https://camo.githubusercontent.com/53e349fb80b401dd45a89ac8447896438992b361d44decf825e752f1159cab3f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f426c696e6b444c2f52574b562d4c4d3f7374796c653d736f6369616c)](https://github.com/BlinkDL/RWKV-LM)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.13048), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.14103), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2002.05202)<br>- [data](https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip)<br>- [demo](https://josephrocca.github.io/rwkv-v4-web/demo/)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/bDSBUMeFpc)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/saharNooby/rwkv.cpp), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cgisky1980/ai00_rwkv_server), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/harrisonvanderbyl/rwkv-cpp-cuda), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Blealtan/RWKV-LM-LoRA), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/TheRamU/Fay/blob/main/README_EN.md), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ridgerchu/SpikeGPT), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BlinkDL/RWKV-v2-RNN-Pile/tree/main/RWKV-v3), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BlinkDL/SmallInitEmb), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BlinkDL/RWKV-CUDA), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BlinkDL/minGPT-tuned)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/BlinkDL), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/BlinkDL/clip-guided-binary-autoencoder)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/umq908/r_rwkvv2rnn_a_parallelizable_rnn_with/)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/BlinkDL_AI), [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/HochreiterSepp/status/1524270961314484227)<br>- [website](https://www.rwkv.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/x8pW19wKfXQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/B3Qa2rRsaXo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/w-xydM6C6Qc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM) | 21.09.2022 |
| NetKet | Open-source project delivering cutting-edge methods for the study of many-body quantum systems with artificial neural networks and machine learning techniques | - [Filippo Vicentini](https://filippovicentini.com/)<br>- [Damian Hofmann](https://github.com/femtobit)<br>- [Attila Szabó](https://github.com/attila-i-szabo)<br>- [Dian Wu](https://github.com/wdphy16)<br>others[Christopher Roth](https://github.com/chrisrothUT)<br>[Clemens Giuliani](https://github.com/inailuig)<br>[Gabriel Pescia](https://github.com/gpescia)<br>[Jannes Nys](https://github.com/jwnys)<br>[Vladimir Vargas-Calderón](https://github.com/VolodyaCO)<br>[Nikita Astrakhantsev](https://github.com/nikita-astronaut)<br>[Giuseppe Carleo](https://github.com/gcarleo)<br>[Kenny Choo](https://github.com/kchoo1118)<br>[James Smith](https://jamesetsmith.github.io/)<br>[Tom Westerhout](https://github.com/twesterhout)<br>[Fabien Alet](https://github.com/fabienalet)<br>[Emily Davis](https://github.com/emilyjd)<br>[Stavros Efthymiou](https://github.com/stavros11)<br>[Ivan Glasser](https://www.researchgate.net/profile/Ivan-Glasser)<br>[Sheng-Hsuan Lin](https://shhslin.github.io/)<br>[Marta Mauri](https://github.com/martamau)<br>[Mazzola Guglielmo](https://www.ics.uzh.ch/en/research/research-groups/Guglielmo-Mazzola0.html)<br>[Christian Mendl](http://christian.mendl.net/)<br>[Evert Nieuwenburg](https://evert.info/)<br>[Ossian O'Reilly](https://github.com/ooreilly)<br>[Hugo Théveniaut](https://github.com/theveniaut)<br>[Giacomo Torlai](https://github.com/GTorlai)<br>[Alexander Wietek](https://awietek.github.io/) | [![](https://camo.githubusercontent.com/47c94a7eacbeadb23d660b9964e99f3693b01ce07b48d84eef054b624d229ae6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e65746b65742f6e65746b65743f7374796c653d736f6369616c)](https://github.com/netket/netket)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.10526)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://netket.readthedocs.io/en/latest/index.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mpi4jax/mpi4jax), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/cloudhan/jax-windows-builder)<br>- [website](https://www.netket.org/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Ryz-o71tuy8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/PhilipVinc/Lectures/blob/main/2202_NetKet/01_intro.ipynb) | 15.09.2022 |
| Stable Diffusion | A latent text-to-image diffusion model | - [Robin Rombach](https://github.com/rromb)<br>- [Andreas Blattmann](https://github.com/ablattmann)<br>- [Dominik Lorenz](https://github.com/qp-qp)<br>- [Patrick Esser](https://github.com/pesser)<br>- [Björn Ommer](https://ommer-lab.com/people/ommer/) | [![](https://camo.githubusercontent.com/457ff305a9229a76b0f4753eb095da3e4fbefbadf1d234e9ee2a9ea03041c596/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6d705669732f737461626c652d646966667573696f6e3f7374796c653d736f6369616c)](https://github.com/CompVis/stable-diffusion)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2205.11487), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.12598), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.09778), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.01073)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://arxiv.org/abs/2112.10752), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/christophschuhmann/improved-aesthetic-predictor), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ShieldMnt/invisible-watermark), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/guided-diffusion), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/denoising-diffusion-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/x-transformers)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/CompVis), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/laion/laion2B-en), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/datasets/laion/laion-high-resolution) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/CompVis/stable-diffusion/blob/main/scripts/latent_imagenet_diffusion.ipynb) | 10.08.2022 |
| Deep-MAC | Welcome to the Novel class segmentation demo | [Vighnesh Birodkar](http://vighneshbirodkar.github.io/) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.00613)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/method/deep-mac) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/deepmac_colab.ipynb) | 10.08.2022 |
| NL-Augmenter | A collaborative effort intended to add transformations of datasets dealing with natural language | - [Aadesh Gupta](https://github.com/aadesh11)<br>- [Timothy Sum Hon Mun](https://github.com/timothy22000)<br>- [Aditya Srivatsa](https://github.com/kvadityasrivatsa)<br>- [Xudong Shen](https://github.com/XudongOliverShen)<br>others[Juan Diego Rodriguez](https://github.com/juand-r)<br>[Ashish Shrivastava](https://github.com/ashish3586)<br>[Nagender Aneja](https://researchid.co/naneja)<br>[Zijie Wang](https://zijie.wang/)<br>[Yiwen Shi](https://github.com/Yiwen-Shi)<br>[Afnan Mir](https://github.com/afnanmmir)<br>[William Soto](https://github.com/sotwi)<br>[Chandan Singh](https://csinva.io/)<br>[Claude Roux](https://github.com/ClaudeRoux)<br>[Abinaya Mahendiran](https://github.com/AbinayaM02)<br>[Anna Shvets](https://github.com/asnota)<br>[Kaustubh Dhole](https://github.com/kaustubhdhole)<br>[Bryan Wilie](https://github.com/bryanwilie)<br>[Jamie Simon](https://james-simon.github.io/)<br>[Mukund Varma](https://github.com/MukundVarmaT)<br>[Sang Han](https://github.com/jjangsangy)<br>[Denis Kleyko](https://github.com/denkle)<br>[Samuel Cahyawijaya](https://github.com/SamuelCahyawijaya)<br>[Filip Cornell](https://github.com/Filco306)<br>[Tanay Dixit](https://tanay2001.github.io/)<br>[Connor Boyle](https://github.com/boyleconnor)<br>[Genta Indra Winata](https://gentawinata.com/)<br>[Seungjae Ryan Lee](https://github.com/seungjaeryanlee)<br>[Marcin Namysl](https://github.com/mnamysl)<br>[Roman Sitelew](https://github.com/RomanPlusPlus)<br>[Zhenhao Li](https://zhenhaoli.net/)<br>[Fiona Tan](https://tanfiona.github.io/) | [![](https://camo.githubusercontent.com/2f07aa7132dbd774d6ecb611d36f543aa074b6cfb3df1caf78a34a5260abdba8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f47454d2d62656e63686d61726b2f4e4c2d4175676d656e7465723f7374796c653d736f6369616c)](https://github.com/GEM-benchmark/NL-Augmenter) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.02721)<br>- [website](https://gem-benchmark.com/nl_augmenter) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/GEM-benchmark/NL-Augmenter/blob/main/notebooks/Write_a_sample_transformation.ipynb) | 06.08.2022 |
| Accelerate | A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision | [Hugging Face](https://huggingface.co/) | [![](https://camo.githubusercontent.com/ca6009c69da73bc2b872924cb0b9586004ef02c2c11e70a4a035cacd52b0f7ec/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68756767696e67666163652f616363656c65726174653f7374796c653d736f6369616c)](https://github.com/huggingface/accelerate)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://huggingface.co/docs/accelerate/index) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/accelerate/simple_nlp_example.ipynb) | 27.07.2022 |
| YOLOv5 on Custom Objects | This notebook shows training on your own custom objects | [Jacob Solawetz](https://blog.roboflow.com/author/jacob/) | - [blog post](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/)<br>- [data](https://public.roboflow.ai/object-detection/bccd) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ) | 20.07.2022 |
| MindsEye | Graphical user interface built to run multimodal ai art models for free from a Google Colab, without needing edit a single line of code or know any programming | - [multimodal.art](https://multimodal.art/)<br>- [João Paulo Apolinário Passos](http://www.apolinariopassos.com.br/portfolio/) | [![](https://camo.githubusercontent.com/9d6d28996994ed500b75509df21d2210f2dd3a63ca50cc66678af404aa34cfc4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d756c74696d6f64616c6172742f6d696e64736579653f7374796c653d736f6369616c)](https://github.com/multimodalart/mindseye) <br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/guided-diffusion)<br>- [project](https://multimodal.art/mindseye) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1cg0LZ5OfN9LAIB37Xq49as0fSJxcKtC5) | 06.07.2022 |
| py-irt | Fitting Item Response Theory models using variational inference | - [John Lalor](https://jplalor.github.io/)<br>- [Hong Yu](https://scholar.google.com/citations?user=TyXe64wAAAAJ)<br>- [Pedro Rodriguez](https://www.pedro.ai/)<br>- [Joe Barrow](https://jbarrow.ai/)<br>others[Alexander Hoyle](https://alexanderhoyle.com/)<br>[Robin Jia](https://robinjia.github.io/)<br>[Jordan Boyd-Graber](https://github.com/ezubaric) | [![](https://camo.githubusercontent.com/af5ac1c16111dfcb1030f9a9d6e1e14c11818ddfe658ba567db9b02a51e84678/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f323032312e61636c2d6c6f6e672e333436)](https://doi.org/10.18653/v1/2021.acl-long.346) [![](https://camo.githubusercontent.com/9b7c00e31013a132ecc0b70f1efb285d463951b82bd56b37c873e56923fbebc3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e642d62616c6c2f70792d6972743f7374796c653d736f6369616c)](https://github.com/nd-ball/py-irt) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1908.11421)<br>- [paper](https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01422/full)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/akUxtt21Mlc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/nd-ball/py-irt/blob/master/examples/py-irt_example.ipynb) | 30.06.2022 |
| BIG-bench | A collaborative benchmark intended to probe large language models and extrapolate their future capabilities | - [Jaehoon Lee](https://jaehlee.github.io/)<br>- [Jascha Sohl-Dickstein](http://www.sohldickstein.com/)<br>- [Vinay Ramasesh](https://ramasesh.github.io/)<br>- [Sajant Anand](https://github.com/sajantanand)<br>others[Alicia Parrish](https://aliciaparrish.com/)<br>[Ethan Dyer](https://github.com/ethansdyer)<br>[Liam Dugan](http://liamdugan.com/)<br>[Dieuwke Hupkes](https://github.com/dieuwkehupkes)<br>[Daniel Freeman](https://github.com/cdfreeman-google)<br>[Guy Gur-Ari](https://github.com/guygurari)<br>[Aitor Lewkowycz](https://github.com/lewkowycz) | [![](https://camo.githubusercontent.com/e4dcb6f3a8f5ddf428835aa64c8a9936a6b1cd19f2bc29e3a2a982ab6aa7edf8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f4249472d62656e63683f7374796c653d736f6369616c)](https://github.com/google/BIG-bench) <br>- [API](https://google.github.io/BIG-bench/docs/html/bigbench/index.html)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2206.04615) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/BIG-bench/blob/master/notebooks/colab_examples.ipynb) | 28.06.2022 |
| HuggingArtists | Choose your favorite Artist and train a language model to write new lyrics based on their unique voice | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![](https://camo.githubusercontent.com/c7943ff8fa049280fc249aa4427eb1b309bf485f94b3c4254a1e09f4180b5163/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f416c656b7365794b6f727368756b2f68756767696e67617274697374733f7374796c653d736f6369616c)](https://github.com/AlekseyKorshuk/huggingartists)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/AlekseyKorshuk/huggingartists), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/huggingartists) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) | 25.06.2022 |
| Introduction to the TensorFlow Models NLP library | You will learn how to build transformer-based models for common NLP tasks including pretraining, span labelling and classification using the building blocks from NLP modeling library | [Chen Chen](https://github.com/chenGitHuber) | [![](https://camo.githubusercontent.com/a05c6a10141aa5829d7218a8c698391289a603709741d593b3f9f8005a8ce9e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f6d6f64656c733f7374796c653d736f6369616c)](https://github.com/tensorflow/models/tree/master/official/nlp/modeling)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.04805) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/nlp/nlp_modeling_library_intro.ipynb) | 22.06.2022 |
| Cirq | A python framework for creating, editing, and invoking Noisy Intermediate Scale Quantum circuits | - [Balint Pato](https://refactorium.com/)<br>- [Matthew Harrigan](https://mpharrigan.com/)<br>- [Animesh Sinha](https://github.com/AnimeshSinha1309)<br>- [Matthew Neeley](https://github.com/maffoo)<br>others[Dave Bacon](https://dabacon.org/)<br>[Matteo Pompili](https://github.com/matpompili)<br>[Michael Broughton](https://github.com/MichaelBroughton) | [![](https://camo.githubusercontent.com/99357f7e316d3bfc0ea344a228e1db66cfa71b0b66e91cbe1789e00a922273e0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7175616e74756d6c69622f436972713f7374796c653d736f6369616c)](https://github.com/quantumlib/Cirq)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Quantum_logic_gate#Hadamard_gate)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/16ZfkPRVf2w) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/quantumlib/Cirq/blob/master/docs/tutorials/basics.ipynb) | 21.06.2022 |
| CLIP-as-service | A low-latency high-scalability service for embedding images and text | [Han Xiao](https://hanxiao.io/) | [![](https://camo.githubusercontent.com/ef63ebb1ffdf5414351abdc4b37d05432d200a2556cd304c2f92c80d6d9ad7dc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a696e612d61692f636c69702d61732d736572766963653f7374796c653d736f6369616c)](https://github.com/jina-ai/clip-as-service) <br>- [data](https://sites.google.com/view/totally-looks-like-dataset)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jina-ai/docarray)<br>- [website](https://clip-as-service.jina.ai/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/c/jina-ai) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jina-ai/clip-as-service/blob/main/docs/hosting/cas-on-colab.ipynb) | 19.06.2022 |
| TPOT | Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming | - [Jason Moore](https://epistasis.org/jason-h-moore-phd/)<br>- [Randal Olson](https://randalolson.com/)<br>- [Ryan Urbanowicz](https://ryanurbanowicz.com/) | [![](https://camo.githubusercontent.com/36e4faec6d83d933f9d1d64db7902b182d3f8c09b7505ab94f0e7f3f9b5bac9b/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033302d30353331382d355f38)](https://doi.org/10.1007/978-3-030-05318-5_8)[![](https://camo.githubusercontent.com/e39ca63c2b82e05e84622da7f74a534b71e267f329ebc546f997eba348a886b5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4570697374617369734c61622f74706f743f7374796c653d736f6369616c)](https://github.com/EpistasisLab/tpot)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://epistasislab.github.io/tpot/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/TPOT/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YzRpjfUCFsQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/25lQ-JbN6Mw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/cH4LrTdnUrY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/olOgCv_Ej6A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/PR82ABIwMds) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/EpistasisLab/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb) | 15.06.2022 |
| Jina | MLOps framework that empowers anyone to build cross-modal and multi-modal applications on the cloud | [Han Xiao](https://hanxiao.io/) | [![](https://camo.githubusercontent.com/ebbca4d78a2545ffffd8575b16431ca56d5c27a3b57fd68439e4189a50fc80cd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a696e612d61692f6a696e613f7374796c653d736f6369616c)](https://github.com/jina-ai/jina) <br>- [data](https://sites.google.com/view/totally-looks-like-dataset)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.jina.ai/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jina-ai/example-grafana-prometheus/blob/main/grafana-dashboards/flow.json)<br>- [hub](https://hub.jina.ai/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/c/jina-ai) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/jina-ai/jina/blob/master/docs/Using_Jina_on_Colab.ipynb) | 11.06.2022 |
| MMRotate | Toolbox for rotated object detection based on PyTorch | - [Yue Zhou](https://zytx121.github.io/)<br>- [Xue Yang](https://yangxue0827.github.io/)<br>- [Gefan Zhang](https://github.com/zhanggefan)<br>- [Jiabao Wang](https://jbwang1997.github.io/)<br>others[Yanyi Liu](https://github.com/liuyanyi)<br>[Liping Hou](https://scholar.google.com/citations?user=XoEzZukAAAAJ)<br>[Xue Jiang](https://dl.acm.org/profile/99659833933)<br>[Xingzhao Liu](https://dl.acm.org/profile/81430639972)<br>[Junchi Yan](https://thinklab.sjtu.edu.cn/)<br>[Chengqi Lyu](https://scholar.google.com/citations?user=kV3WvXcAAAAJ)<br>[Wenwei Zhang](https://zhangwenwei.cn/)<br>[Kai Chen](https://chenkai.site/) | [![](https://camo.githubusercontent.com/59bcd41886baca0f233a9d0163d55cfde27b9095cf3e46b0e9c62690602852e1/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333530333136312e33353438353431)](https://doi.org/10.1145/3503161.3548541)[![](https://camo.githubusercontent.com/7c06e40cb5f399e5c19731454ce6191a08d59f3d9bcc46625b28774e951b8332/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e2d6d6d6c61622f6d6d726f746174653f7374796c653d736f6369616c)](https://github.com/open-mmlab/mmrotate)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.13317)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://mmrotate.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/open-mmlab/mmcv)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/real-time-instance-segmentation-on-mscoco?p=rtmdet-an-empirical-study-of-designing-real), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/object-detection-in-aerial-images-on-hrsc2016?p=rtmdet-an-empirical-study-of-designing-real), [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/sota/object-detection-in-aerial-images-on-dota-1?p=rtmdet-an-empirical-study-of-designing-real)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/mmrotate)<br>- [website](https://openmmlab.com/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/hKZUV0AySNk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/open-mmlab/mmrotate/blob/main/demo/MMRotate_Tutorial.ipynb) | 10.06.2022 |
| Aesthetics Predictor | A linear estimator on top of clip to predict the aesthetic quality of pictures | [LAION AI](https://laion.ai/) | [![](https://camo.githubusercontent.com/983c7701a7b59871400086a2c9ce9a85c6cdfdffb20e30456a91ebe0ad7405f0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4c41494f4e2d41492f6165737468657469632d707265646963746f723f7374796c653d736f6369616c)](https://github.com/LAION-AI/aesthetic-predictor) <br>- [blog post](https://laion.ai/blog/laion-aesthetics/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rom1504/embedding-reader/blob/main/examples/aesthetic_inference.py)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/discussions/general/464229) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/LAION-AI/aesthetic-predictor/blob/main/asthetics_predictor.ipynb) | 04.06.2022 |
| Flashlight | Fast, flexible machine learning library written entirely in C++ | - [Jacob Kahn](https://jacobkahn.me/)<br>- [Vineel Pratap](https://github.com/vineelpratap)<br>- [Tatiana Likhomanenko](https://github.com/tlikhomanenko)<br>- [Qiantong Xu](https://github.com/xuqiantong)<br>others[Awni Hannun](https://awnihannun.com/)<br>[Jeff Cai](https://ieeexplore.ieee.org/author/37086866180)<br>[Paden Tomasello](https://github.com/padentomasello)<br>[Ann Lee](https://scholar.google.com/citations?user=Am6PakYAAAAJ)<br>[Edouard Grave](https://github.com/EdouardGrave)<br>[Gilad Avidov](https://github.com/avidov)<br>[Benoit Steiner](http://bsteiner.info/)<br>[Vitaliy Liptchinsky](https://scholar.google.com/citations?user=zl4dA-gAAAAJ)<br>[Gabriel Synnaeve](https://syhw.github.io/)<br>[Ronan Collobert](https://ronan.collobert.com/) | [![](https://camo.githubusercontent.com/55f8f4c3fae4c98762b0f02fd43c8391abe0037fd5959033ca9876033f863430/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f666c6173686c696768742f666c6173686c696768743f7374796c653d736f6369616c)](https://github.com/flashlight/flashlight)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.12465)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://hub.docker.com/r/flml/flashlight/tags?page=1&ordering=last_updated&name=cuda-latest)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://fl.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/arrayfire/arrayfire), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/vcpkg), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/arrayfire/arrayfire-ml/), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nvidia/cub), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/USCiLab/cereal), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nothings/stb), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookincubator/gloo), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/oneapi-src/oneDNN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google/glog), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/gflags/gflags), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/flashlight/text) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/flashlight/flashlight/blob/master/flashlight/app/asr/tutorial/notebooks/FinetuneCTC.ipynb) | 01.06.2022 |
| RL Unplugged | Suite of benchmarks for offline reinforcement learning | - [Caglar Gulcehre](https://www.caglarg.com/)<br>- [Ziyu Wang](https://ziyuw.github.io/)<br>- [Alexander Novikov](https://scholar.google.com/citations?user=jMUkLqwAAAAJ)<br>- [Tom Le Paine](http://tomlepaine.github.io/)<br>others[Sergio Gómez Colmenarejo](https://scholar.google.com/citations?user=0Dkf68EAAAAJ)<br>[Konrad Żołna](https://github.com/kondiz)<br>[Rishabh Agarwal](https://agarwl.github.io/)<br>[Josh Merel](https://sites.google.com/site/jsmerel/)<br>[Daniel Mankowitz](https://danielmankowitz.wixsite.com/danielm)<br>[Cosmin Paduraru](https://scholar.google.com/citations?user=oz4Ca9AAAAAJ)<br>[Gabriel Dulac-Arnold](http://gabe.squirrelsoup.net/)<br>[Jerry Li](https://github.com/jerryli27)<br>[Mohammad Norouzi](https://norouzi.github.io/)<br>[Matt Hoffman](https://www.mwhoffman.com/)<br>[Ofir Nachum](https://scholar.google.com/citations?user=C-ZlBWMAAAAJ)<br>[George Tucker](https://sites.google.com/view/gjt)<br>[Nicolas Heess](https://scholar.google.com/citations?user=79k7bGEAAAAJ)<br>[Nando de Freitas](https://github.com/nandodf) | [![](https://camo.githubusercontent.com/84e2ed71d8b746b7df87e90d2aebee940aec2e3dbec7ac93fba00b0f1d46f326/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f646565706d696e642d72657365617263683f7374796c653d736f6369616c)](https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.13888), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.04543), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1709.06009), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1811.09656), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1811.11711), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.12238), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1911.09451), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1801.00690), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.11881), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.09575)<br>- [data](https://console.cloud.google.com/storage/browser/rl_unplugged)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/lab), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-research/realworldrl_suite#installation)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/n8yNYzbUMJ0) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/deepmind_research/blob/master/rl_unplugged/dmlab_r2d2.ipynb) | 26.05.2022 |
| Scenic | Codebase with a focus on research around attention-based models for computer vision | - [Mostafa Dehghani](https://www.mostafadehghani.com/)<br>- [Alexey Gritsenko](https://github.com/AlexeyG)<br>- [Anurag Arnab](https://github.com/anuragarnab)<br>- [Matthias Minderer](https://matthias.minderer.net/)<br>- [Yi Tay](https://vanzytay.github.io/) | [![](https://camo.githubusercontent.com/3ef73c3399daddd038b59070ae4937f95e14a097b443ae87f0335b5ee0e8cf12/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3032303730)](https://doi.org/10.1109/CVPR52688.2022.02070)[![](https://camo.githubusercontent.com/6cf0b227c4b12adec17a5192800a6d67fe3069d6602b616419ac9193036a991c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f7363656e69633f7374796c653d736f6369616c)](https://github.com/google-research/scenic)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2110.11403)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/syncedreview/google-open-sources-scenic-a-jax-library-for-rapid-computer-vision-model-prototyping-and-894dbdeddbae)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/deeplearning/comments/qgyjck/r_google_opensources_scenic_a_jax_library_for/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-research/scenic/blob/main/scenic/common_lib/colabs/scenic_playground.ipynb) | 04.05.2022 |
| Text generation with RNN | This tutorial demonstrates how to generate text using a character-based RNN | [Anirudh Dubey](https://github.com/anirudh161) | - [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/text-generation)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/text/tutorials/text_generation) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb) | 03.05.2022 |
| CLIPDraw | Synthesize drawings to match a text prompt | - [Kevin Frans](https://www.kvfrans.com/)<br>- [Lisa Soros](https://scholar.google.com/citations?user=iUkpvMUAAAAJ)<br>- [Olaf Witkowski](https://olafwitkowski.com/) | [![](https://camo.githubusercontent.com/0e852568eaac6cdaf63339ad8d0d2ec3709339ca696fc74af82ff2c22791c98a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b766672616e732f636c6970647261773f7374796c653d736f6369616c)](https://github.com/kvfrans/clipdraw)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.14843), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1508.06576), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2105.00162)<br>- [blog post](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/BachiLi/diffvg/blob/master/apps/painterly_rendering.py) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb) | 29.04.2022 |
| DeepLab2 | Library for deep labeling, aiming to provide a unified and state-of-the-art TensorFlow codebase for dense pixel labeling tasks, including, but not limited to semantic segmentation, instance segmentation, panoptic segmentation, depth estimation, or even video panoptic segmentation | - [Mark Weber](https://github.com/markweberdev)<br>- [Huiyu Wang](https://github.com/csrhddlam)<br>- [Siyuan Qiao](https://www.cs.jhu.edu/~syqiao/)<br>- [Jun Xie](https://github.com/ClaireXie)<br>others[Maxwell Collins](https://pages.cs.wisc.edu/~mcollins/)<br>[Yukun Zhu](https://github.com/markweberdev)<br>[Liangzhe Yuan](https://github.com/yuanliangzhe)<br>[Dahun Kim](https://mcahny.github.io/)<br>[Qihang Yu](https://yucornetto.github.io/)<br>[Daniel Cremers](https://cvg.cit.tum.de/members/cremers)<br>[Laura Leal-Taixé](https://dvl.in.tum.de/team/lealtaixe/)<br>[Alan Yuille](https://www.cs.jhu.edu/~ayuille/index.html)<br>[Florian Schroff](https://www.florian-schroff.de/)<br>[Hartwig Adam](https://scholar.google.com/citations?user=fWd88tEAAAAJ)<br>[Liang-Chieh Chen](http://liangchiehchen.com/) | [![](https://camo.githubusercontent.com/a3b737b476a326ff20885baa36fa161b12ac7fe909d8d3a26ed1a2d63ddc9a05/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f646565706c6162323f7374796c653d736f6369616c)](https://github.com/google-research/deeplab2)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.09748), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2206.07704), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2207.04044)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/od2csk/p_deeplab2_a_tensorflow_library_for_deep_labeling/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-research/deeplab2/blob/main/ViP_DeepLab_Demo.ipynb) | 27.04.2022 |
| CodeGen | Family of open-source model for program synthesis | - [Erik Nijkamp](https://eriknijkamp.com/)<br>- [Bo Pang](https://scholar.google.com/citations?user=s9fNEVEAAAAJ)<br>- [Hiroaki Hayashi](https://hiroakih.me/)<br>- [Lifu Tu](https://lifu-tu.github.io/)<br>others[Huan Wang](https://huan-december.github.io/)<br>[Yingbo Zhou](https://scholar.google.com/citations?user=H_6RQ7oAAAAJ)<br>[Silvio Savarese](https://cvgl.stanford.edu/silvio/)<br>[Caiming Xiong](http://cmxiong.com/) | [![](https://camo.githubusercontent.com/88c958cff5bd61d813e568ce7f6fadb275966976588319c97e87c01867ad510a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73616c6573666f7263652f436f646547656e3f7374796c653d736f6369616c)](https://github.com/salesforce/CodeGen)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2203.13474), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2305.02309)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/salesforce/jaxformer)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/models?search=salesforce+codegen) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1fQI8OgzMAR0bquCrvhlAtXSw6iMFbVgI) | 23.04.2022 |
| Jraph | library for graph neural networks in jax | - [Jonathan Godwin](https://github.com/jg8610)<br>- [Thomas Keck](https://github.com/thomaskeck)<br>- [Peter Battaglia](https://scholar.google.com/citations?user=nQ7Ij30AAAAJ)<br>- [Victor Bapst](https://linkedin.com/in/victor-bapst-73430a89)<br>others[Thomas Kipf](https://tkipf.github.io/)<br>[Yujia Li](https://yujiali.github.io/)<br>[Kimberly Stachenfeld](https://neurokim.com/)<br>[Petar Veličković](https://petar-v.com/)<br>[Alvaro Sanchez-Gonzalez](https://github.com/alvarosg) | [![](https://camo.githubusercontent.com/c4d53a6a332380d14d2ff813c397104125e3441dc2110a65789428b26b7b6b9e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d646565706d696e642f6a726170683f7374796c653d736f6369616c)](https://github.com/google-deepmind/jraph)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1806.01261)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://jraph.readthedocs.io/en/latest/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/S3sRy4oqvCM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb) | 15.04.2022 |
| deep-significance | Easy-to-use package containing different significance tests and utility functions specifically tailored towards research needs and usability | - [Dennis Ulmer](http://dennisulmer.eu/)<br>- [Christian Hardmeier](https://christianhardmeier.rax.ch/)<br>- [Jes Frellsen](https://frellsen.org/) | [![](https://camo.githubusercontent.com/39fa83ddbb6e164bad1369d445ce9dc52366dc50ea07cca69bf56ce7d1d333e5/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f7031392d31323636)](https://doi.org/10.18653/v1/p19-1266) [![](https://camo.githubusercontent.com/a57eae76f7d255ea8bb01f37d9618e221d3a2d4ab1389635ac19c5195984be2c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b616c6569646f70686f6e2f646565702d7369676e69666963616e63653f7374796c653d736f6369616c)](https://github.com/Kaleidophon/deep-significance) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2204.06815)<br>- [blog post](https://machinelearningmastery.com/statistical-hypothesis-tests/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://deep-significance.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rtmdrr/replicability-analysis-NLP), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rtmdrr/testSignificanceNLP), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rtmdrr/DeepComparison)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Multiple_comparisons_problem) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/Kaleidophon/deep-significance/blob/main/paper/deep-significance%20demo.ipynb) | 12.04.2022 |
| Text classification with RNN | This text classification tutorial trains a recurrent neural network on the IMDB large movie review dataset for sentiment analysis | [Anirudh Dubey](https://github.com/anirudh161) | - [data](http://ai.stanford.edu/~amaas/data/sentiment/)<br>- [link](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/text-classification) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb) | 17.03.2022 |
| TriMap | Dimensionality reduction technique based on triplet constraints, which preserves the global structure of the data better than the other commonly used methods such as t-SNE, LargeVis, and UMAP | - [Ehsan Amid](https://sites.google.com/view/eamid/)<br>- [Manfred Warmuth](https://mwarmuth.bitbucket.io/) | [![](https://camo.githubusercontent.com/de5ef3b159d9aaa25501dd20be7dd5afc005415c935c35b0cdc7c5a222858b33/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f65616d69642f7472696d61703f7374796c653d736f6369616c)](https://github.com/eamid/trimap) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.00204)<br>- [data](https://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-research/google-research/tree/master/trimap), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/spotify/annoy), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zalandoresearch/fashion-mnist)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/MNIST_database) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/eamid/examples/blob/master/TriMap.ipynb#scrollTo=nSyGA-ymB-jN) | 17.03.2022 |
| RLDS | Reinforcement Learning Datasets and it is an ecosystem of tools to store, retrieve and manipulate episodic data in the context of Sequential Decision Making including RL, Learning for Demonstrations, Offline RL or Imitation Learning | - [Sabela Ramos](https://github.com/sabelaraga)<br>- [Sertan Girgin](https://sites.google.com/site/girgint/home)<br>- [Léonard Hussenot](https://leonardhussenot.github.io/)<br>- [Damien Vincent](https://www.linkedin.com/in/damien-vincent-1958381)<br>others[Hanna Yakubovich](https://github.com/yakubanna)<br>[Daniel Toyama](https://github.com/kenjitoyama)<br>[Anita Gergely](https://www.linkedin.com/in/anita-g-318064b2/)<br>[Piotr Stanczyk](https://scholar.google.com/citations?user=fKVK0dYAAAAJ)<br>[Raphaël Marinier](https://github.com/RaphaelMarinier)<br>[Jeremiah Harmsen](https://github.com/jharmsen)<br>[Olivier Pietquin](https://research.google/people/105812/)<br>[Nikola Momchev](https://scholar.google.com/citations?user=PbWgaswAAAAJ) | [![](https://camo.githubusercontent.com/4c716714eeeda26ecd419e917182c5c15ffb733010f0b1c4a9f3efab9d00bfdd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f726c64733f7374796c653d736f6369616c)](https://github.com/google-research/rlds) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.02767)<br>- [blog post](https://ai.googleblog.com/2021/12/rlds-ecosystem-to-generate-share-and.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/envlogger), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/google-research/rlds-creator), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Farama-Foundation/D4RL), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/deepmind/dm_env/blob/master/docs/index.md)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](http://www.tensorflow.org/datasets/catalog/overview), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/robosuite_panda_pick_place_can), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/locomotion), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/mt_opt), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/external_tfrecord?hl=en#load_dataset_with_tfds), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/data), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/data_performance#optimize_performance), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/splits), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google-research/rlds/blob/main/rlds/examples/rlds_tutorial.ipynb) | 16.03.2022 |
| Real-Time Voice Cloning | SV2TTS with a vocoder that works in real-time | - [Corentin Jemine](https://github.com/CorentinJ)<br>- [Erdene-Ochir Tuguldur](https://github.com/tugstugi) | [![](https://camo.githubusercontent.com/f456c3d4d684a0d2d675a4e76766f323ab320243fb7ec4b1b760020c9f10dc41/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f72656e74696e4a2f5265616c2d54696d652d566f6963652d436c6f6e696e673f7374796c653d736f6369616c)](https://github.com/CorentinJ/Real-Time-Voice-Cloning)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1806.04558), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1802.08435), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1703.10135), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1710.10467)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fatchord/WaveRNN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/coqui-ai/tts), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/resemble-ai/Resemblyzer)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/-O_hYhToKoA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tugstugi/dl-colab-notebooks/blob/master/notebooks/RealTimeVoiceCloning.ipynb) | 08.03.2022 |
| BLIP | VLP framework which transfers flexibly to both vision-language understanding and generation tasks | - [Junnan Li](https://github.com/LiJunnan1992)<br>- [Dongxu Li](https://sites.google.com/view/dongxu-li/home)<br>- [Caiming Xiong](http://cmxiong.com/)<br>- [Steven Hoi](https://sites.google.com/view/stevenhoi) | [![](https://camo.githubusercontent.com/aa5f19dae3cfffb43d9276fb6ed2f719b33c503ed03462da39b8b0edc4d8baef/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73616c6573666f7263652f424c49503f7374796c653d736f6369616c)](https://github.com/salesforce/BLIP) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.12086)<br>- [blog post](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/fairscale), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/salesforce/ALPRO), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/dmlc/decord), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/salesforce/ALBEF), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rwightman/pytorch-image-models/tree/main/timm)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/X2k7n4FuI7c) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb) | 03.03.2022 |
| VideoGPT | A conceptually simple architecture for scaling likelihood based generative modeling to natural videos | - [Wilson Yan](https://wilson1yan.github.io/)<br>- [Yunzhi Zhang](https://zzyunzhi.github.io/)<br>- [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)<br>- [Aravind Srinivas](https://people.eecs.berkeley.edu/~aravind/) | [![](https://camo.githubusercontent.com/1bd58332b31bfec2a9627b6dc9b35ec817d377165e2efe50bfafcd3e1ff4577b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f77696c736f6e3179616e2f566964656f4750543f7374796c653d736f6369616c)](https://github.com/wilson1yan/VideoGPT)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.10157), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1904.10509)<br>- [data](https://www.crcv.ucf.edu/data/UCF101.php)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/VideoGPT)<br>- [project](https://wilson1yan.github.io/videogpt/index.html) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/wilson1yan/VideoGPT/blob/master/notebooks/Using_VideoGPT.ipynb) | 02.03.2022 |
| Silero Models | Pre-trained speech-to-text, text-to-speech and text-enhancement models made embarrassingly simple | [Silero team](https://www.silero.ai/about/) | [![](https://camo.githubusercontent.com/d9704e079514f2bef9fc6c6ccbafc0280eb7c14f67982a1c584680b8a9bd4e21/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f736e616b657273342f73696c65726f2d6d6f64656c733f7374796c653d736f6369616c)](https://github.com/snakers4/silero-models) <br>- [STT](https://thegradient.pub/towards-an-imagenet-moment-for-speech-to-text/), [STT](https://thegradient.pub/a-speech-to-text-practitioners-criticisms-of-industry-and-academia/), [STT](https://habr.com/ru/post/519562/)<br>- [TTS](https://habr.com/ru/post/660571/), [TTS](https://habr.com/ru/post/549482/)<br>- [Text Enhancement](https://habr.com/ru/post/581960/)<br>- [VAD](https://thegradient.pub/one-voice-detector-to-rule-them-all/), [VAD](https://habr.com/ru/post/537276/)<br>- [website](https://www.silero.ai/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples.ipynb) | 27.02.2022 |
| Real-CUGAN | AI super resolution model for anime images, trained in a million scale anime dataset, using the same architecture as Waifu2x-CUNet | [bilibili](https://github.com/bilibili) | [![](https://camo.githubusercontent.com/f0af248b1adeddb4ed0bd3ae1e5a2c34d31732d2a5007c29bf65c16b6816d692/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f62696c6962696c692f61696c61623f7374796c653d736f6369616c)](https://github.com/bilibili/ailab/tree/main/Real-CUGAN)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nihui/realcugan-ncnn-vulkan), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/nagadomi/nunif), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Justin62628/Squirrel-RIFE)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/mayhug/Real-CUGAN)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/IVo19n4zFsc) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/bilibili/ailab/blob/main/Real-CUGAN/colab-demo.ipynb) | 27.02.2022 |
| ArcaneGAN | Process video in the style of the Arcane animated series | [Alexander Spirin](https://github.com/Sxela) | [![](https://camo.githubusercontent.com/e2ba955e70928e3feb9eb27132ce6038bcd631e860ba3f9f0a5d2dbd0e66e386/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f5378656c612f417263616e6547414e3f7374796c653d736f6369616c)](https://github.com/Sxela/ArcaneGAN)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Sxela/stylegan3_blending)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Fi199uFW6jE), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/AJG4X7IokG8) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1r1hhciakk5wHaUn1eJk7TP58fV9mjy_W) | 17.02.2022 |
| textlesslib | A library aimed to facilitate research in Textless NLP | - [Eugene Kharitonov](https://eugene-kharitonov.github.io/)<br>- [Jade Copet](https://scholar.google.com/citations?user=GRMLwjAAAAAJ)<br>- [Kushal Lakhotia](https://about.me/hikushalhere)<br>- [Nguyễn Tú Anh](https://tuanh208.github.io/)<br>others[Paden Tomasello](https://scholar.google.com/citations?user=sBtWMGYAAAAJ)<br>[Ann Lee](https://ai.facebook.com/people/ann-lee)<br>[Ali Elkahky](https://scholar.google.com/citations?user=KB3S8RoAAAAJ)<br>[Wei-Ning Hsu](https://wnhsu.github.io/)<br>[Abdelrahman Mohamed](https://ai.facebook.com/people/abdelrahman-mohamed/)<br>[Emmanuel Dupoux](http://www.lscp.net/persons/dupoux/)<br>[Yossi Adi](https://www.cs.huji.ac.il/~adiyoss/) | [![](https://camo.githubusercontent.com/f31e445bf222ff1ee68767a4e06eb5e85de4301c0cab18946d4ecec2d59c1e43/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f746578746c6573736c69623f7374796c653d736f6369616c)](https://github.com/facebookresearch/textlesslib)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2202.07359)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/waveglow), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/keithito/tacotron), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/tacotron2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/pseeth/torch-stft)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/dataset/librispeech) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/textlesslib/blob/main/examples/resynthesis_and_continuation.ipynb) | 15.02.2022 |
| AV-HuBERT | Self-supervised representation learning framework for audio-visual speech | - [Bowen Shi](https://home.ttic.edu/~bshi/)<br>- [Wei-Ning Hsu](http://people.csail.mit.edu/wnhsu/)<br>- [Kushal Lakhotia](https://about.me/hikushalhere)<br>- [Abdelrahman Mohamed](http://www.cs.toronto.edu/~asamir/) | [![](https://camo.githubusercontent.com/03a5787bf4286fc550bf2d3371a3ec3435fc87649528ae7172e03a9bf146f8b7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f61765f6875626572743f7374796c653d736f6369616c)](https://github.com/facebookresearch/av_hubert)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.02184), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2201.01763), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1810.04805), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1911.04890)<br>- [blog post](https://ai.facebook.com/blog/ai-that-understands-speech-by-looking-as-well-as-hearing/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1bNXkfpHiVHzXQH8WjGhzQ-fsDxolpUjD) | 12.02.2022 |
| Lingvo | Framework for building neural networks in Tensorflow, particularly sequence models | - [Jonathan Shen](https://github.com/jonathanasdf)<br>- [Patrick Nguyen](https://scholar.google.com/citations?user=38fqeIYAAAAJ)<br>- [Yonghui Wu](https://scholar.google.com/citations?user=55FnA9wAAAAJ)<br>- [Zhifeng Chen](https://github.com/zffchen78) | [![](https://camo.githubusercontent.com/d2e38ecfb50ece0c959be229be88da4ad69564d7b91c1c368fea287ba91f5e72/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f6c696e67766f3f7374796c653d736f6369616c)](https://github.com/tensorflow/lingvo)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1902.08295), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1508.01211), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1412.1602), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1602.02410), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.16668), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.04060)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://github.com/tensorflow/lingvo/blob/master/docker/dev.Dockerfile), [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://github.com/tensorflow/lingvo/blob/master/docker/lib.dockerfile)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://tensorflow.github.io/lingvo/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/lingvo/blob/master/codelabs/introduction.ipynb) | 28.01.2022 |
| DeepDream | This tutorial contains a minimal implementation of DeepDream: an experiment that visualizes the patterns learned by a neural network | - [Alexander Mordvintsev](https://znah.net/)<br>- [Billy Lamberta](https://github.com/lamberta) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1409.4842)<br>- [blog post](https://research.google/blog/inceptionism-going-deeper-into-neural-networks/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@nik.nagarajan2/deepdream-a-psychedelic-ai-experience-ab482dd5228b), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/dreaming-over-text-f6745c829cee)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/generative/deepdream)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Inception), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/DeepDream) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb) | 13.01.2022 |
| FuseDream | Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization | - [Xingchao Liu](https://scholar.google.com/citations?user=VOTVE0UAAAAJ)<br>- [Chengyue Gong](https://github.com/ChengyueGongR)<br>- [Lemeng Wu](https://github.com/klightz)<br>- [Hao Su](https://cseweb.ucsd.edu//~haosu/)<br>- [Qiang Liu](https://www.cs.utexas.edu/~lqiang/) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2112.01573) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/190tKQf0aFj-Hi8STUrLc2m4DeOviv7NO) | 02.01.2022 |
| MLP | The most basic neural network architectures, a multilayer perceptron, also known as a feedforward network | [Ben Trevett](https://bentrevett.com/) | - [NN and DL](http://neuralnetworksanddeeplearning.com/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1702.03118), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2108.12943), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.04020)<br>- [optimization](https://ruder.io/optimizing-gradient-descent/)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-only), [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/vision/stable/transforms.html#transforms-on-torch-tensor-only)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Multilayer_perceptron) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb) | 26.12.2021 |
| AlexNet | A neural network model that uses convolutional neural network layers and was designed for the ImageNet challenge | [Ben Trevett](https://bentrevett.com/) | [![](https://camo.githubusercontent.com/e95f7433acf3159a548445cde54daf88010d67b22b45e16d21f95965a1616ee5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f64617669647476732f7079746f7263682d6c722d66696e6465723f7374796c653d736f6369616c)](https://github.com/davidtvs/pytorch-lr-finder) <br>- [ILSVRC](https://image-net.org/challenges/LSVRC/)<br>- [LR](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html)<br>- [PMLR](https://proceedings.mlr.press/v9/glorot10a.html)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1409.0575)<br>- [cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html)<br>- [dropout](https://sebastianraschka.com/faq/docs/dropout-activation.html)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/vision/stable/models.html)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/method/alexnet)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Regularization_(mathematics)), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/AlexNet) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/3_alexnet.ipynb) | 26.12.2021 |
| VGG | Very Deep Convolutional Networks for Large-Scale Image Recognition | [Ben Trevett](https://bentrevett.com/) | [![](https://camo.githubusercontent.com/3312f4db7e829f5cec95eaf55adbfc82e8be5d1755662f864770d83b4ef608be/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7079746f7263682f766973696f6e3f7374796c653d736f6369616c)](https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py#L47) <br>- [ILSVRC](https://image-net.org/challenges/LSVRC/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1409.1556), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1506.01186), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1801.06146), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1502.03167), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1805.11604)<br>- [cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html)<br>- [![pt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pt.svg)](https://pytorch.org/vision/stable/models.html)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/method/vgg)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/HR0lt1hlR6U?t=5900), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/j1jIoHN3m0s), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/RNnKtNrsrmg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/4_vgg.ipynb) | 26.12.2021 |
| LeNet | A neural network model that uses convolutional neural network layers and was designed for classifying handwritten characters | [Ben Trevett](https://bentrevett.com/) | - [CNN](https://cs231n.github.io/convolutional-networks/)<br>- [LeNet-5](http://yann.lecun.com/exdb/lenet/)<br>- [guide](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)<br>- [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/method/lenet)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Convolution), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Sobel_operator), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Gaussian_blur) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/2_lenet.ipynb) | 26.12.2021 |
| Music Composer | Synthesizing symbolic music in MIDI format using the Music Transformer model | [bazanovvanya](https://github.com/bazanovvanya) | [![](https://camo.githubusercontent.com/52551397b3328333ad39975b113c6757304cbd8ae44e3878a653a9f9ae6f5137/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61692d666f72657665722f6d757369632d636f6d706f7365723f7374796c653d736f6369616c)](https://github.com/ai-forever/music-composer) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.05858)<br>- [blog post](https://habr.com/ru/company/sberbank/blog/583592/)<br>- [data](https://magenta.tensorflow.org/datasets/maestro), [data](https://colinraffel.com//projects/lmd/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/gwinndr/MusicTransformer-Pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/bytedance/GiantMIDI-Piano), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/mdeff/fma) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ai-forever/music-composer/blob/master/src/Music_Composer_Demo_Colab_en.ipynb) | 20.12.2021 |
| FLAML | Lightweight Python library that finds accurate machine learning models automatically, efficiently and economically | - [Chi Wang](https://github.com/sonichi)<br>- [Qingyun Wu](https://qingyun-wu.github.io/) | [![](https://camo.githubusercontent.com/e4f2e405a4a032a9706f98849ada23cb3278157ffea29249aa4d04a20a9605c5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f464c414d4c3f7374796c653d736f6369616c)](https://github.com/microsoft/FLAML)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2106.04815), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.01571)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://microsoft.github.io/FLAML/)<br>- [paper](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/channel/UCfU0zfFXHXdAd5x-WvFBk5A), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/euXpDYGgkGM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/microsoft/FLAML/blob/master/notebook/flaml_automl.ipynb) | 17.12.2021 |
| CompilerGym | A reinforcement learning toolkit for compiler optimizations | - [Chris Cummins](https://chriscummins.cc/)<br>- [Bram Wasti](https://github.com/bwasti)<br>- [Jiadong Guo](https://jd-eth.github.io/)<br>- [Brandon Cui](https://www.linkedin.com/in/bcui19/)<br>others[Jason Ansel](https://jasonansel.com/)<br>[Sahir Gomez](https://github.com/sahirgomez1)<br>[Olivier Teytaud](https://github.com/teytaud)<br>[Benoit Steiner](http://bsteiner.info/)<br>[Yuandong Tian](http://yuandong-tian.com/)<br>[Hugh Leather](https://github.com/hughleat) | [![](https://camo.githubusercontent.com/2b53338f0c1d45e23793ecd5695bfa494994a4ff6c1b5a7c6ad2e718d7d27fec/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f436f6d70696c657247796d3f7374796c653d736f6369616c)](https://github.com/facebookresearch/CompilerGym)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2109.08267)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://facebookresearch.github.io/CompilerGym/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/CompilerGym/blob/development/examples/getting-started.ipynb) | 16.11.2021 |
| Reformer | Performs on par with Transformer models while being much more memory-efficient and much faster on long sequences | - [Phil Wang](https://lucidrains.github.io/)<br>- [Nikita Kitaev](https://kitaev.com/)<br>- [Łukasz Kaiser](https://scholar.google.com/citations?user=JWmiQR0AAAAJ)<br>- [Anselm Levskaya](https://anselmlevskaya.com/) | [![](https://camo.githubusercontent.com/31b07958c94e7122ea20dd14a56d5fc4d7021aed09c4f424345294cf9d5fa995/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c756369647261696e732f7265666f726d65722d7079746f7263683f7374796c653d736f6369616c)](https://github.com/lucidrains/reformer-pytorch)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2001.04451), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.01470), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.05895), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1909.11556), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1911.02150), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2002.05202), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.05997), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2003.04887), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2002.07028), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.03404), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.09864)<br>- [blog post](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/routing-transformer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/sinkhorn-transformer), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/performer-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/linear-attention-transformer/), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/lucidrains/compressive-transformer-pytorch)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper/2019/hash/9d8df73a3cfbf3c5b47bc9b50f214aff-Abstract.html), [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/reformer-pytorch/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/i4H0kjxrias), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Kf3x3lqf9cQ), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/0eTULzrOztQ) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1awNgXYtjvUeXl1gS-v1iyDXTJJ-fyJIK) | 07.11.2021 |
| ruDALL·E | Generate images from texts in Russian | [Alex Shonenkov](https://github.com/shonenkov) | [![](https://camo.githubusercontent.com/cbece366d1c0b173bd548d65aea34038b194706a618c959a655735540fe3c774/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f61692d666f72657665722f72752d64616c6c653f7374796c653d736f6369616c)](https://github.com/ai-forever/ru-dalle)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/bes-dev/vqvae_dwt_distiller.pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/boomb0om/Real-ESRGAN-colab)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/multimodalart/rudalle)<br>- [project](https://rudalle.ru/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/ai-forever/ru-dalle/blob/master/jupyters/ruDALLE-example-generation-A100.ipynb) | 03.11.2021 |
| DeepStyle | The Neural Style algorithm synthesizes a pastiche by separating and combining the content of one image with the style of another image using convolutional neural networks | - [Cameron Smith](https://github.com/cysmith)<br>- [Alexander Spirin](https://github.com/Sxela) | [![](https://camo.githubusercontent.com/2dc7b97c983a64234860088116147e70e27b61a30221a35505b0213e6a023bf0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6379736d6974682f6e657572616c2d7374796c652d74663f7374796c653d736f6369616c)](https://github.com/cysmith/neural-style-tf)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1604.08610), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1606.05897), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1508.06576)<br>- [cvpr](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Pastiche), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/The_Starry_Night), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/YUV), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Lab_color_space), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/YCbCr), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/CIELUV), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Pareidolia) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/14aJ7HQPbcP0sNRIY-FRO4u6lxtlyyxI_) | 01.10.2021 |
| Text2Animation | Generate images from text phrases with VQGAN and CLIP with animation and keyframes | - [Katherine Crowson](https://kath.io/)<br>- [Ryan Murdock](https://twitter.com/advadnoun)<br>- [Chigozie Nri](https://github.com/chigozienri)<br>- [Denis Malimonov](https://github.com/tg-bomze) | [![](https://camo.githubusercontent.com/956696746b44e1ccd66564bbbd65426e9eb8ead47908dbc828c73c2edb78bbf1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636869676f7a69656e72692f565147414e2d434c49502d616e696d6174696f6e733f7374796c653d736f6369616c)](https://github.com/chigozienri/VQGAN-CLIP-animations)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.09841), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.00020)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/channel/UCToztRy9FSTIhEen_1x4FAw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/Text2Animation.ipynb) | 29.09.2021 |
| EfficientNetV2 | A family of image classification models, which achieve better parameter efficiency and faster training speed than prior arts | - [Mingxing Tan](https://scholar.google.com/citations?user=6POeyBoAAAAJ)<br>- [Quoc Le](https://cs.stanford.edu/~quocle/) | [![](https://camo.githubusercontent.com/8ae44b767fda77289686df670643028ded5f184b89750df099c94ef4bd3e93e5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6175746f6d6c3f7374796c653d736f6369616c)](https://github.com/google/automl/tree/master/efficientnetv2)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.00298), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1905.11946)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVIDIA/TensorRT/tree/master/samples/python/efficientnet) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/automl/blob/master/efficientnetv2/tutorial.ipynb) | 24.09.2021 |
| Clip retrieval | Easily compute clip embeddings and build a clip retrieval system with them | [Romain Beaumont](https://github.com/rom1504) | [![](https://camo.githubusercontent.com/a673fde51e68056802aeab2eab1191ebfd7d08869eca3f88be9fc1bf71397996/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726f6d313530342f636c69702d72657472696576616c3f7374796c653d736f6369616c)](https://github.com/rom1504/clip-retrieval)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/eq3cAMZtCC)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/LAION-AI/CLIP_benchmark), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rom1504/laion-prepro), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/dzryk/antarctic-captions), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/LAION-AI/CLIP-based-NSFW-Detector), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/ml-research/OffImgDetectionCLIP)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://rom1504.medium.com/semantic-search-with-embeddings-index-anything-8fb18556443c)<br>- [project](https://rom1504.github.io/clip-retrieval)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.python.org/pypi/clip-retrieval)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Locality_of_reference) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/rom1504/clip-retrieval/blob/master/notebook/clip-retrieval-getting-started.ipynb) | 21.09.2021 |
| img2dataset | Easily turn large sets of image urls to an image dataset | [Romain Beaumont](https://github.com/rom1504) | [![](https://camo.githubusercontent.com/271a6ee22f2cc4c3b44e8f3ff481c05a45f826e110b468a2f6d87c34e1d5958d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f726f6d313530342f696d6732646174617365743f7374796c653d736f6369616c)](https://github.com/rom1504/img2dataset)<br>- [![discord](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/discord.svg)](https://discord.gg/eq3cAMZtCC)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/uber/petastorm), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fsspec/filesystem_spec/blob/6233f315548b512ec379323f762b70764efeb92c/fsspec/registry.py#L87), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/fsspec/sshfs), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rom1504/cah-prepro)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/hub/datasets-viewer), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/docs/huggingface_hub/guides/hf_file_system)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://rom1504.medium.com/semantic-search-at-billions-scale-95f21695689a)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.python.org/pypi/img2dataset)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/data) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/rom1504/img2dataset/blob/master/notebook/img2dataset_getting_started.ipynb) | 17.09.2021 |
| Droidlet | A modular embodied agent architecture and platform for building embodied agents | - [Anurag Pratik](https://github.com/anuragprat1k)<br>- [Soumith Chintala](https://soumith.ch/)<br>- [Kavya Srinet](https://github.com/kavyasrinet)<br>- [Dhiraj Gandhi](https://dhiraj100892.github.io/)<br>others[Rebecca Qian](https://github.com/Rebecca-Qian)<br>[Yuxuan Sun](https://github.com/snyxan)<br>[Ryan Drew](https://rdrew.dev/)<br>[Sara Elkafrawy](https://github.com/saraEbrahim)<br>[Anoushka Tiwari](https://www.linkedin.com/in/anoushka-tiwari)<br>[Tucker Hart](https://www.linkedin.com/in/tucker-hart-05a638133)<br>[Mary Williamson](https://scholar.google.com/citations?user=Ys4xB-QAAAAJ)<br>[Abhinav Gupta](http://www.cs.cmu.edu/~abhinavg/)<br>[Arthur Szlam](https://scholar.google.com/citations?user=u3-FxUgAAAAJ) | [![](https://camo.githubusercontent.com/d0a5d6e4e9d2d03d0947cab40e15f7e3deee25cc9934fbba5d30fb724a75bce2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f64726f69646c65743f7374796c653d736f6369616c)](https://github.com/facebookresearch/droidlet)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2101.10384), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.08584)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://facebookresearch.github.io/droidlet/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/droidlet/blob/master/examples_and_tutorials/tutorials/droidlet_for_physical_robots.ipynb) | 15.09.2021 |
| GPT-J-6B | A 6 billion parameter, autoregressive text generation model trained on The Pile | - [Ben Wang](https://benwang.dev/)<br>- [Aran Komatsuzaki](https://arankomatsuzaki.wordpress.com/about-me/)<br>- [Janko Prester](https://www.jankoprester.com/) | [![](https://camo.githubusercontent.com/4b8b93237dab786df6ddfff746c990dc1d363f22a9589882639ab9a49e9def3e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6b696e676f666c6f6c7a2f6d6573682d7472616e73666f726d65722d6a61783f7374796c653d736f6369616c)](https://github.com/kingoflolz/mesh-transformer-jax) <br>- [The Pile](https://pile.eleuther.ai/)<br>- [blog post](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/EleutherAI/gpt-neox), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/microsoft/DeepSpeed)<br>- [web demo](https://6b.eleuther.ai/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb) | 15.09.2021 |
| Lucid Sonic Dreams | Syncs GAN-generated visuals to music | [Mikael Alafriz](https://github.com/mikaelalafriz) | [![](https://camo.githubusercontent.com/ecebcddb682cde5f4c78a231a158925423ad39e0bf864fb8cb1c08d2aac78efd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d696b61656c616c616672697a2f6c756369642d736f6e69632d647265616d733f7374796c653d736f6369616c)](https://github.com/mikaelalafriz/lucid-sonic-dreams)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/NVlabs/stylegan2), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/justinpinkney/awesome-pretrained-stylegan2)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/introducing-lucid-sonic-dreams-sync-gan-art-to-music-with-a-few-lines-of-python-code-b04f88722de1)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/l-nGC-ve7sI) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1Y5i50xSFIuN3V4Md8TB30_GOAtts7RQD) | 24.08.2021 |
| textgenrnn | Generate text using a pretrained neural network with a few lines of code, or easily train your own text-generating neural network of any size and complexity | [Max Woolf](https://minimaxir.com/) | [![](https://camo.githubusercontent.com/a76703de032b6954e597f4e55e21e279cf4ccda71fa0347ac4e7f5e82ce4b4d5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d696e696d617869722f7465787467656e726e6e3f7374796c653d736f6369616c)](https://github.com/minimaxir/textgenrnn) <br>- [blog post](http://minimaxir.com/2018/05/text-neural-networks/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=RW7mP6BfZuY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK) | 13.07.2021 |
| DDSP | Differentiable Digital Signal Processing library, which enables direct integration of classic signal processing elements with deep learning methods | - [Jesse Engel](https://github.com/jesseengel)<br>- [Lamtharn Hantrakul](https://lh-hantrakul.com/)<br>- [Chenjie Gu](https://scholar.google.com/citations?user=_4B6OTAAAAAJ)<br>- [Adam Roberts](https://github.com/adarob) | [![](https://camo.githubusercontent.com/0e7770e49faef404511be1a57fc3517ae0617a60dcbb422e8cb2df1f9605bae8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6167656e74612f646473703f7374796c653d736f6369616c)](https://github.com/magenta/ddsp) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2001.04643)<br>- [blog post](https://magenta.tensorflow.org/ddsp)<br>- [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/acids-ircam/ddsp_pytorch)<br>- [project](https://storage.googleapis.com/ddsp/index.html)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Chorus_effect), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Flanging), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Vibrato), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Nyquist_frequency), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Aliasing)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLBUMAYA6kvGXyzrZJMACr6VNZuunv8DMU), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/EXz1TJQ-hSo), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/g1D4brEhxWw?t=1272), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/IZBlqcbpmxY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/tutorials/0_processor.ipynb) | 01.07.2021 |
| BasicSR | Open Source Image and Video Restoration Toolbox for Super-resolution, Denoise, Deblurring, etc. | - [Xintao Wang](https://xinntao.github.io/)<br>- [Liangbin Xie](https://liangbinxie.github.io/)<br>- [Ke Yu](https://github.com/yuke93)<br>- [Kelvin Chan](https://ckkelvinchan.github.io/)<br>others[Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)<br>[Chao Dong](https://scholar.google.com/citations?user=OSDCB0UAAAAJ) | [![](https://camo.githubusercontent.com/0c0bce28bbba8eba5e29ae774e42fc35586500cf6201e878b2c5533c3dc106a9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f58506978656c47726f75702f426173696353523f7374796c653d736f6369616c)](https://github.com/XPixelGroup/BasicSR)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2012.02181)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://basicsr.readthedocs.io/en/latest/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/ESRGAN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xindongzhang/ECBSR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Lotayou/Face-Renovation), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/csxmli2016/DFDNet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/rosinality/stylegan2-pytorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/facexlib), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/HandyView), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/HandyFigure), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/SFTGAN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/DNI), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/HandyCrawler), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/xinntao/HandyWriting)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/KaMYsxWkmww) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1JQScYICvEC3VqaabLu-lxvq9h7kSV1ML) | 07.06.2021 |
| TensorFlowTTS | Real-time state-of-the-art speech synthesis architectures such as Tacotron-2, Melgan, Multiband-Melgan, FastSpeech, FastSpeech2 based-on TensorFlow 2 | - [Minh Nguyen Quan Anh](https://github.com/dathudeptrai)<br>- [Eren Gölge](https://github.com/erogol)<br>- [Kuan Chen](https://github.com/azraelkuan)<br>- [Takuya Ebata](https://github.com/MokkeMeguru) | [![](https://camo.githubusercontent.com/63313a68018c9e618c696a85dd1f6b37a9ceaf86fe75f29e9062c779c040a5b3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f54656e736f725370656563682f54656e736f72466c6f775454533f7374796c653d736f6369616c)](https://github.com/TensorSpeech/TensorFlowTTS)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/thorstenMueller/Thorsten-Voice)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/spaces/akhaliq/TensorFlowTTS), [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/tensorspeech)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/datasets/bryanpark/korean-single-speaker-speech-dataset)<br>- [project](https://tensorspeech.github.io/TensorFlowTTS/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/TensorFlowTTS/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/TensorSpeech/TensorFlowTTS/blob/master/notebooks/TensorFlowTTS_FastSpeech_with_TFLite.ipynb) | 01.06.2021 |
| Hyperopt | Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions | - [James Bergstra](https://github.com/jaberg)<br>- [Dan Yamins](https://github.com/yamins81)<br>- [David Cox](https://scholar.google.com/citations?user=6S-WgLkAAAAJ) | [![](https://camo.githubusercontent.com/a7727c360409d7d5d618e220ab0d68e60d779e6a0067505b0e1c971386a2549e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f68797065726f70742f68797065726f70743f7374796c653d736f6369616c)](https://github.com/hyperopt/hyperopt) <br>- [ICML](https://proceedings.mlr.press/v28/bergstra13.html)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](http://hyperopt.github.io/hyperopt/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hyperopt/hyperopt-sklearn), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hyperopt/hyperopt-nnet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hyperopt/hyperopt-nnet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hyperopt/hyperopt-convnet), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hyperopt/hyperopt-gpsmbo)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Mp1xnPfE4PY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/tdwgR1AqQ8Y), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/tteE_Vtmrv4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/hyperopt/hyperopt/blob/master/tutorial/01.BasicTutorial.ipynb) | 01.06.2021 |
| CNN | This tutorial demonstrates training a simple Convolutional Neural Network to classify CIFAR images | [Billy Lamberta](https://github.com/lamberta) | - [cifar](https://www.cs.toronto.edu/~kriz/cifar.html)<br>- [link](https://developers.google.com/machine-learning/glossary/#convolutional_neural_network)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/images/cnn) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb) | 22.05.2021 |
| Custom GPT-2 + Tokenizer | Train a custom GPT-2 model for free on a GPU using aitextgen! | [Max Woolf](https://minimaxir.com/) | [![](https://camo.githubusercontent.com/7530cdf29f0b5bb9470e2ee833868c593c6a2cef21c8c263d059aa30b5a15fb3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d696e696d617869722f61697465787467656e3f7374796c653d736f6369616c)](https://github.com/minimaxir/aitextgen) <br>- [data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.aitextgen.io/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/144MdX5aLqrQ3-YW-po81CQMrD6kpgpYh) | 17.05.2021 |
| Train a GPT-2 Text-Generating Model | Retrain an advanced text generating neural network on any text dataset for free on a GPU using Colaboratory using aitextgen! | [Max Woolf](https://minimaxir.com/) | [![](https://camo.githubusercontent.com/7530cdf29f0b5bb9470e2ee833868c593c6a2cef21c8c263d059aa30b5a15fb3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d696e696d617869722f61697465787467656e3f7374796c653d736f6369616c)](https://github.com/minimaxir/aitextgen) <br>- [data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.aitextgen.io/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/task/text-generation) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD) | 17.05.2021 |
| EasyNMT | Easy to use, state-of-the-art machine translation for more than 100+ languages | [Nils Reimers](https://www.nils-reimers.de/) | [![](https://camo.githubusercontent.com/8dae0135ff63a4792f8909ae6acbb394cdb4be7962aff63297621e2a261fdd23/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f554b504c61622f456173794e4d543f7374796c653d736f6369616c)](https://github.com/UKPLab/EasyNMT)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2008.00401), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.11125)<br>- [demo](http://easynmt.net/demo/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Helsinki-NLP/Opus-MT), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/facebookresearch/fairseq/tree/main/examples/multilingual) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1X47vgSiOphpxS5w_LPtjQgJmiSTNfRNC) | 26.04.2021 |
| SkinDeep | Remove Body Tattoo Using Deep Learning | [Vijish Madhavan](https://github.com/vijishmadhavan) | [![](https://camo.githubusercontent.com/7b8abafda52d063eb2a6f758afc0ddeaf2e891f10085d49094ba2b66e84806c3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76696a6973686d6164686176616e2f536b696e446565703f7374796c653d736f6369616c)](https://github.com/vijishmadhavan/SkinDeep)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1805.08318), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1710.10196), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1707.02921), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1603.08155)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/jantic/DeOldify) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep_good.ipynb) | 24.04.2021 |
| PaddleHub | Pre-trained models toolkit based on PaddlePaddle: 400+ models including Image, Text, Audio, Video and Cross-Modal with Easy Inference & Serving | - [Zeyu Chen](https://github.com/ZeyuChen)<br>- [Zewu Wu](https://github.com/nepeplwu)<br>- [Bin Long](https://github.com/sjtubinlong)<br>- [Xuefei Zhang](https://github.com/Steffy-zxf)<br>others[Jinxuan Qiu](https://github.com/kinghuin)<br>[Yuhan Shen](https://github.com/ShenYuhan)<br>[Yuying Hao](https://github.com/haoyuying)<br>[Xiaojie Chen](https://github.com/KPatr1ck) | [![](https://camo.githubusercontent.com/1b7bc80be92678efbda4f9245b2fc47fd2ee71e3ade77cfff0d9ed8094ba1d04/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f506164646c65506164646c652f506164646c654875623f7374796c653d736f6369616c)](https://github.com/PaddlePaddle/PaddleHub)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://paddlehub.readthedocs.io/en)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PaddlePaddle/PaddleOCR), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PaddlePaddle/PaddleDetection), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PaddlePaddle/PaddleGAN), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CMU-Perceptual-Computing-Lab/openpose), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PaddlePaddle/PaddleSeg), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PaddlePaddle/PaddleClas), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PaddlePaddle/ERNIE), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/baidu/LAC), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/baidu/DDParser), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/PaddlePaddle/PaddleSpeech)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/PaddlePaddle)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/analytics-vidhya/paddlehub-fdd1ec75a07b)<br>- [website](https://www.paddlepaddle.org.cn/en)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/9adXuF_lTSg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/PaddlePaddle/PaddleHub/blob/develop/demo/serving/bentoml/cloud-native-model-serving-with-bentoml.ipynb) | 20.04.2021 |
| OCTIS | Framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach | - [Silvia Terragni](https://silviatti.github.io/)<br>- [Elisabetta Fersini](https://www.unimib.it/elisabetta-fersini)<br>- [Antonio Candelieri](https://www.unimib.it/antonio-candelieri)<br>- [Pietro Tropeano](https://github.com/pietrotrope)<br>others[Bruno Galuzzi](https://github.com/brunoG89)<br>[Lorenzo Famiglini](https://github.com/lorenzofamiglini)<br>[Davide Pietrasanta](https://github.com/davidepietrasanta) | [![](https://camo.githubusercontent.com/96225a787a6e98ef42e3d6d8baa8a7ff6016cfec83b9ae06851350ff0097de81/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d696e642d4c61622f6f637469733f7374796c653d736f6369616c)](https://github.com/mind-Lab/octis) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1703.01488)<br>- [data](https://www.dbpedia.org/resources/ontology/), [data](https://www.statmt.org/europarl/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/estebandito22/PyTorchAVITM)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/a-beginners-guide-to-octis-optimizing-and-comparing-topic-models-is-simple-590554ec9ba6), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/a-beginners-guide-to-octis-vol-2-optimizing-topic-models-1214e58be1e5)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2000/hash/f9d1152547c0bde01830b7e8bd60024c-Abstract.html)<br>- [paper](https://aclanthology.org/2021.eacl-demos.31/)<br>- [![pwc](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pwc.svg)](https://paperswithcode.com/dataset/20-newsgroups)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nPmiWBFFJ8E) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_Optimizing_CTM.ipynb) | 19.04.2021 |
| PyTorchVideo | Deeplearning library with a focus on video understanding work | - [Haoqi Fan](https://haoqifan.github.io/)<br>- [Tullie Murrell](https://github.com/tullie)<br>- [Heng Wang](https://hengcv.github.io/)<br>- [Kalyan Vasudev Alwala](https://github.com/kalyanvasudev)<br>others[Yanghao Li](https://github.com/lyttonhao)<br>[Yilei Li](https://liyilui.github.io/personal_page/)<br>[Bo Xiong](https://github.com/bxiong1202)<br>[Nikhila Ravi](https://nikhilaravi.com/)<br>[Meng Li](https://mengli.me/)<br>[Haichuan Yang](https://hyang1990.github.io/)<br>[Jitendra Malik](https://scholar.google.com/citations?user=oY9R5YQAAAAJ)<br>[Ross Girshick](https://github.com/rbgirshick)<br>[Matt Feiszli](https://scholar.google.com/citations?user=A-wA73gAAAAJ)<br>[Aaron Adcock](https://scholar.google.com/citations?&user=oa78zHUAAAAJ)<br>[Wan-Yen Lo](https://github.com/wanyenlo)<br>[Christoph Feichtenhofer](http://feichtenhofer.github.io/) | [![](https://camo.githubusercontent.com/19a46489656c034fd5c55e933432ba40cfe13028252d3ffd1c807fe525686277/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333437343038352e33343738333239)](https://doi.org/10.1145/3474085.3478329)[![](https://camo.githubusercontent.com/cdf9bf05d978a8329b8dd07fdc288e7bb6ac639b124335ce8775206705ec25b1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7079746f726368766964656f3f7374796c653d736f6369616c)](https://github.com/facebookresearch/pytorchvideo)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2111.09887), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2104.11227)<br>- [blog post](https://ai.facebook.com/blog/pytorchvideo-a-deep-learning-library-for-video-understanding/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://pytorchvideo.readthedocs.io/en/latest/index.html)<br>- [website](https://github.com/facebookresearch/pytorchvideo)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/b7-gnpqz9Qg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/facebookresearch/pytorchvideo/blob/main/tutorials/accelerator/Build_your_model_with_PytorchVideo_Accelerator.ipynb) | 13.04.2021 |
| NeuSpell | Open-source toolkit for spelling correction in English | - [Sai Muralidhar Jayanthi](https://github.com/murali1996)<br>- [Danish Pruthi](https://danishpruthi.com/)<br>- [Graham Neubig](https://phontron.com/index.php) | [![](https://camo.githubusercontent.com/1d2bd548b4b0fdde7084683acf5be66dd6f5fc0f631455846d5a0b562367e372/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f323032302e656d6e6c702d64656d6f732e3231)](https://doi.org/10.18653/v1/2020.emnlp-demos.21)[![](https://camo.githubusercontent.com/82aa04a59971c3717eab0a4d449246025a2016fa217139dc2dd476f8ae250d08/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6e65757370656c6c2f6e65757370656c6c3f7374796c653d736f6369616c)](https://github.com/neuspell/neuspell)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2010.11085), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1312.3005)<br>- [![hf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/hf.svg)](https://huggingface.co/transformers/bertology.html)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@kunalgkjoshi/implementing-spell-correction-a-journey-with-xfspell-and-neuspell-4bc33e3bcde7)<br>- [project](https://neuspell.github.io/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/neuspell/neuspell/blob/master/scripts/english_baselines/2_jamspell.ipynb) | 03.04.2021 |
| GPT Neo | An implementation of model & data parallel GPT2 & GPT3 -like models, with the ability to scale up to full GPT3 sizes (and possibly more!), using the mesh-tensorflow library | [EleutherAI](https://www.eleuther.ai/) | [![](https://camo.githubusercontent.com/4108aa9d82bdbe283c2ffc90e4b60bfa8c4be4e355a7f8d25ad4c3cf49e46bd2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f456c65757468657241492f6770742d6e656f3f7374796c653d736f6369616c)](https://github.com/EleutherAI/gpt-neo) <br>- [GPT-2](https://openai.com/blog/better-language-models/)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2005.14165), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.05150), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1701.06538)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tensorflow/mesh), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/EleutherAI/gpt-neox/)<br>- [pretrained](https://the-eye.eu/public/AI/gptneo-release/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb) | 28.03.2021 |
| CVAE | This notebook demonstrates how train a Variational Autoencoder on the MNIST dataset | - [Diederik Kingma](http://www.dpkingma.com/)<br>- [Max Welling](https://staff.fnwi.uva.nl/m.welling/)<br>- [Danilo Rezende](https://danilorezende.com/about/)<br>- [Shakir Mohamed](https://shakirm.com/)<br>- [Daan Wierstra](https://scholar.google.com/citations?user=aDbsf28AAAAJ) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1312.6114), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1401.4082)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/generative/cvae) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb) | 22.03.2021 |
| Big Sleep | Text to image generation, using OpenAI's CLIP and a BigGAN | [Phil Wang](https://lucidrains.github.io/) | [![](https://camo.githubusercontent.com/7de737e5968badaffb173a05984549a3cd30e483cfada3221b681e998e06d536/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c756369647261696e732f6269672d736c6565703f7374796c653d736f6369616c)](https://github.com/lucidrains/big-sleep)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.00020), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1809.11096)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/big-sleep/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/bigsleep/comments/lxawb4/how_to_use_some_of_the_newer_features_of/), [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/bigsleep/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb) | 17.03.2021 |
| Deep Daze | Text to image generation using OpenAI's CLIP and Siren | [Phil Wang](https://lucidrains.github.io/) | [![](https://camo.githubusercontent.com/7715fa42f27d8cacbb694219c3f4a5720e6934f36b8a5d1f8a5540d311a53eb2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c756369647261696e732f646565702d64617a653f7374796c653d736f6369616c)](https://github.com/lucidrains/deep-daze)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2103.00020), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2006.09661)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/deep-daze/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/deepdaze/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj) | 17.03.2021 |
| DCGAN | This tutorial demonstrates how to generate images of handwritten digits using a Deep Convolutional Generative Adversarial Network | - [Alec Radford](https://scholar.google.com/citations?user=dOad5HoAAAAJ)<br>- [Luke Metz](https://lukemetz.com/)<br>- [Soumith Chintala](https://soumith.ch/) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1511.06434), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1701.00160)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/jessicali9530/celeba-dataset)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@vedantjagtap2002/artificial-intelligence-approach-to-reduce-energy-used-for-cooling-data-centres-d2d78d92c107)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/generative/dcgan) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb) | 12.03.2021 |
| Adversarial FGSM | This tutorial creates an adversarial example using the Fast Gradient Signed Method attack. This was one of the first and most popular attacks to fool a neural network. | - [Ian Goodfellow](https://www.iangoodfellow.com/)<br>- [Jonathon Shlens](https://shlens.github.io/)<br>- [Christian Szegedy](https://scholar.google.com/citations?user=bnQMuzgAAAAJ) | - [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1412.6572)<br>- [imagenet](http://www.image-net.org/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/@zachariaharungeorge/a-deep-dive-into-the-fast-gradient-sign-method-611826e34865)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications/MobileNetV2), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb) | 12.03.2021 |
| GAN steerability | We will navigate in GAN latent space to simulate various camera transformations | - [Ali Jahanian](http://people.csail.mit.edu/jahanian/)<br>- [Lucy Chai](http://people.csail.mit.edu/lrchai/)<br>- [Phillip Isola](http://web.mit.edu/phillipi/) | [![](https://camo.githubusercontent.com/0cc7b68bb15b1b8a81b6942499c4d6a03b216b62ea1aa550788985d3e78af356/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f616c692d64657369676e2f67616e5f73746565726162696c6974793f7374796c653d736f6369616c)](https://github.com/ali-design/gan_steerability)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1907.07171), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1809.11096)<br>- [project](https://ali-design.github.io/gan_steerability/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/nS0V64sF7Cw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1kn6yG8PqD1U2bUcy32V1iAVjzlcQWcG3) | 04.03.2021 |
| Trax | End-to-end library for deep learning that focuses on clear code and speed | [Google](https://research.google/teams/brain/) | [![](https://camo.githubusercontent.com/97342b1af8b09a5cdda1dcdc188a8d46c10e217539da04fa3a75fdde97f31da7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f747261783f7374796c653d736f6369616c)](https://github.com/google/trax) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.00177)<br>- [discuss](https://groups.google.com/u/1/g/trax-discuss)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://trax-ml.readthedocs.io/en/latest/)<br>- [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus), [![kaggle](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/kaggle.svg)](https://www.kaggle.com/code/dschettler8845/exploration-of-trax-framework)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/get-started-with-google-trax-for-nlp-ff8dcd3119cf), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/analytics-vidhya/brief-view-of-googles-trax-library-b78eae008cb6)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/datasets/catalog/overview), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://tensorflow.org/guide/tf_numpy)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/qlTsaHAtJBY) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb) | 18.02.2021 |
| bsuite | A collection of carefully-designed experiments that investigate core capabilities of an RL agent with two main objectives | - [Ian Osband](http://iosband.github.io/)<br>- [Yotam Doron](http://www.yotamdoron.com/)<br>- [Matteo Hessel](https://github.com/mtthss)<br>- [John Aslanides](https://www.aslanides.io/)<br>others[Eren Sezener](http://erensezener.com/)<br>[Andre Saraiva](https://andresnds.wordpress.com/)<br>[Katrina McKinney](https://medium.com/@katrinamckinney)<br>[Tor Lattimore](http://tor-lattimore.com/)<br>[Csaba Szepesvari](https://sites.ualberta.ca/~szepesva/)<br>[Satinder Singh](http://web.eecs.umich.edu/~baveja/)<br>[Benjamin Van Roy](https://web.stanford.edu/~bvr/)<br>[Richard Sutton](http://www.incompleteideas.net/)<br>[David Silver](https://www.davidsilver.uk/)<br>[Hado Van Hasselt](https://hadovanhasselt.com/) | [![](https://camo.githubusercontent.com/adcf7a3a18c46d260c3b2026fd0ab5e5713450877cd8ee3215a4ff80eaa0fa95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f6273756974653f7374796c653d736f6369616c)](https://github.com/deepmind/bsuite) <br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/gym)<br>- [paper](https://openreview.net/forum?id=rygf-kSYwH)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Wcv4eU_qtZU) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1rU20zJ281sZuMD1DHbsODFr1DbASL0RH) | 13.02.2021 |
| TF-Ranking | End-to-end walkthrough of training a TensorFlow Ranking neural network model which incorporates sparse textual features | [Rama Kumar](https://github.com/ramakumar1729) | [![](https://camo.githubusercontent.com/ea3e5414eeeec753b3fda06f7066753b9e5102d408ea4459e0eee5a4510086ef/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f72616e6b696e673f7374796c653d736f6369616c)](https://github.com/tensorflow/ranking)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1910.09676), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1812.00073), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1905.08957), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1811.04415)<br>- [data](http://hamedz.ir/resources/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/input.proto#L72)<br>- [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank), [![wiki](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/wiki.svg)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb) | 04.02.2021 |
| Toon-Me | A fun project to toon portrait images | [Vijish Madhavan](https://github.com/vijishmadhavan) | [![](https://camo.githubusercontent.com/beb7fbf1b897e6bed33e220dd1a73e352b16c528197719cb55f257dee66a9064/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f76696a6973686d6164686176616e2f546f6f6e2d4d653f7374796c653d736f6369616c)](https://github.com/vijishmadhavan/Toon-Me)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1710.10196), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1707.02921), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1603.08155) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/Light-Up/blob/master/Toon_Me_(Try_it_on_Colab).ipynb) | 22.01.2021 |
| TensorNetwork | A library for easy and efficient manipulation of tensor networks | [Chase Roberts](http://thenerdstation.github.io/) | [![](https://camo.githubusercontent.com/d7cf6b053f5d2624f8d737014ccb11a1dacd246630606296447a893795930058/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f54656e736f724e6574776f726b3f7374796c653d736f6369616c)](https://github.com/google/TensorNetwork)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1708.00006), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1306.2164)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://tensornetwork.readthedocs.io/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=YN2YBB0viKo) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/TensorNetwork/blob/master/colabs/Tensor_Networks_in_Neural_Networks.ipynb) | 21.01.2021 |
| Spleeter | Deezer source separation library including pretrained models | - [Romain Hennequin](http://romain-hennequin.fr/)<br>- [Anis Khlif](https://github.com/alreadytaikeune)<br>- [Félix Voituret](https://github.com/Faylixe)<br>- [Manuel Moussallam](https://mmoussallam.github.io/) | [![](https://camo.githubusercontent.com/662b8639c6cbd0026bf385cfe60ede5f586f9baad4f186fb511c2b7bc2358fb5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6465657a65722f73706c65657465723f7374796c653d736f6369616c)](https://github.com/deezer/spleeter) <br>- [blog post](https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e)<br>- [data](https://sigsep.github.io/datasets/musdb.html)<br>- [project](https://research.deezer.com/projects/spleeter.html) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb) | 10.01.2021 |
| Bullet Physics SDK | Real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc | - [Erwin Coumans](https://github.com/erwincoumans)<br>- [Yunfei Bai](https://github.com/YunfeiBai) | [![](https://camo.githubusercontent.com/ea24260ea8e7680896127bd028f774cde4c2d83d06477d81e729cbb6f872152d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f62756c6c6574706879736963732f62756c6c6574333f7374796c653d736f6369616c)](https://github.com/bulletphysics/bullet3) <br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.2ye70wns7io3)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/Microsoft/vcpkg)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/pybullet/)<br>- [website](https://pybullet.org/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PLinBNdD-7nkNCfoEKap4z3qadLVj8QB4a), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/9p0O941opGc), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/kZxPaGdoSJY), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/playlist?list=PL9LUFPiB6N3YrS0O7XM_1sBVWRnSRB643) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/bulletphysics/bullet3/blob/master/examples/pybullet/notebooks/HelloPyBullet.ipynb) | 14.10.2020 |
| Person Remover | Project that combines Pix2Pix and YOLO arhitectures in order to remove people or other objects from photos | - [Javier Gamazo](https://www.javiergamazo.com/)<br>- [Daryl Autar](https://github.com/Daryl149) | [![](https://camo.githubusercontent.com/3a850b9f0aeaa290bde5e589683c4eb6b6120826b60ca2531158c318da920919/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a617669726b2f506572736f6e5f72656d6f7665723f7374796c653d736f6369616c)](https://github.com/javirk/Person_remover)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/javirk/Person-remover-partial-convolutions), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/zzh8829/yolov3-tf2)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=_dRjY9gMcxE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1JDpH8MAjaKoekQ_H9ZaxYJ9_axiDtDGm) | 22.08.2020 |
| Semantic Segmentation | Pytorch implementation for Semantic Segmentation/Scene Parsing on MIT ADE20K dataset | - [Bolei Zhou](https://boleizhou.github.io/)<br>- [Hang Zhao](https://hangzhaomit.github.io/)<br>- [Xavier Puig](https://people.csail.mit.edu/xavierpuig/)<br>- [Sanja Fidler](http://www.cs.toronto.edu/~fidler/index.html)<br>- [Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/) | [![](https://camo.githubusercontent.com/5212787913cd24da58b994f8f26592cf4dc9ed2a78392ee5a9d2862a233e5bc3/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f7331313236332d3031382d313134302d30)](https://doi.org/10.1007/s11263-018-1140-0)[![](https://camo.githubusercontent.com/809428e54616b2b9022e2897f0c4ebe85d98a5e7fcea3c6c76c105b4c3342e3e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f435341494c566973696f6e2f73656d616e7469632d7365676d656e746174696f6e2d7079746f7263683f7374796c653d736f6369616c)](https://github.com/CSAILVision/semantic-segmentation-pytorch)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1608.05442), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1612.01105), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1807.10221), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1904.04514)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CSAILVision/sceneparsing), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/vacancy/Synchronized-BatchNorm-PyTorch), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/hszhao/semseg)<br>- [project](http://sceneparsing.csail.mit.edu/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb) | 21.08.2020 |
| Gin Config | Lightweight configuration framework for Python, based on dependency injection | - [Dan Holtmann-Rice](https://github.com/dhr)<br>- [Sergio Guadarrama](https://github.com/sguada)<br>- [Nathan Silberman](http://nsilberman.com/) | [![](https://camo.githubusercontent.com/aafaa141a2cbf69f1c8fda68cf016b150d1aa5972d8f1ea56f0df04bdda75bea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f67696e2d636f6e6669673f7374796c653d736f6369616c)](https://github.com/google/gin-config)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/stop-worrying-about-configs-with-gin-218562dd5c91)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/gin-config/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/gin-config/blob/master/gin/gin_intro.ipynb) | 13.08.2020 |
| Dopamine | Research framework for fast prototyping of reinforcement learning algorithms | - [Pablo Castro](https://psc-g.github.io/)<br>- [Subhodeep Moitra](http://www.deepmoitra.com/)<br>- [Carles Gelada](https://github.com/cgel)<br>- [Saurabh Kumar](https://scholar.google.com/citations?user=Rkr2uT8AAAAJ)<br>- [Marc Bellemare](http://www.marcgbellemare.info/) | [![](https://camo.githubusercontent.com/05831e7421de864bddc57fd5fcf93866737d7d2731880b10080b507b979e7179/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f646f70616d696e653f7374796c653d736f6369616c)](https://github.com/google/dopamine)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1812.06110), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1511.05952), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1812.05905), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1806.06923)<br>- [baselines](https://google.github.io/dopamine/baselines/)<br>- [blog post](https://opensource.googleblog.com/2019/02/dopamine-2.0.html)<br>- [![docker](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docker.svg)](https://google.github.io/dopamine/docker/)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://google.github.io/dopamine/docs/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/atari-py#roms), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/openai/mujoco-py#install-mujoco)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/the-21st-century/google-dopamine-new-rl-framework-f84a35b7fb3f)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/dopamine-rl/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/live/FWFoyFjeAaM?feature=share), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/bd4CsDp00RA) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/dopamine/blob/master/dopamine/colab/jax_agent_visualizer.ipynb) | 04.08.2020 |
| Analyzing Tennis Serve | We'll use the Video Intelligence API to analyze a tennis serve, including the angle of the arms and legs during the serve | [Dale Markowitz](https://daleonai.com/) | [![](https://camo.githubusercontent.com/de56df62b33d665013dd4eda1410e5d22486e21272ca6cc81f12e7b4f18df5d8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652f6d616b696e675f776974685f6d6c3f7374796c653d736f6369616c)](https://github.com/google/making_with_ml/tree/master/sports_ai) <br>- [blog post](https://daleonai.com/machine-learning-for-sports)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://manivannan-ai.medium.com/find-the-angle-between-three-points-from-2d-using-python-348c513e2cd)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=yLrOy2Xedgk) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/google/making_with_ml/blob/master/sports_ai/Sports_AI_Analysis.ipynb) | 14.07.2020 |
| YOLOv4 | This tutorial will help you build YOLOv4 easily in the cloud with GPU enabled so that you can run object detections in milliseconds! | [Alexey Bochkovskiy](http://www.alexeyab.com/) | [![](https://camo.githubusercontent.com/7505022ae605a5ef90517df8e6a4dc3968e5ef0690bf8a25b096b10e7be52e02/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f416c6578657941422f6461726b6e65743f7374796c653d736f6369616c)](https://github.com/AlexeyAB/darknet)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2004.10934), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/2011.08036)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://alexeyab84.medium.com/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe), [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982)<br>- [project](https://pjreddie.com/darknet/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/yolov4/)<br>- [![reddit](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/reddit.svg)](https://www.reddit.com/r/MachineLearning/comments/gydxzd/p_yolov4_the_most_accurate_realtime_neural/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/1_SiUOYUoOI), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/YDFf-TqJOFE) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg) | 25.06.2020 |
| TensorFlow Graphics | Differentiable computer graphics in tensorflow | - [Julien Valentin](https://github.com/julienvalentin)<br>- [Cem Keskin](https://github.com/cem-keskin)<br>- [Pavel Pidlypenskyi](https://github.com/podlipensky)<br>- [Ameesh Makadia](https://github.com/amakadia)<br>others[Avneesh Sud](https://github.com/avneesh-g)<br>[Sofien Bouaziz](http://sofienbouaziz.com/) | [![](https://camo.githubusercontent.com/5e42f642317e30a20ca33c4bb9f890d2f13076f456557b38c16321c4d81d3f02/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333435303530382e33343634353935)](https://doi.org/10.1145/3450508.3464595)[![](https://camo.githubusercontent.com/1479e5a2200e46904d04e29b5a27ec738c459cf45ba86f15b669560d8ce15bde/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f67726170686963733f7374796c653d736f6369616c)](https://github.com/tensorflow/graphics)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/syncedreview/computer-graphics-computer-vision-tensorflow-graphics-110e955e26bb)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensorflow-graphics/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/graphic)<br>- [![twitter](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/twitter.svg)](https://twitter.com/_TFGraphics_)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/Un0JDL3i5Hg) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/graphics/blob/master/tensorflow_graphics/notebooks/6dof_alignment.ipynb) | 20.05.2020 |
| GAN Dissection | Visualizing and Understanding Generative Adversarial Networks | - [David Bau](https://people.csail.mit.edu/davidbau/home/)<br>- [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)<br>- [Hendrik Strobelt](http://hendrik.strobelt.com/)<br>- [Bolei Zhou](https://boleizhou.github.io/)<br>others[Joshua Tenenbaum](https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/)<br>[William Freeman](https://billf.mit.edu/)<br>[Antonio Torralba](https://groups.csail.mit.edu/vision/torralbalab/) | [![](https://camo.githubusercontent.com/e87d82b333307c6d31aaa8218d64645c9bb993386b378ca5a382c6d1312b6741/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f435341494c566973696f6e2f47414e446973736563743f7374796c653d736f6369616c)](https://github.com/CSAILVision/GANDissect)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1811.10597), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1901.09887), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1807.10221)<br>- [demo](http://gandissect.res.ibm.com/ganpaint.html)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/CSAILVision/NetDissect), [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/junyanz/iGAN)<br>- [project](https://gandissect.csail.mit.edu/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=yVCgUYe4JTM) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/SIDN-IAP/global-model-repr/blob/master/notebooks/gandissect_solutions.ipynb) | 04.05.2020 |
| Sonnet | Library built on top of TensorFlow 2 designed to provide simple, composable abstractions for machine learning research | - [Malcolm Reynolds](https://github.com/malcolmreynolds)<br>- [Jack Rae](https://github.com/dm-jrae)<br>- [Andreas Fidjeland](https://github.com/akfidjeland)<br>- [Fabio Viola](https://github.com/fabioviola)<br>others[Adrià Puigdomènech](https://github.com/adria-p)<br>[Frederic Besse](https://github.com/fbesse)<br>[Tim Green](http://tfgg.me/)<br>[Sébastien Racanière](https://scholar.google.com/citations?user=o-h0vrQAAAAJ)<br>[Gabriel Barth-Maron](https://github.com/fastturtle)<br>[Diego Casas](https://github.com/diegolascasas) | [![](https://camo.githubusercontent.com/053bc273224a6ebfc8c7c83c7761ef0ec8fb5d91cb092799d2f8d054d80c9dc2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646565706d696e642f736f6e6e65743f7374796c653d736f6369616c)](https://github.com/deepmind/sonnet)<br>- [![deepmind](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/deepmind.svg)](https://www.deepmind.com/blog/open-sourcing-sonnet-a-new-library-for-constructing-neural-networks)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://sonnet.readthedocs.io/en/latest/index.html)<br>- [![neurips](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/neurips.svg)](https://papers.nips.cc/paper/2016/hash/fb87582825f9d28a8d42c5e5e5e8b23d-Abstract.html)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/checkpoint), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://www.tensorflow.org/guide/saved_model)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/rlpQjnUvoKw) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/deepmind/sonnet/blob/v2/examples/little_gan_on_mnist.ipynb) | 17.04.2020 |
| Classification of chest vs. adominal X-rays | The goal of this tutorial is to build a deep learning classifier to accurately differentiate between chest and abdominal X-rays | [tmoneyx01](https://github.com/tmoneyx01) | [![](https://camo.githubusercontent.com/7f2c287f36b82a86364603755161c230c0e0d6858ffa5f5096843b13eb94f0bb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6461692f6d6461692d636c69656e742d70793f7374796c653d736f6369616c)](https://github.com/mdai/mdai-client-py) <br>- [annotator](https://public.md.ai/annotator/project/PVq9raBJ)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](https://docs.md.ai/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/mdai/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson1-xray-images-classification.ipynb) | 07.03.2020 |
| Earth Engine Python API and Folium Interactive Mapping | This notebook demonstrates how to setup the Earth Engine and provides several examples for visualizing Earth Engine processed data interactively using the folium library | [Qiusheng Wu](https://wetlands.io/) | [![](https://camo.githubusercontent.com/ec7c94964aa8675f246ccadc58e2fd13d2acd6f560397526d2247c1f2c00abeb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f707974686f6e2d76697375616c697a6174696f6e2f666f6c69756d3f7374796c653d736f6369616c)](https://github.com/python-visualization/folium) <br>- [api](https://developers.google.com/earth-engine/python_install)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/earthengine-api/), [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/folium/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/giswqs/qgis-earthengine-examples/blob/master/Folium/ee-api-folium-setup.ipynb) | 20.01.2020 |
| Tensor2Tensor | Library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model | - [Ashish Vaswani](https://scholar.google.com/citations?user=oR9sCGYAAAAJ)<br>- [Samy Bengio](https://scholar.google.com/citations?user=Vs-MdPcAAAAJ)<br>- [Eugene Brevdo](https://ebrevdo.github.io/)<br>- [François Chollet](https://fchollet.com/)<br>others[Aidan Gomez](https://gom.ai/)<br>[Stephan Gouws](https://scholar.google.com/citations?user=lLTdYUYAAAAJ)<br>[Llion Jones](https://www.linkedin.com/in/llion-jones-9ab3064b)<br>[Łukasz Kaiser](https://scholar.google.com/citations?user=JWmiQR0AAAAJ)<br>[Nal Kalchbrenner](https://www.nal.ai/)<br>[Niki Parmar](https://github.com/nikiparmar)<br>[Ryan Sepassi](https://ryansepassi.com/)<br>[Noam Shazeer](https://github.com/nshazeer)<br>[Jakob Uszkoreit](https://scholar.google.com/citations?user=mOG0bwsAAAAJ) | [![](https://camo.githubusercontent.com/3f59190a813bad52c86de647afd84ea83751820c3e6d34f4876022c43bb876dd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f74656e736f723274656e736f723f7374796c653d736f6369616c)](https://github.com/tensorflow/tensor2tensor)<br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1803.07416), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1812.02825), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.03762), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.03059), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1706.05137), [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1801.09797)<br>- [blog post](https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html)<br>- [data](https://research.fb.com/downloads/babi/)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://towardsdatascience.com/tensor2tensor-and-one-model-to-learn-them-all-7ef3f9b61ba4)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/tensor2tensor/)<br>- [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://tensorflow.github.io/tensor2tensor/cloud_mlengine.html), [![tf](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/tf.svg)](https://tensorflow.github.io/tensor2tensor/cloud_tpu.html)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/O2UvKxaOH7c), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/VYQ8n3Besrw), [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://youtu.be/cS2UZKHq4i4) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/Transformer_translate.ipynb) | 14.01.2020 |
| Traffic counting | Making Road Traffic Counting App based on Computer Vision and OpenCV | [Andrey Nikishaev](https://github.com/creotiv) | [![](https://camo.githubusercontent.com/44349f14f728cd80d926fab9430b6c034b7d0cabc08a9ce7b70b60187dac9eae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7363696b69742d766964656f2f7363696b69742d766964656f3f7374796c653d736f6369616c)](https://github.com/scikit-video/scikit-video)<br>- [![docs](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/docs.svg)](http://www.scikit-video.org/stable/)<br>- [![git](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/git.svg)](https://github.com/creotiv/object_detection_projects/tree/master/opencv_traffic_counting)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://medium.com/machine-learning-world/tutorial-making-road-traffic-counting-app-based-on-computer-vision-and-opencv-166937911660)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/scikit-video/)<br>- [![yt](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/yt.svg)](https://www.youtube.com/watch?v=_o5iLbRHKao) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/drive/12N4m_RYKqrpozRzh9qe7nQE_sIqQH9U8) | 10.01.2020 |
| Imagededup | This package provides functionality to make use of hashing algorithms that are particularly good at finding exact duplicates as well as convolutional neural networks which are also adept at finding near duplicates | - [Tanuj Jain](https://github.com/tanujjain)<br>- [Christopher Lennan](https://github.com/clennan)<br>- [Dat Tran](https://dat-tran.com/) | [![](https://camo.githubusercontent.com/bd10187638a40e07219c0be1c560221c9f99628c739c26daa1ec8ee40a9fa497/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696465616c6f2f696d61676564656475703f7374796c653d736f6369616c)](https://github.com/idealo/imagededup) <br>- [![arxiv](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/arxiv.svg)](https://arxiv.org/abs/1704.04861)<br>- [![medium](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/medium.svg)](https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5)<br>- [project](https://idealo.github.io/imagededup/)<br>- [![pypi](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/pypi.svg)](https://pypi.org/project/imagededup/) | [![Open In Colab](https://github.com/amrzv/awesome-colab-notebooks/raw/main/images/colab.svg)](https://colab.research.google.com/github/idealo/imagededup/blob/master/examples/CIFAR10_duplicates.ipynb) | 03.10.2019 |

# Best of the best

[Permalink: Best of the best](https://github.com/amrzv/awesome-colab-notebooks/blob/main/README.md#best-of-the-best)

| authors | repositories | papers | packages |
| --- | --- | --- | --- |
| - [Chen Change Loy](https://www.mmlab-ntu.com/person/ccloy/)<br>- [Ziwei Liu](https://liuziwei7.github.io/)<br>- [Xintao Wang](https://xinntao.github.io/)<br>- [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ)<br>- [Adam Roberts](https://github.com/adarob)<br>- [Daniel Cohen-Or](https://danielcohenor.com/)<br>- [Jesse Engel](https://github.com/jesseengel)<br>- [Curtis Hawthorne](https://github.com/cghawthorne)<br>- [Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)<br>- [Björn Ommer](https://ommer-lab.com/people/ommer/)<br>- [Yuval Alaluf](https://yuval-alaluf.github.io/)<br>- [Or Patashnik](https://orpatashnik.github.io/)<br>- [Michael Black](https://ps.is.mpg.de/~black)<br>- [Yong Zhang](https://yzhang2016.github.io/)<br>- [Billy Lamberta](https://github.com/lamberta)<br>- [Nikhila Ravi](https://nikhilaravi.com/)<br>- [Patrick Esser](https://github.com/pesser)<br>- [Robin Rombach](https://github.com/rromb)<br>- [Amit Bermano](https://www.cs.tau.ac.il/~amberman/)<br>- [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)<br>- [Bolei Zhou](https://boleizhou.github.io/)<br>- [Xiaodong Cun](https://vinthony.github.io/academic/)<br>- [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | - ollama [![](https://camo.githubusercontent.com/57812a95a319d8a004bd11776a82cf59fb1d76c0648972874d7fbd3d53a34a01/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f6c6c616d612f6f6c6c616d613f7374796c653d736f6369616c)](https://github.com/ollama/ollama)<br>- langchain [![](https://camo.githubusercontent.com/6910d0f894fb43e876ff0b5d40d135a18084d314f1bcda5c43f91501eef67aa1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c616e67636861696e2d61692f6c616e67636861696e3f7374796c653d736f6369616c)](https://github.com/langchain-ai/langchain)<br>- models [![](https://camo.githubusercontent.com/a05c6a10141aa5829d7218a8c698391289a603709741d593b3f9f8005a8ce9e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f74656e736f72666c6f772f6d6f64656c733f7374796c653d736f6369616c)](https://github.com/tensorflow/models)<br>- whisper [![](https://camo.githubusercontent.com/51657ac9ca7fad1589f7b58645272d798ce880a002b5360f2d84b43c58112cbe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6f70656e61692f776869737065723f7374796c653d736f6369616c)](https://github.com/openai/whisper)<br>- stable-diffusion [![](https://camo.githubusercontent.com/457ff305a9229a76b0f4753eb095da3e4fbefbadf1d234e9ee2a9ea03041c596/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f6d705669732f737461626c652d646966667573696f6e3f7374796c653d736f6369616c)](https://github.com/CompVis/stable-diffusion)<br>- ComfyUI [![](https://camo.githubusercontent.com/57efebdb98eae1b1a9ccda191d64c0937aa4a3ac775c6aacd529671e7d7bc566/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f6d6679616e6f6e796d6f75732f436f6d667955493f7374796c653d736f6369616c)](https://github.com/comfyanonymous/ComfyUI)<br>- open-interpreter [![](https://camo.githubusercontent.com/592340d8095bb1bf9d191fce244c639a462c686b89b9d02a42bb8417fd583e09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4b696c6c69616e4c756361732f6f70656e2d696e7465727072657465723f7374796c653d736f6369616c)](https://github.com/KillianLucas/open-interpreter)<br>- Real-Time-Voice-Cloning [![](https://camo.githubusercontent.com/f456c3d4d684a0d2d675a4e76766f323ab320243fb7ec4b1b760020c9f10dc41/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436f72656e74696e4a2f5265616c2d54696d652d566f6963652d436c6f6e696e673f7374796c653d736f6369616c)](https://github.com/CorentinJ/Real-Time-Voice-Cloning)<br>- yolov5 [![](https://camo.githubusercontent.com/c4b9baacd1c41802bd1cab446fd9e0918cdead966293fc2713e04ecd067b431d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756c7472616c79746963732f796f6c6f76353f7374796c653d736f6369616c)](https://github.com/ultralytics/yolov5)<br>- segment-anything [![](https://camo.githubusercontent.com/fda9f7bfbe5462db4c5ea420348ca3592c669a0d5b92eca51d8bb1044b5c11b5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f66616365626f6f6b72657365617263682f7365676d656e742d616e797468696e673f7374796c653d736f6369616c)](https://github.com/facebookresearch/segment-anything)<br>- PythonDataScienceHandbook [![](https://camo.githubusercontent.com/b2279f99106dd1c53cc266774c7d9c7f5ba63be4b9d32a7cf57931a0f5154719/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a616b657664702f507974686f6e44617461536369656e636548616e64626f6f6b3f7374796c653d736f6369616c)](https://github.com/jakevdp/PythonDataScienceHandbook)<br>- Fooocus [![](https://camo.githubusercontent.com/7d2e634a8152ba3e72a9ef46a35eb94fcf516a01b64d67b47543148a6e1bf067/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6c6c6c7961737669656c2f466f6f6f6375733f7374796c653d736f6369616c)](https://github.com/lllyasviel/Fooocus)<br>- stablediffusion [![](https://camo.githubusercontent.com/3afb12adc6f6de4f662f92a76ac00bb2673378d06ac3a76a108eaf8e7624db17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f53746162696c6974792d41492f737461626c65646966667573696f6e3f7374796c653d736f6369616c)](https://github.com/Stability-AI/stablediffusion)<br>- autogen [![](https://camo.githubusercontent.com/bbb6473b9c51bb308dd56301a0b8326babe2f298f927cbebf276c59781266e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f6175746f67656e3f7374796c653d736f6369616c)](https://github.com/microsoft/autogen)<br>- llama\_index [![](https://camo.githubusercontent.com/42916e39576dd21ceb99b5b15260429304ae53f5659833c19a08eda99247e0fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f72756e2d6c6c616d612f6c6c616d615f696e6465783f7374796c653d736f6369616c)](https://github.com/run-llama/llama_index)<br>- TTS [![](https://camo.githubusercontent.com/985a8454c38b97b8047e479b3b1fff8a85457a90ec642febc8d8a241b0bd52d9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f636f7175692d61692f5454533f7374796c653d736f6369616c)](https://github.com/coqui-ai/TTS)<br>- Open-Assistant [![](https://camo.githubusercontent.com/d4cede2e2e5b93a58295ff3543844a79874e157b9f71450c95c7395930eaa72c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4c41494f4e2d41492f4f70656e2d417373697374616e743f7374796c653d736f6369616c)](https://github.com/LAION-AI/Open-Assistant)<br>- bark [![](https://camo.githubusercontent.com/e2a355eb86386917f553ff0fcb7328f3a3447c00dc28b180fbf86716de1ed3e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f73756e6f2d61692f6261726b3f7374796c653d736f6369616c)](https://github.com/suno-ai/bark)<br>- GFPGAN [![](https://camo.githubusercontent.com/ac0e568d14ba24fe240806ead658e4e6c6df6e51e104ea8eb7e3a01e06522070/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f54656e63656e744152432f47465047414e3f7374796c653d736f6369616c)](https://github.com/TencentARC/GFPGAN)<br>- ultralytics [![](https://camo.githubusercontent.com/6bb27e4c4557560b490abaee8cae2be867468ad0fe3c46ad3d106cbb0ef8766e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f756c7472616c79746963732f756c7472616c79746963733f7374796c653d736f6369616c)](https://github.com/ultralytics/ultralytics)<br>- ray [![](https://camo.githubusercontent.com/279f21209daffaf2ca2523725b48849a6ad98e437b8c27c9808c1512df143efd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7261792d70726f6a6563742f7261793f7374796c653d736f6369616c)](https://github.com/ray-project/ray)<br>- google-research [![](https://camo.githubusercontent.com/d133dccb193ebd202b15270c232696ecf695dfe76659dd32fb47e91659f29482/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f676c652d72657365617263682f676f6f676c652d72657365617263683f7374796c653d736f6369616c)](https://github.com/google-research/google-research)<br>- visual-chatgpt [![](https://camo.githubusercontent.com/d65d13f6190269b204396f9fe0d885826b0be26936a67dbb737ac59d1e89188a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f76697375616c2d636861746770743f7374796c653d736f6369616c)](https://github.com/microsoft/visual-chatgpt) | - Image segmentation [![](https://camo.githubusercontent.com/d5b16ded3f2751f2e2320d381be7cad1b73e6fdd1982987e4612f4b0bc79d76a/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3331392d32343537342d345f3238)](http://doi.org/10.1007/978-3-319-24574-4_28)<br>- AlphaFold [![](https://camo.githubusercontent.com/8f7a7e3b0986e960b45ad9d55f67d8d77db012af63bd610413b7ac809cc39088/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313033382f7334313538362d3032312d30333831392d32)](https://doi.org/10.1038/s41586-021-03819-2)<br>- XGBoost [![](https://camo.githubusercontent.com/cbdd5105a30ed3ebbc581fe471bcb6b4d3cf178b5e89613e7c7139d829b9dc20/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f323933393637322e32393339373835)](https://doi.org/10.1145/2939672.2939785)<br>- CycleGAN [![](https://camo.githubusercontent.com/6304a1266960c58592222e3dc858952bad8977d6044b0e693e6234d4af1dc6b3/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343562e323031372e323434)](https://doi.org/10.1109/ICCV.2017.244)<br>- Pix2Pix [![](https://camo.githubusercontent.com/d1af62c11f5e671314c7b037137a684e68055193fd4e46a6a8fcbdcfda205884/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f435650522e323031372e363332)](https://doi.org/10.1109/CVPR.2017.632)<br>- MoCo [![](https://camo.githubusercontent.com/f95b02deedc30b9c6ce3b74d605ee4c7ddf8585bf979673915741d37b33e8484/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030393735)](https://doi.org/10.1109/CVPR42600.2020.00975)<br>- LDM [![](https://camo.githubusercontent.com/8c4fa5252fc13ca4d91ee41af3758ba32cb46bdc152d22f93726c4eadd2e9e44/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031303432)](https://doi.org/10.1109/CVPR52688.2022.01042)<br>- EfficientDet [![](https://camo.githubusercontent.com/e84d9e5ee1a3139135899cd624f70e3204f1daf1e4de4528961fea44c693d6aa/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3031303739)](https://doi.org/10.1109/CVPR42600.2020.01079)<br>- ConvNeXt [![](https://camo.githubusercontent.com/4b031437079f37f50d20aeb205d5424ce9579163f952d00f6a4f23b7923b2637/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3031313637)](https://doi.org/10.1109/CVPR52688.2022.01167)<br>- DeepLabCut [![](https://camo.githubusercontent.com/f40a38780935dfd621aff7c621d417a0e2db792bb690a7c61d11ee6fe9a43982/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313033382f7334313539332d3031382d303230392d79)](https://doi.org/10.1038/s41593-018-0209-y)<br>- StyleGAN 2 [![](https://camo.githubusercontent.com/27f5849a7ec26f869cf77e380d67947f076a456bc781dba7cf6d489dc6f6e9d1/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234323630302e323032302e3030383133)](https://doi.org/10.1109/CVPR42600.2020.00813)<br>- Fine-tuning a BERT [![](https://camo.githubusercontent.com/6ca17dd7aec85150f2a72bdf5a7aa9823a7687123bc8e0b0c2c0c9f6a00a3152/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e31383635332f76312f4e31392d31343233)](https://doi.org/10.18653/v1/N19-1423)<br>- SwinIR [![](https://camo.githubusercontent.com/ece466f20be873aae4835f38d6cde52bca853acae72c65b006f8938fab791a4d/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343565735343132302e323032312e3030323130)](https://doi.org/10.1109/ICCVW54120.2021.00210)<br>- Instant-NGP [![](https://camo.githubusercontent.com/fa902589fe27bac1966816cda6751feaee4572a1bf1e1be1727bf23a21c7f5a0/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f333532383232332e33353330313237)](https://doi.org/10.1145/3528223.3530127)<br>- Mask2Former [![](https://camo.githubusercontent.com/3d5d7c73a37f166b3978ca5645d1d20c16e079e3f2487e90dec68f12a397a154/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505235323638382e323032322e3030313335)](https://doi.org/10.1109/CVPR52688.2022.00135)<br>- HMR [![](https://camo.githubusercontent.com/2e733b204d7179fa514e5d7176d5b00cc56c5cedf44f50fcf9bb4eb02f0d5413/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f435650522e323031382e3030373434)](https://doi.org/10.1109/CVPR.2018.00744)<br>- Taming Transformers for High-Resolution Image Synthesis [![](https://camo.githubusercontent.com/683877407900aacb05b3ae2150350eaf744787350a8926714d632133595cc78e/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f4356505234363433372e323032312e3031323638)](https://doi.org/10.1109/CVPR46437.2021.01268)<br>- Semantic Segmentation [![](https://camo.githubusercontent.com/5212787913cd24da58b994f8f26592cf4dc9ed2a78392ee5a9d2862a233e5bc3/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f7331313236332d3031382d313134302d30)](https://doi.org/10.1007/s11263-018-1140-0)<br>- Gaussian Splatting [![](https://camo.githubusercontent.com/93a5d019e7b54e05da79b0fd8c1f346a35ad402f854723361819d208f335b2e6/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313134352f33353932343333)](https://doi.org/10.1145/3592433)<br>- ByteTrack [![](https://camo.githubusercontent.com/3ef9dd33928453ecee81a3a3932f920fd957c23b710af3038bc41559c5312851/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313030372f3937382d332d3033312d32303034372d325f31)](https://doi.org/10.1007/978-3-031-20047-2_1)<br>- PIFu [![](https://camo.githubusercontent.com/43d0e68e7d9348cff526d58bbaccbf401ab1ef6a9fb36bd9ceb43a6542737855/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343562e323031392e3030323339)](https://doi.org/10.1109/ICCV.2019.00239)<br>- Neural Style Transfer [![](https://camo.githubusercontent.com/7f32c38a96d6f993ad368ece4c0b3e0771e246470641df300ff9f52dd8fd19c5/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313136372f31362e31322e333236)](https://doi.org/10.1167/16.12.326)<br>- Real-ESRGAN [![](https://camo.githubusercontent.com/27227b86825f3aaa07ea756e093ab595fc4930487cb1ea1f6591ed07e282c8da/68747470733a2f2f6170692e6a756c65736b72657565722e65752f6369746174696f6e2d62616467652e7068703f646f693d31302e313130392f494343565735343132302e323032312e3030323137)](https://doi.org/10.1109/ICCVW54120.2021.00217) | - xgboost [![](https://camo.githubusercontent.com/edfb2b59a496a2fd1cc4e8d5da1b568b0a736bc7b3aa744bddbb437ec9e9aadf/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f7867626f6f73743f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/xgboost/)<br>- langchain [![](https://camo.githubusercontent.com/1ce135024d6fb46fab2413cf8fe2eeea47a0bd29181b3836d2d56b4c6dab2dfe/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6c616e67636861696e3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/langchain/)<br>- catboost [![](https://camo.githubusercontent.com/a43c6a5917ccb72b52bf834e2899d3ff431f23fd0611b43054d744e22be0fbe8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f636174626f6f73743f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/catboost/)<br>- llama-index [![](https://camo.githubusercontent.com/4c5df5728dfc443dd1616ac84dc793fafb3796049fd4025e893ab52acfac314e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6c6c616d612d696e6465783f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/llama-index/)<br>- langgraph [![](https://camo.githubusercontent.com/4048395f30c913785307749668c2898f005585805541143b4f10be4d36d2b945/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6c616e6767726170683f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/langgraph/)<br>- folium [![](https://camo.githubusercontent.com/1d49244e8e43072327a8f4a613a43d24ad713152860165c43d221b1606c191d8/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f666f6c69756d3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/folium/)<br>- ollama [![](https://camo.githubusercontent.com/523b8523e109b9fd96b381bf61cbc54aef8798d48fa9f4fb62206493eda14ce6/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6f6c6c616d613f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/ollama/)<br>- bert-score [![](https://camo.githubusercontent.com/f1dbfbd8b4df417f8fd4f4da5e950a7ce024975888332cbcbaa30e50d3becc88/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f626572742d73636f72653f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/bert-score/)<br>- gin-config [![](https://camo.githubusercontent.com/3b4eb1b427c9cae039cf90304d28bf3a25b890a484de547b1f3acc8fc390eea1/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f67696e2d636f6e6669673f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/gin-config/)<br>- autofaiss [![](https://camo.githubusercontent.com/47641f2278539286bb3497fdae51227f70ce6c17b1eda3ed27bcc8c04f926958/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6175746f66616973733f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/autofaiss/)<br>- pyglove [![](https://camo.githubusercontent.com/2fbd3b3d67aefde8436df9ef6c84695df22be4d2ae1a7eeca775c33614a86d89/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f7079676c6f76653f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/pyglove/)<br>- mmdet [![](https://camo.githubusercontent.com/a446e9d558f94a7ff8b0ff26c1343220c7afc0bf750020dbec6ea582a85a8902/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6d6d6465743f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/mmdet/)<br>- earthengine-api [![](https://camo.githubusercontent.com/cb3788ae6f59bed4bcc5366aa3690340152145aacd753b249b1deb5ca2f528e5/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6561727468656e67696e652d6170693f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/earthengine-api/)<br>- unsloth [![](https://camo.githubusercontent.com/344806a6a28817c79fee68d0ac122961653e4ea07f340265025e3cc223441f46/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f756e736c6f74683f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/unsloth/)<br>- pybullet [![](https://camo.githubusercontent.com/8af7a46415649e7b6a004dcb8f5580d28b89bf599d2f6a3ea699f21d22e65837/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f707962756c6c65743f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/pybullet/)<br>- scikit-video [![](https://camo.githubusercontent.com/555f1e3fa005634ef52aee9feb338e0e2fb9a26309e44d582a9e5128580c1c24/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f7363696b69742d766964656f3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/scikit-video/)<br>- mmsegmentation [![](https://camo.githubusercontent.com/94ffe49b84a358d6ee5ae7727e4699fc39a4bcb35cdb03a8f7199916ed6b58cc/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6d6d7365676d656e746174696f6e3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/mmsegmentation/)<br>- Crawl4AI [![](https://camo.githubusercontent.com/c779d847164dc9e45b4a546b1a26b85a81d45595343509cb9c07d402340f68f0/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f437261776c3441493f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/Crawl4AI/)<br>- TPOT [![](https://camo.githubusercontent.com/a8f86e781fc7e87ad04f573a6379501df1b01796390819f1753a92af024c3a31/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f54504f543f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/TPOT/)<br>- tensorflow-graphics [![](https://camo.githubusercontent.com/93424c120fd59b00cf0e8f29327c1326a26013958128b88c70a99f23430e06fd/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f74656e736f72666c6f772d67726170686963733f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/tensorflow-graphics/)<br>- tensorflow-federated [![](https://camo.githubusercontent.com/47f8362d39daa5c8303bf2e1fc55ad0a91b59439e85291985f459529a055934e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f74656e736f72666c6f772d6665646572617465643f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/tensorflow-federated/)<br>- langfun [![](https://camo.githubusercontent.com/94a1852a340d9cf033d274de53529ecd6dc0f8ada560d1539c3abb6397424d62/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6c616e6766756e3f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/langfun/)<br>- imagededup [![](https://camo.githubusercontent.com/05109a6aeb4bbfb729f86cbc5d14d270ad422356b3ddef6867e3a7bf98894b17/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f696d61676564656475703f7374796c653d666c6174266c6f676f3d70797069266c6162656c3d254532253830253844266c6162656c436f6c6f723d66376637663426636f6c6f723d303036646164)](https://pypi.org/imagededup/) |

[![Stargazers over time](https://camo.githubusercontent.com/bf53246a3fb06c52e2088ebd0a604e0e8ef1af42e7444f4ced3d5e5b91ead4cd/68747470733a2f2f7374617263686172742e63632f616d727a762f617765736f6d652d636f6c61622d6e6f7465626f6f6b732e7376673f76617269616e743d6164617074697665)](https://starchart.cc/amrzv/awesome-colab-notebooks)

(generated by [generate\_markdown.py](https://github.com/amrzv/awesome-colab-notebooks/blob/main/generate_markdown.py) based on [research.json](https://github.com/amrzv/awesome-colab-notebooks/blob/main/data/research.json), [tutorials.json](https://github.com/amrzv/awesome-colab-notebooks/blob/main/data/tutorials.json), [cources.json](https://github.com/amrzv/awesome-colab-notebooks/blob/main/data/cources.json))

You can’t perform that action at this time.